\documentclass{mpaper}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{thmtools, thm-restate}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{tikz}
\usepackage{layouts}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{pgfplots}

\usetikzlibrary{shapes,bayesnet,arrows.meta,arrows,calc,pgfplots.colormaps}
%\tikzstyle{myarrow} = [-{Latex[length=2mm,width=2mm]}]

% Makes the legend have a single rectangle per colour
\pgfplotsset{
  /pgfplots/ybar legend/.style={
    /pgfplots/legend image code/.code={%
      \draw[##1,/tikz/.cd,bar width=3pt,yshift=-0.2em,bar shift=0pt]
      plot coordinates {(0cm,0.8em)};},
  },
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{problem}[theorem]{Open Problem}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\DKL}{D_{\mathrm{KL}}\infdivx}
\newcommand{\V}{V_{\mathbf{r}}}
\newcommand{\dx}{\,d\mathbf{r}\,d\mathbf{u}}
\newcommand{\vbound}{\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma}}
\newcommand{\rinf}{\lVert \mathbf{r} \rVert_\infty}

\newcommand{\approximationS}{q(\mathbf{u}_s, \mathbf{r}_s)}
\newcommand{\pfullS}{p(\mathcal{D}, \mathbf{u}_s, \mathbf{r}_s)}

\newcommand{\f}{f(\mathbf{r}, \mathbf{u}, t)}
\newcommand{\fn}{f_n(\mathbf{r}, \mathbf{u})}
\newcommand{\ftn}{f(\mathbf{r}, \mathbf{u}, t_n)}
\newcommand{\g}{g(\mathbf{r}, \mathbf{u})}

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}
\newcommand{\Luu}{\mathbf{L}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Euu}{\mathbf{E}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Err}{\mathbf{E}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Eru}{\mathbf{E}_{\mathbf{r},\mathbf{u}}}

\newcommand{\pfull}{p(\mathcal{D}, \mathbf{u}, \mathbf{r})}
\newcommand{\approximation}{q(\mathbf{u}, \mathbf{r})}
\newcommand{\posterior}{p(\mathbf{u}, \mathbf{r} \mid \mathcal{D})}

\newcommand{\dm}{\frac{\partial}{\partial\bm\mu}}
\newcommand{\dB}{\frac{\partial}{\partial\mathbf{B}}}
\newcommand{\dl}{\frac{\partial}{\partial \lambda_i}}
\newcommand{\dlj}{\frac{\partial}{\partial \lambda_j}}
\newcommand{\dt}{\frac{\partial}{\partial t}}
\newcommand{\df}{\left.\frac{\partial f}{\partial t}\right|_{(\mathbf{r},
    \mathbf{u}, t)}}

\newcommand{\pione}{\pi(a_1 \mid s_1)}
\newcommand{\pitwo}{\pi(a_1 \mid s_2)}
\newcommand{\pithree}{\pi(a_2 \mid s_3)}

\begin{document} % 14 pages

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian Processes}
\author{Paulius Dilkas}
\matricnum{2146879}
\maketitle

\begin{abstract}
  The inverse reinforcement learning (IRL) problem asks us to find a reward
  function of a Markov decision process that explains observed behaviour. Many
  approaches are only able to construct reward functions as linear combinations
  of state features. Out of those that can handle nonlinearity, none can provide
  a full posterior distribution of rewards. Providing variance estimates for
  rewards would allow one to judge how well the model has learned its policy and
  discover any weak spots the model may have. We show how to perform variational
  inference (VI) on a Gaussian process-based IRL model in order to approximate
  the posterior distribution of rewards. We prove the correctness of the
  approach and demonstrate the model's behaviour in practice. Being able to
  provide full posterior probability distributions in IRL unlocks many new
  research frontiers ranging from integrating recent developments in VI to make
  the models more efficient and flexible, to developing complex reinforcement
  learning agents that can explicitly search for opportunities to fix their
  weaknesses.
\end{abstract}

\section{Introduction} % first page
% 1. Problem (with examples, intuitive)
% 2. Its importance/relevance/motivation
% 3. My idea
% 4. List of contributions
% Forward-reference which section deals with what

Imagine using a machine learning (ML) algorithm to teach a robot how to move
around people so that it learns to predict where people are going and adjust its
path accordingly. The ML algorithm would use data about various possible
situations. But do we have enough data to ensure reasonably optimal behaviour?
Perhaps the robot behaves well in most situations, but fails in less common
scenarios. Can the ML model itself describe its weaknesses so that we could
ensure it is exposed to sufficiently many uncommon or difficult situations?

This learning problem
\cite{DBLP:journals/ijsr/KimP16,DBLP:journals/ijrr/KretzschmarSSB16} as well as
many others have benefited from an approach called \emph{inverse reinforcement
  learning} (IRL) (also known as inverse optimal control). IRL proposes a way to
learn behaviour from \emph{demonstrations} that typically come from human
actions. More formally, the IRL problem asks us to find a reward function for a
Markov decision process (MDP), where demonstrations are encoded as sets of
state-action pairs.

IRL is an important problem because adjusting the reward function by hand is
often unwieldy, since human behaviour often depends on many factors in
complicated ways
\cite{DBLP:conf/icml/PieterN04}. Moreover,
learning the reward function rather than the policy itself makes the model more
transferable to new environments---a minor change in the environment can
reorganise the whole policy but only have a local effect in the reward structure
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}.
IRL has been used to teach helicopters how to perform tricks
\cite{DBLP:conf/nips/AbbeelCQN06}, predict taxi destinations
\cite{DBLP:conf/huc/ZiebartMDB08}, and make driving safer and more efficient by
predicting pedestrian movement \cite{DBLP:conf/iros/ZiebartRGMPBHDS09} and the
driver's intentions \cite{DBLP:conf/aaai/VogelRGR12}.

% TODO: need to rewrite the first sentence
However, most IRL models in the literature make a convenient yet unjustified
assumption that the reward function can be expressed as a linear combination of
features
\cite{DBLP:conf/icml/PieterN04,DBLP:conf/icml/NgR00,ziebart2008maximum}.
This assumption makes the models unable to represent many reward structures. Out
of the non-linear models proposed to date, none can answer the questions posed
in the first paragraph. Quite often, the models assume that rewards have no
variance \cite{DBLP:conf/nips/LevinePK11,DBLP:conf/uai/JinDAS17}. In this paper,
we show how that assumption can be lifted by switching from maximum likelihood
estimation to \emph{variational inference} (VI), i.e., we approximate the
posterior distribution of the model by optimising the parameters of a simpler
distribution to make it similar to the posterior. This approach can prove useful
in four major ways:
\begin{enumerate}
\item By working with full distributions instead of point estimates, we can
  expect more precise reward predictions.
\item Variance estimates can be used to guide what data should be collected
  next, i.e., if the rewards of some states have abnormally high variance, we
  might want to expose the model to more data visiting those and surrounding
  states.
\item Variances estimates can also be used to judge whether we can trust the
  predictions of the model or, perhaps, the model could benefit from some
  adjustments or more data.
\item By adopting a more Bayesian approach, we automatically incorporate Occam's
  razor into the model that guards against overfitting
  \cite{DBLP:conf/uai/JinDAS17}.
\end{enumerate}

Our main contribution is a lengthy proof in Section \ref{sec:proof} that shows
how VI can be applied to the maximum-entropy IRL model with Gaussian processes
(GPs) proposed by Levine et al. \cite{DBLP:conf/nips/LevinePK11}. We describe
how we adapt the model and set up the VI problem in Section \ref{sec:model}.
Finally, in Section \ref{sec:evaluation} we examine the convergence properties
of our model and its ability to deduce optimal policies in practice.

\paragraph{Notation and Conventions}

For any matrix $\mathbf{A}$, we use either $A_{i,j}$ or
$[\mathbf{A}]_{i,j}$ to denote the element of $\mathbf{A}$ in row $i$ and column
$j$. We use $\tr(\mathbf{A})$ to denote its \emph{trace} and $\adj(\mathbf{A})$
for its \emph{adjugate} (or \emph{classical adjoint}). For any vector
$\mathbf{x}$, we write $\mathbb{R}_d[\mathbf{x}]$ to denote a vector space of
polynomials with degree at most $d$, where variables are elements of
$\mathbf{x}$, and coefficients are in $\mathbb{R}$.

Throughout the paper, all integrals should be interpreted as definite integrals
over the entire sample space. When referencing measurability, we assume Lebesgue
measure in a Euclidean space with a suitable number of dimensions. Similarly,
whenever we consider the existence of an integral, we use the Lebesgue
definition of integration.

\section{The Problem} \label{sec:problem}

In this section, we introduce definitions and mathematical details relevant to
the problem. We begin by formally defining what the problem is.

\begin{definition}[MDP]
  A \emph{Markov decision process} is a set $\mathcal{M} = \{ \mathcal{S},
  \mathcal{A}, \mathcal{T}, \gamma, \mathbf{r} \}$, where $\mathcal{S}$ and
  $\mathcal{A}$ are sets of states and actions, respectively; $\mathcal{T} :
  \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is a function
  defined so that $\mathcal{T}(s, a, s')$ is the probability of moving to state
  $s'$ after taking action $a$ in state $s$; $\gamma \in [0, 1)$ is the discount
  factor; and $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$ is the reward
  vector\footnote{Depending on the situation, we will sometimes represent
    rewards as a function $r : \mathcal{S} \to \mathbb{R}$.}.
\end{definition}

\begin{definition}[IRL]
  Given an MDP without rewards $\mathcal{M} \setminus \{ \mathbf{r} \}$, an
  $|\mathcal{S}| \times d$ feature matrix $\mathbf{X}$ (where $d$ is the number
  of features), and a set of expert demonstrations $\mathcal{D} = \{\zeta_i
  \}_{i=1}^N$, where each demonstration $\zeta_i = \{ (s_{i,t}, a_{i,t})
  \}_{t=1}^T$ is a multiset of state-action pairs representing optimal actions
  executed by an expert, find the reward function that maximises the probability
  of observing the demonstrations, i.e.,
  \[
    \argmax_{\mathbf{r}} p(\mathcal{D} \mid \mathbf{r}).
  \]
\end{definition}

The optimal (deterministic) \emph{policy} $\pi : \mathcal{S} \to \mathcal{A}$
(i.e., a choice of actions for each state that maximises reward over time) is
usually constructed by defining a \emph{value (utility) function} $\V :
\mathcal{S} \to \mathbb{R}$ that measures how good a state is based on the
reward $\mathbf{r}$ as well as the structure of the MDP. One can then find $\V$
by applying the Bellman backup operator until convergence to every $s \in
\mathcal{S}$ (the technique is known as \emph{value iteration})
\cite{DBLP:books/daglib/0023820}:
\[
  V_{\mathbf{r}}(s) \coloneqq r(s) + \gamma \max_{a \in \mathcal{A}} \sum_{s' \in
    \mathcal{S}} \mathcal{T}(s, a, s')V_{\mathbf{r}}(s').
\]

However, we follow previous work on GP IRL
\cite{DBLP:conf/nips/LevinePK11,DBLP:conf/uai/JinDAS17}, and use a
\emph{linearly solvable} (or \emph{maximum causal entropy}) MDP with a
stochastic policy that defines probability distributions over actions (instead
of suggesting a single action for each state) \cite{ziebart2008maximum}. This
type of MDP can be solved by applying the `soft' version of the operator
\cite{DBLP:conf/nips/LevinePK11,supplementary_material}:
\begin{equation} \label{eq:update_rule}
  \V(s) \coloneqq \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s'
      \in \mathcal{S}} \mathcal{T}(s, a, s')\V(s') \right).
\end{equation}
With this model, we can express the likelihood as
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}
\begin{align*}
  p(\mathcal{D} \mid \mathbf{r}) &= \prod_{i=1}^N \prod_{t=1}^T p(a_{i,t} \mid s_{i,t}) \\
                                 &= \exp\left( \sum_{i=1}^N \sum_{t=1}^T Q_{\mathbf{r}}(s_{i,t}, a_{i,t}) - \V(s_{i,t}) \right),
\end{align*}
where
\[
  Q_{\mathbf{r}}(s, a) = r(s) + \gamma\sum_{s' \in \mathcal{S}}
  \mathcal{T}(s, a, s')\V(s').
\]

% TODO: rewrite the 'we write' sentence and the sentence before that
As we want learned rewards to generalise to previously unseen states and
for states to have a notion of similarity, the IRL definition also includes $d$
features associated with each state. In this paper, we will focus on modelling
rewards as a function from feature space to $\mathbb{R}$ using a \emph{Gaussian
  process}. A GP is defined as a collection of random variables, any finite
number of which has a joint Gaussian distribution
\cite{DBLP:books/lib/RasmussenW06}. We write $r \sim \mathcal{GP}(0, k)$ to say
that $r$ is a GP with mean $0$ and covariance function $k$. \emph{Covariance
  functions} (also known as \emph{kernels}) take two state feature vectors as
input and quantify how similar the two states are, in a sense that we would
expect high covariance scores to be associated with similar rewards.

A common way to scale GPs to larger data sets is by selecting $m$ points in the
feature space--called \emph{inducing points}---and focus most of the training
effort on them \cite{DBLP:journals/corr/abs-1807-01065}. Let $\mathbf{X_u}$ be
the $m \times d$ matrix of features at inducing points, and let $\mathbf{u}$ be
the rewards at those states. Then the full joint probability distribution can be
factorised as
\begin{equation} \label{eq:full}
  \pfull = p(\mathbf{u}) \times p(\mathbf{r} \mid \mathbf{u}) \times
  p(\mathcal{D} \mid \mathbf{r}),
\end{equation}
where
\begin{align*}
  p(\mathbf{u}) &= \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu) \\
                &= \frac{1}{(2\pi)^{m/2}|\Kuu|^{1/2}}\exp \left( -\frac{1}{2} \mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} \right) \\
                &= \exp\left(-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} - \frac{1}{2}\log|\Kuu| - \frac{m}{2}\log 2\pi\right)
\end{align*}
is the GP prior \cite{DBLP:books/lib/RasmussenW06}. The GP posterior is then a
multivariate Gaussian \cite{DBLP:conf/nips/LevinePK11} defined as
\begin{equation} \label{eq:r}
  p(\mathbf{r} \mid \mathbf{u}) =
  \mathcal{N}(\mathbf{r}; \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr - \Kru^\intercal\Kuu^{-1}\Kru).
\end{equation}
The matrices such as
$\Kru$ are called \emph{covariance matrices} (also \emph{kernel matrices} and
\emph{Gram matrices}) and are defined as
$[\Kru]_{i,j} = k(\mathbf{x}_{\mathbf{r},i},
\mathbf{x}_{\mathbf{u},j})$, where $\mathbf{x}_{\mathbf{r},i}$ and
$\mathbf{x}_{\mathbf{u},j}$ denote feature vectors for the $i$th state in
$\mathcal{S}$ and the $j$th inducing point, respectively
\cite{DBLP:conf/uai/JinDAS17}.

Our goal is then to use VI in order to approximate $\posterior$.
Let $\approximation$ denote our approximation. VI aims to optimise this
approximation by minimising the \emph{Kullback-Leibler} (KL) divergence between
the original probability distribution and our approximation. KL divergence
(asymmetrically) measures the difference between two probability distributions
and can be defined as \cite{blei2017variational}
\begin{align*}
  \DKL{\approximation}{\posterior} &= \mathbb{E}_{\approximation}[\log\approximation - \log\posterior ] \\
                                   &= \mathbb{E}_{\approximation}[\log\approximation - \log\pfull] \\
                                   &+ \mathbb{E}_{\approximation}[\log p(\mathcal{D})].
\end{align*}
The last term is both hard to compute and constant with respect to
$\approximation$ \cite{blei2017variational}, so we can remove it from our
optimisation objective. The negation of what remains is known as the
\emph{evidence lower bound} (ELBO) and is defined as
\cite{DBLP:books/lib/Bishop07,blei2017variational}
\begin{equation} \label{eq:elbo}
  \begin{split}
    \mathcal{L} &= \mathbb{E}_{\approximation}\left[ \log \frac{\pfull}{\approximation} \right] \\
    &= \iiint \log \frac{\pfull}{\approximation} \approximation\dx.
  \end{split}
\end{equation}
Thus, instead of minimising KL divergence, we focus on maximising $\mathcal{L}$
by optimising the values of parameters to be defined in Section \ref{sec:model}.
Note that hereafter we drop the subscript notation, as all expected values will
be with respect to $\approximation$.

\section{Background}

We introduce a few definitions and results from linear algebra, numerical
analysis, and measure theory that will be used later in the paper. Namely, we
will use several different vector and matrix norms, consider how an inverse of a
matrix changes with a small perturbation, and introduce Lebesgue's dominated
convergence theorem which plays a major role in our theoretical results in
Section \ref{sec:proof}.

\begin{definition}[Norms]
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} (\emph{$\ell_\infty$-norm}) is
  \[
    \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|
  \]
  whereas its \emph{$\ell_1$-norm} is
  \[
    \lVert \mathbf{x} \rVert_1 = \sum_{i = 1}^n |x_i|.
  \]
  Let $\mathbf{A}$ be a matrix. For any vector norm $\lVert
  \cdot \rVert_p$, we can also define its \emph{induced norm} for matrices as
  \[
    \lVert \mathbf{A} \rVert_p = \sup_{\mathbf{x} \ne \mathbf{0}} \frac{\lVert
      \mathbf{Ax} \rVert_p}{\lVert \mathbf{x} \rVert_p}.
  \]
  In particular, for $p = \infty$, we have
  \[
    \lVert \mathbf{A} \rVert_\infty = \max_i \sum_{j} |A_{i,j}|.
  \]
\end{definition}

\begin{lemma}[Perturbation Lemma
  \cite{layton2014numerical}] \label{lemma:perturbation}
  Let $\lVert \cdot \rVert$ be any matrix norm, and let $\mathbf{A}$ and
  $\mathbf{E}$ be matrices such that $\mathbf{A}$ is invertible and $\lVert
  \mathbf{A}^{-1} \rVert \lVert \mathbf{E} \rVert < 1$, then $\mathbf{A} +
  \mathbf{E}$ is invertible, and
  \[
    \lVert (\mathbf{A} + \mathbf{E})^{-1} \rVert \le \frac{\lVert
      \mathbf{A}^{-1} \rVert}{1 - \lVert \mathbf{A}^{-1} \rVert \lVert
      \mathbf{E} \rVert}.
  \]
\end{lemma}

\begin{theorem}[Dominated Convergence Theorem
  \cite{royden2010real}] \label{thm:lebesgue}
  Let $(X, \mathcal{M}, \mu)$ be a measure space and $\{ f_n \}$ a sequence of
  measurable functions on $X$ for which $\{ f_n \} \to f$ pointwise a.e. on $X$
  and the function $f$ is measurable. Assume there is a non-negative function
  $g$ that is integrable over $X$ and dominates the sequence $\{ f_n \}$ on $X$
  in the sense that
  \[
    |f_n| \le g \text{ a.e. on $X$ for all $n$.}
  \]
  Then $f$ is integrable over $X$ and
  \[
    \lim_{n \to \infty} \int_X f_n\,d\mu = \int_X f\,d\mu.
  \]
\end{theorem}

\section{The Model} \label{sec:model}

In order to have a fully functional model, we still need to make several key
design decisions. In this section, we describe the covariance function used to
define the GP, consider what parameters should be used to optimise $\mathcal{L}$
and how they should be initialised, and in Section \ref{sec:elbo}, we derive our
final expression for $\mathcal{L}$.

We stick with the same covariance function as in the work by Levine et al.
\cite{DBLP:conf/nips/LevinePK11}, which is a version of the \emph{automatic
  relevance detection kernel}:
\begin{align*}
  k(\mathbf{x}_i, \mathbf{x}_j) = \lambda_0\exp\left( \vphantom{\frac{1}{2}} \right. &-\frac{1}{2}(\mathbf{x}_i - \mathbf{x}_j)^\intercal\bm\Lambda(\mathbf{x}_i - \mathbf{x}_j) \\
                                                                                     &- \mathbbm{1}[i \ne j]\sigma^2\tr(\bm\Lambda) \left. \vphantom{\frac{1}{2}} \right).
\end{align*}
Here, $\lambda_0$ is the overall `scale' factor, $\bm\Lambda = \diag(\lambda_1,
\dots, \lambda_d)$ is a diagonal matrix that determines the importance of each
feature, $\mathbbm{1}$ is defined as
\[
  \mathbbm{1}[b] = \begin{cases}
    1 & \mbox{if $b$ is true} \\
    0 & \mbox{otherwise},
  \end{cases}
\]
and $\sigma^2$ is fixed as $10^{-2}/2$, since this value has little influence on
the behaviour of the algorithm and is here as a noise factor used to avoid
singular covariance matrices \cite{DBLP:conf/nips/LevinePK11}. We will write
$\bm\lambda = (\lambda_0, \dots, \lambda_d)^\intercal$ to refer to both
$\lambda_0$ and $\bm\Lambda$ at the same time.

Ideally, we would like to model $\bm\lambda$ with an approximating distribution.
However, due to how the prior probability density function (PDF) for
$\mathbf{u}$ involves $\Kuu^{-1}$, and $\mathcal{L}$ has an expected value that
cannot be eliminated (see Section \ref{sec:elbo}), we are unable to show that
such an $\mathcal{L}$ is well-defined. More generally, we pose the following
problem, which, to the best of our knowledge, is currently open:
\begin{problem}
  Let $\mathbf{A}$ be a $n \times n$ matrix of coefficients, $X$ be a random
  variable, and $\mathbf{M}$ be an $n \times n$ matrix such that $M_{i,j} = f(X,
  A_{i,j})$, where $f$ is an arbitrary function. Under what circumstances does
  $\mathbb{E}[\mathbf{M}^{-1}]$ exist?
\end{problem}
While there are some obvious examples where the expected value exists (e.g.,
$f(X, A_{i,j}) = A_{i,j}X$ for an invertible $\mathbf{A}$ and many distributions
of $X$), it would be particularly interesting to know whether the answer is
`always'. A proof of such a result would allow us to model $\bm\lambda$ instead
of treating it as a variational parameter, and would help to guard against
overfitting. For now, $\bm\lambda$ will have to be treated as a variational
parameter.

It remains to decide on the approximation distribution for $\mathbf{u}$ and
$\mathbf{r}$. As is commonly done when applying VI to GPs
\cite{DBLP:conf/nips/ChengB17}, we set
\begin{equation} \label{eq:approximation}
  \approximation = q(\mathbf{u}) q(\mathbf{r} \mid \mathbf{u}),
\end{equation}
where $q(\mathbf{r} \mid \mathbf{u}) = p(\mathbf{r} \mid \mathbf{u})$ and
$q(\mathbf{u}) = \mathcal{N}(\mathbf{u}; \bm\mu, \bm\Sigma)$.

Ong et al. \cite{ong2018gaussian} have recently suggested that, in order to make
variational approximation of a multivariate Gaussian more scalable, the
covariance matrix should be decomposed as $\bm\Sigma =
\mathbf{B}\mathbf{B}^\intercal + \mathbf{D}^2$, where $\mathbf{B}$ is a lower
triangular $m \times p$ matrix with positive diagonal entries, and $\mathbf{D}$
is a diagonal matrix. Typically, we would set $p$ so that $p \ll m$ to get an
efficient approximation, but in this case we will simply set $p = m$ and
$\mathbf{D} = \mathbf{O}_m$ in order to retain full covariance structure.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node[latent] (u) {$\mathbf{u}$};
    \node[latent, right=of u] (r) {$\mathbf{r}$};
    \node[obs, right=of r] (D) {$\mathcal{D}$};
    \node[const, below=of r] (lambda) {$\bm\lambda$};
    \node[const, below=of u] (mu) {$\bm\mu$};
    \node[const, left=of mu] (B) {$\mathbf{B}$};
    \edge {mu, B} {u};
    \edge {lambda, u} {r};
    \edge {r} {D};
  \end{tikzpicture}
  \caption{Our VI problem expressed as a (simplified)
    Bayesian network. The only observed variable (representing the
    demonstrations) is in a gray circle, modelled latent variables are in white
    circles, and the variational parameters are at the bottom.}
  \label{fig:graphical_model}
\end{figure}

The resulting model is summarised in Figure \ref{fig:graphical_model}. We rely
on $p(\mathcal{D} \mid \mathbf{r})$ as the only link between data and our model.
Since the expression for $q(\mathbf{r} \mid \mathbf{u})$ has both $\mathbf{u}$
and covariance matrices in it, $\mathbf{r}$ depends on both $\mathbf{u}$ and the
parameters of the kernel, $\bm\lambda$. The two remaining dependencies stem from
the fact that the approximating distribution for $\mathbf{u}$ is
$\mathcal{N}(\bm\mu, \mathbf{BB}^\intercal)$.

As we want to restrict some parameters (namely, $\bm\lambda$ and the diagonal of
$\mathbf{B}$) to positive values, we express them as exponentials and later
adjust their derivatives accordingly. Specifically, we can set $\lambda_i =
e^{\lambda'_i}$ and optimise $\lambda'_i$ using the chain rule:
\[
  \frac{\partial \mathcal{L}}{\partial \lambda'_i} = e^{\lambda'_i}
  \frac{\partial \mathcal{L}}{\partial \lambda_i}.
\]
This way, we restrict $\lambda_i$ to positive values while allowing $\lambda'_i$
to range over $\mathbb{R}$.

Finally, the parameters can be initialised as follows\footnote{In practice, it
  is often easier to start with a fixed $\mathbf{B} = \mathbf{I}_m$, as random
  values of $\mathbf{B}$ can often result in a near-singular $\bm\Sigma$.}:
\begin{alignat*}{3}
  \mu_i &\sim \mathcal{U}(0, 1) \quad &&\text{for } i = 1, \dots, m, \\
  \lambda_0 &\sim \chi^2_5, && \\
  \lambda_i &\sim \chi^2_1 \quad &&\text{for } i = 1, \dots, d, \\
  \diag(\mathbf{B}) &\sim \chi^2_4, && \\
  \text{the rest of } \mathbf{B} &\sim \mathcal{N}(0, 1). &&
\end{alignat*}
The initialisation of $\bm\mu$ mirrors the initialisation of $\mathbf{r}$ in
previous work by Levine et al. \cite{DBLP:conf/nips/LevinePK11}. In contrast,
while they have constant initial values for $\bm\lambda$, we sample from
$\chi^2$ distributions centred around those values ($5$ for $\lambda_0$ and $1$
for any other $\lambda_i$). The distributions for initial values of $\mathbf{B}$
are simply set to provide a reasonable spread of positive values for the
diagonal, and both positive and negative values for all other entries in the
matrix.

\subsection{Evidence Lower Bound} \label{sec:elbo}

It remains to express $\mathcal{L}$ for our (now fully specified) model. Note
that in order to keep the derivation simple, we drop all constant terms in the
expression of $\mathcal{L}$, i.e., equality is taken to mean `equality up to an
additive constant'.

Firstly, let us return to \eqref{eq:elbo} and write
\[
  \mathcal{L} = \mathbb{E}[\log\pfull] - \mathbb{E}[\log\approximation].
\]
By substituting in \eqref{eq:full} and \eqref{eq:approximation}, we get
\begin{align*}
  \mathcal{L} &= \mathbb{E}[\log p(\mathbf{u}) + \log p(\mathbf{r} \mid \mathbf{u}) + \log p(\mathcal{D} \mid \mathbf{r})] \\
              &- \mathbb{E}[\log q(\mathbf{u}) + \log q(\mathbf{r} \mid \mathbf{u})].
\end{align*}
Since $q(\mathbf{r} \mid \mathbf{u}) = p(\mathbf{r} \mid \mathbf{u})$, they
cancel each other out. Also notice that
\begin{align*}
  \mathbb{E}[\log p(\mathbf{u}) - \log q(\mathbf{u})] &= -\DKL{q(\mathbf{u})}{p(\mathbf{u})} \\
                                                      &= -\frac{1}{2} (\tr (\Kuu^{-1}\bm\Sigma) + \bm\mu^\intercal\Kuu^{-1}\bm\mu - m \\
                                                      &+ \log |\Kuu| - \log |\bm\Sigma|),
\end{align*}
by the definition of KL divergence between two multivariate Gaussians
\cite{kl}. Hence,
\begin{align*}
  \mathcal{L} &= \mathbb{E}\left[ \sum_{i=1}^N \sum_{t=1}^T Q_{\mathbf{r}}(s_{i,t}, a_{i,t}) - \V(s_{i,t}) \right] \\
              &- \frac{1}{2} \left(\tr \left( \Kuu^{-1}\bm\Sigma \right) + \bm\mu^\intercal\Kuu^{-1}\bm\mu + \log |\Kuu| - \log |\bm\Sigma| \right).
\end{align*}
Using the expressions for $Q_{\mathbf{r}}$ we get
\begin{align*}
  \mathcal{L} &= \mathbb{E}\left[\sum_{i=1}^N \sum_{t=1}^T r(s_{i,t}) - \V(s_{i,t}) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\V(s') \right] \\
              &- \frac{1}{2} \left(\tr \left( \Kuu^{-1}\bm\Sigma \right) + \bm\mu^\intercal\Kuu^{-1}\bm\mu + \log |\Kuu| - \log |\bm\Sigma| \right).
\end{align*}
We can simplify $\sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t})$ by defining a new vector
$\mathbf{t} = (t_1, \dots, t_{|\mathcal{S}|})^\intercal$, where $t_i$ is the
number of times the state associated with reward $r_i$ has been visited
across all demonstrations. Then,
\begin{align*}
  \mathbb{E} \left[ \sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t}) \right] &= \mathbb{E}[\mathbf{t}^\intercal\mathbf{r}] = \mathbf{t}^\intercal\mathbb{E}[\mathbf{r}] \\
                                                               &= \mathbf{t}^\intercal\mathbb{E}\left[\Kru^\intercal\Kuu^{-1}\mathbf{u}\right] = \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu.
\end{align*}
This allows us to simplify $\mathcal{L}$ to
\begin{align*}
  \mathcal{L} &= \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu - \mathbb{E}[v] \\
              &- \frac{1}{2} \left(\tr \left( \Kuu^{-1}\bm\Sigma \right) + \bm\mu^\intercal\Kuu^{-1}\bm\mu + \log |\Kuu| - \log |\bm\Sigma| \right),
\end{align*}
where
\[
  v = \sum_{i=1}^N \sum_{t=1}^T \V(s_{i,t}) - \gamma\sum_{s' \in \mathcal{S}}
  \mathcal{T}(s_{i,t}, a_{i,t}, s')\V(s').
\]

\section{Theoretical Justification} \label{sec:proof}
% TODO: Possible improvements:
% 1. Use limits more often.
% 2. Perturbation lemma can be replaced by continuity of A |-> A^-1.
% 3. epsilon is treated as an arbitrary constant.

The typical way to optimise a quantity (the ELBO, in this case) involves
computing its gradient. Unfortunately, the term $\mathbb{E}[v]$ in $\mathcal{L}$
complicates the situation. The goal of this section is to show how Theorem
\ref{thm:lebesgue} can be applied to our model in order to derive the gradient
anyway\footnote{This technique is inspired by black box VI
  \cite{DBLP:conf/aistats/RanganathGB14}, but takes a more detailed look at the
  problem and requires significantly more work to prove correctness.}. After
showing that the theorem applies to our situation, we can estimate
$\nabla\mathbb{E}[v]$ with
\begin{align*}
  \nabla\mathbb{E}[v] &= \nabla \iint v q(\mathbf{r} \mid \mathbf{u}) q(\mathbf{u})\dx \\
                      &= \iint \nabla[v q(\mathbf{r} \mid \mathbf{u}) q(\mathbf{u})]\dx \\
                      &= \iint \frac{\nabla[v q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})]}{q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})} q(\mathbf{r} \mid \mathbf{u}) q(\mathbf{u})\dx \\
                      &\approx \frac{1}{S} \sum_{s=1}^S \frac{\nabla[v q(\mathbf{r}_s \mid \mathbf{u}_s)q(\mathbf{u}_s)]}{q(\mathbf{r}_s \mid \mathbf{u}_s)q(\mathbf{u}_s)},
\end{align*}
which can be computed by drawing $S$ samples $\{(\mathbf{u}_s,
\mathbf{r}_s)\}_{s=1}^S$ from $\approximation$.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node [rectangle, draw, fill=lightgray] (perturbation) at (2, 0) {\textsc{Perturbation Lemma}};
    \node [rectangle, draw, fill=lightgray] (lebesgue) at (-1, -3) {\textsc{Dominated Convergence Theorem}};
    \node [rectangle, draw] (derivatives) at (-2, 0) {\textsc{Derivatives of PDFs}};
    \node [rectangle, draw] (measurability) at (-3, -2) {\textsc{Measurability}};
    \node [rectangle, draw] (boundedness) at (3, -2) {\textsc{Boundedness}};
    \node [rectangle, draw] (lemma1) at (-2, -1) {\textsc{Lemma \ref{lemma:bound1}}};
    \node [rectangle, draw] (lemma2) at (0, -1) {\textsc{Lemma \ref{lemma:bound2}}};
    \node [rectangle, draw] (lemma3) at (2, -1) {\textsc{Lemma \ref{lemma:bound3}}};
    \node [rectangle, draw] (lemma4) at (3, -3) {\textsc{Lemma \ref{lemma:integral_of_r}}};
    \node [rectangle, draw] (theorem) at (0, -2) {\textsc{Theorem \ref{thm:main}}};

    \draw [->] (perturbation) edge (lemma3);
    \draw [->] (lebesgue) edge (theorem);
    \draw [->] (derivatives) edge (lemma1);
    \draw [->] (derivatives) edge (lemma2);
    \draw [->] (derivatives) edge (lemma3);
    \draw [->] (measurability) edge (theorem);
    \draw [->] (boundedness) edge (theorem);
    \draw [->] (lemma1) edge (theorem);
    \draw [->] (lemma2) edge (theorem);
    \draw [->] (lemma3) edge (theorem);
    \draw [->] (lemma4) edge (theorem);
  \end{tikzpicture}
  \caption{A graphical representation of dependencies between our theoretical
    results. An arrow from $A$ to $B$ means that $A$ was used to prove $B$.
    Results from the literature are in gray.}
  \label{fig:theorems}
\end{figure}

Our main goal is Theorem \ref{thm:main}, which allows us to move differentiation
inside the integral. In order to prove it, we use a number of intermediate
results. We start by stating a few derivatives of PDFs and covariance matrices,
and bound their values with some easy-to-deal-with polynomials. We then provide
a sketch proof of the measurability of MDP value functions as well as new upper
and lower bounds on their values. After another quick lemma, we tackle the main
proof of this paper. See Figure \ref{fig:theorems} for an overview of how these
results fit together.

Before that, however, we define a few extra variables in order to simplify
expressions of derivatives:
\begin{align*}
  \mathbf{U} &= (\mathbf{u} - \bm\mu)(\mathbf{u} - \bm\mu)^\intercal, \\
  \mathbf{S} &= \Kru^\intercal\Kuu^{-1}, \\
  \bm\Gamma &= \Krr - \mathbf{S}\Kru, \\
  \mathbf{R} &= \mathbf{S}\frac{\partial \Kru}{\partial \lambda_i} - \frac{\partial \Krr}{\partial \lambda_i} + \left( \frac{\partial \Kru^\intercal}{\partial \lambda_i} - \mathbf{S}\frac{\partial \Kuu}{\partial \lambda_i} \right) \Kuu^{-1}\Kru, \\
  Q &= (\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu).
\end{align*}
Also note that throughout this section the word `constant' means `constant with
respect to $\mathbf{u}$ and $\mathbf{r}$'.

\begin{restatable}[Derivatives of PDFs]{lemma}{derivatives} \label{lemma:derivatives}
  \leavevmode
  \begin{enumerate}
  \item $\frac{\partial q(\mathbf{u})}{\partial \bm\mu} =
    \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u}
    - \bm\mu)$.
  \item
    \begin{enumerate}
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} =
      \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
      \bm\Sigma^{-1})$.
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
      q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
      \bm\Sigma^{-1})\mathbf{B}$.
    \end{enumerate}
  \item For $i = 0, \dots, d$,
    \begin{enumerate}
    \item
      \begin{align*}
        \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i} &= \frac{1}{2}q(\mathbf{r} \mid \mathbf{u}) (|\bm\Gamma|^{-1} \tr(\mathbf{R} \adj(\bm\Gamma)) \\
                                                                          &- (\mathbf{r} - \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r} - \mathbf{Su})).
      \end{align*}
    \item For any covariance matrix $\mathbf{K}$,
      \[
        \frac{\partial \mathbf{K}}{\partial \lambda_i} =
        \begin{cases}
          \frac{1}{\lambda_i}\mathbf{K} & \text{if } i = 0, \\
          \mathbf{L} & \text{otherwise,}
        \end{cases}
      \]
      where
      \[
        L_{j,k} = k(\mathbf{x}_j, \mathbf{x}_k) \left( -\frac{1}{2}(x_{j,i} -
          x_{k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
      \]
    \end{enumerate}
  \end{enumerate}
\end{restatable}

\begin{lemma} \label{lemma:bound1}
  Let $i \in \{ 0, \dots, d \}$ be arbitrary, and let $c :
  \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (\lambda_i - \epsilon,
  \lambda_i + \epsilon) \subset \mathbb{R}$ be a function with a codomain
  arbitrarily close to $\lambda_i$. Then
  \[
    \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i}
    \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{r} \mid
  \mathbf{u})d(\mathbf{u})$, where $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Let $\mathbf{K}$ be any covariance matrix and set
  \[
    \mathbf{A} = \frac{1}{\lambda_0} \mathbf{K}.
  \]

  % K and its derivative
  First, we can easily deduce that\footnote{Note that we often switch between
    proving limits and bounds. We are fundamentally interested in proving
    constant bounds, but sometimes it is more convenient to show convergence
    instead. In this situation, convergence as $\epsilon \to 0$ implies constant
    upper and lower bounds.}
  \begin{equation} \label{eq:convergence1}
    \mathbf{K}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} \to \mathbf{K}
  \end{equation}
  and
  \begin{equation} \label{eq:convergence2}
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_i} \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} \to \frac{\partial \mathbf{K}}{\partial \lambda_i}
  \end{equation}
  as $\epsilon \to 0$. For \eqref{eq:convergence1}, the result is obvious if $i
  = 0$. Otherwise, it follows from the continuity of the exponential function.
  For \eqref{eq:convergence2}, observe that if $i=0$, then
  \[
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_0} \right|_{\lambda_0 =
      c(\mathbf{r}, \mathbf{u})}
  \]
  as an expression contains no $\lambda_0$, so
  \[
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_0} \right|_{\lambda_0 =
      c(\mathbf{r}, \mathbf{u})} = \frac{\partial \mathbf{K}}{\partial
      \lambda_0}.
  \]
  Finally, if $i>0$, then each element of 
  \[
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_i}
    \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}
  \]
  is a constant multiple of the corresponding element of
  \[
    \mathbf{K}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})},
  \]
  so the same reasoning applies as in case of \eqref{eq:convergence1}.

  % Inverse of a covariance matrix
  Next, we will show that $\mathbf{K}^{-1}|_{\lambda_i = c(\mathbf{r},
    \mathbf{u})}$ exists and
  \begin{equation} \label{eq:limit3}
    \lim_{\epsilon \to 0} \mathbf{K}^{-1}|_{\lambda_i = c(\mathbf{r},
      \mathbf{u})} = \mathbf{K}^{-1}.
  \end{equation}
  If $i = 0$, then $\mathbf{K}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
  c(\mathbf{r}, \mathbf{u})\mathbf{A}$. Therefore\footnote{Note that since
    $\lambda_0 > 0$, $c(\mathbf{r}, \mathbf{u}) \ne 0$ for small enough
    $\epsilon$.},
  \begin{align*}
    \mathbf{K}^{-1}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} &=
                                                               \frac{1}{c(\mathbf{r}, \mathbf{u})}\mathbf{A}^{-1} \\
                                                             &\to \frac{1}{\lambda_0}\mathbf{A}^{-1} = \frac{1}{\lambda_0} \left( \frac{1}{\lambda_0}\mathbf{K} \right)^{-1} = \mathbf{K}^{-1}
  \end{align*}
  as $\epsilon \to 0$. For $i > 0$, by \eqref{eq:convergence1} and continuity of
  $\mathbf{A} \mapsto \mathbf{A}^{-1}$ we immediate get \eqref{eq:limit3}.

  This is enough to prove constant upper and lower bounds on
  $\mathbf{S}$, $\bm\Gamma$, and $\mathbf{R}$ (all with $\lambda_i$ replaced
  with $c(\mathbf{r}, \mathbf{u})$), which means that $(\mathbf{r} -
  \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r} -
  \mathbf{Su})|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}$ has upper and lower
  bounds in $\mathbb{R}_2[\mathbf{u}]$. Furthermore, having convergence results
  for arbitrary covariance matrices and their inverses means that
  \begin{equation} \label{eq:gamma_convergence}
    \lim_{\epsilon \to 0} \bm\Gamma|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
    \bm\Gamma
  \end{equation}
  and, by continuity of the determinant function,
  \begin{equation} \label{eq:det_gamma}
    \lim_{\epsilon \to 0} \det(\bm\Gamma)|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
    \det(\bm\Gamma).
  \end{equation}
  Assuming that
  $\bm\Gamma$ is invertible so that $q(\mathbf{r} \mid \mathbf{u})$ exists,
  \[
    \det(\bm\Gamma)|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} \ne 0
  \]
  for small enough $\epsilon$, and, thus, $\det(\bm\Gamma)^{-1}|_{\lambda_i =
    c(\mathbf{r}, \mathbf{u})}$ exists and is bounded.
  
  From \eqref{eq:gamma_convergence} and continuity of $\mathbf{A} \mapsto
  \mathbf{A}^{-1}$ we can immediately deduce that
  \begin{equation} \label{eq:gamma_inverse}
    \lim_{\epsilon \to 0} \bm\Gamma^{-1}|_{\lambda_i = c(\mathbf{r},
      \mathbf{u})} = \bm\Gamma^{-1}.
  \end{equation}
  Similarly, from \eqref{eq:det_gamma} and \eqref{eq:gamma_inverse} we get that
  \[
    \adj(\bm\Gamma)|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
    \det(\bm\Gamma)\bm\Gamma^{-1}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} \to
    \det(\bm\Gamma)\bm\Gamma^{-1}
  \]
  as $\epsilon \to 0$. This gives us constant bounds on
  \[
    |\bm\Gamma|^{-1}\tr(\mathbf{R}\adj(\bm\Gamma))|_{\lambda_i = c(\mathbf{r},
      \mathbf{u})},
  \]
  which completes the proof that
  \[
    \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i}
    \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}
  \]
  has the required quadratic bounds.
\end{proof}

\begin{remark}
  In order to find a derivative such as $\frac{\partial q(\mathbf{u})}{\partial
    \mu_i}$, we can find $\frac{\partial q(\mathbf{u})}{\partial \bm\mu}$ and
  simply take the $i$th element. A similar line of reasoning applies to matrices
  as well. Thus, we only need to consider derivatives with respect to $\bm\mu$
  and $\bm\Sigma$.
\end{remark}

\begin{lemma} \label{lemma:bound2}
  Let $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (a, b) \subset
  \mathbb{R}$ be an arbitrary bounded function. Then, for $i = 1, \dots, m$,
  every element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$,
  where $d(\mathbf{u}) \in \mathbb{R}_1[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma \ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})} = \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
    \bm\Sigma^{-\intercal})(\mathbf{u} - \mathbf{c}(\mathbf{r}, \mathbf{u})),
  \]
  where $\mathbf{c}(\mathbf{r}, \mathbf{u}) = (\mu_1, \dots, \mu_{i - 1},
  c(\mathbf{r}, \mathbf{u}), \mu_{i + 1} \dots, \mu_m)^\intercal$. Since
  $c(\mathbf{r}, \mathbf{u})$ is bounded and $\bm\Sigma^{-1} +
  \bm\Sigma^{-\intercal}$ is a constant matrix, we can use the bounds on
  $c(\mathbf{r}, \mathbf{u})$ to manufacture both upper and lower bounds on
  \[
     \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  of the required form.
\end{proof}

\begin{lemma} \label{lemma:bound3}
  Let $i, j = 1, \dots, m$, and let
  \[
    c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (\Sigma_{i,j} - \epsilon,
    \Sigma_{i,j} + \epsilon) \subset \mathbb{R}
  \]
  be a function with a codomain arbitrarily close to $\Sigma_{i,j}$. Then every
  element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} \right|_{\Sigma_{i,j} =
    c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$, where
  $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma \ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} \right|_{\Sigma_{i,j} =
    c(\mathbf{r}, \mathbf{u})} =
    \frac{1}{2}q(\mathbf{u})(\mathbf{C}(\mathbf{r},
    \mathbf{u})^{-1}\mathbf{UC}(\mathbf{r}, \mathbf{u})^{-1} -
    \mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}),
  \]
  where
  \[
    [\mathbf{C}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) & \text{if } (k, l) = (i, j), \\
      \Sigma_{k,l} & \text{otherwise.}
    \end{cases}
  \]
  We can also express $\mathbf{C}(\mathbf{r},\mathbf{u})$ as
  $\mathbf{C}(\mathbf{r}, \mathbf{u}) = \bm\Sigma + \mathbf{E}(\mathbf{r},
  \mathbf{u})$, where
  \[
    [\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) - \Sigma_{i,j} & \text{if } (k, l) = (i, j), \\
      0 & \text{otherwise.}
    \end{cases}
  \]
  We will show how we can establish bounds on $\mathbf{C}(\mathbf{r},
  \mathbf{u})^{-1}$ without using the continuity of matrix inversion. For this,
  we use the maximum norm $\lVert \cdot \rVert_\infty$ on both vectors and
  matrices. We can apply Lemma \ref{lemma:perturbation} to $\bm\Sigma$ and
  $\mathbf{E}(\mathbf{r}, \mathbf{u})$ since
  \[
    \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty = \max_k \sum_l
    |[\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l}| = |c(\mathbf{r}, \mathbf{u}) -
    \Sigma_{i,j}| < \epsilon \to 0,
  \]
  ensuring that $\lVert \bm\Sigma^{-1} \rVert_\infty
  \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty < 1$. Then
  $\mathbf{C}(\mathbf{r}, \mathbf{u})$ is invertible, and
  \[
    \lVert \mathbf{C}(\mathbf{r}, \mathbf{u})^{-1} \rVert_\infty \le
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \lVert \bm\Sigma^{-1}
      \rVert_\infty \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty} <
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \epsilon \lVert
      \bm\Sigma^{-1} \rVert_\infty},
  \]
  which means that
  \[
    \max_k \sum_l \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right|
    < \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \epsilon \lVert
      \bm\Sigma^{-1} \rVert_\infty},
  \]
  i.e., for any row $k$ and column $l$,
  \[
    \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right| <
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \epsilon \lVert
      \bm\Sigma^{-1} \rVert_\infty},
  \]
  which bounds all elements of $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ as
  required. Since every element of $\mathbf{U} = (\mathbf{u} - \bm\mu)(\mathbf{u} -
  \bm\mu)^\intercal$ is in $\mathbb{R}_2[\mathbf{u}]$, and the elements of
  $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ are bounded, the desired result
  follows.
\end{proof}

\begin{remark}
  MDP values are characterised by both a state and a reward function/vector. For
  the rest of this section, we will think of the value function as $V :
  \mathcal{S} \to \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$, i.e., $V$ takes a
  state $s \in \mathcal{S}$ and returns a function $V(s) :
  \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$ that takes a reward vector
  $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$ and returns a value of the state
  $s$, $\V(s) \in \mathbb{R}$. Given a reward vector, the function $V(s)$
  computes the values of all states and returns the value of state $s$.
\end{remark}

\begin{proposition}[Measurability] \label{thm:measurability}
  MDP value functions $V(s) : \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$ (for $s
  \in \mathcal{S}$) are Lebesgue measurable.
\end{proposition}
\begin{proof}
  For any reward vector $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$, the
  set of converged value functions $\{ \V(s) \mid s \in
  \mathcal{S} \}$ satisfy
  \begin{equation} \label{eq:linear_mdp}
    \V(s) = \log \sum_{a \in \mathcal{A}}
    \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
      s')\V(s') \right)
  \end{equation}
  for all $s \in \mathcal{S}$. Let $s_0 \in \mathcal{S}$ be an arbitrary state.
  In order to prove that $V(s_0)$ is measurable, it is enough to show that for
  any $\alpha \in \mathbb{R}$, the set
  \[
    \left\{ \mathbf{r} \in \mathbb{R}^{|\mathcal{S}|} \; \middle|
    \begin{aligned}
      \;&\V(s_0) \in (-\infty, \alpha); \\
      \;&\V(s) \in \mathbb{R} \text{ for all } s \in \mathcal{S} \setminus \{ s_0 \}; \\
      \;&\text{\eqref{eq:linear_mdp} is satisfied by all } s \in
      \mathcal{S}
    \end{aligned}
    \right\}
  \]
  is measurable. Since this set can be constructed in Zermelo-Fraenkel set
  theory \emph{without} the axiom of choice, it is measurable
  \cite{herrlich2006axiom}, which proves that $V(s)$ is a measurable function
  for any $s \in \mathcal{S}$.
\end{proof}

\begin{proposition}[Boundedness] \label{thm:bound}
  If the initial values of the MDP value function satisfy the following
  bound, then the bound remains satisfied throughout value iteration:
  \begin{equation} \label{eq:bound}
    |\V(s)| \le \vbound.
  \end{equation}
\end{proposition}
\begin{proof}
  We begin by considering \eqref{eq:bound} without taking the absolute value of
  $\V(s)$, i.e.,
  \begin{equation} \label{eq:positive_bound}
    \V(s) \le \vbound,
  \end{equation}
  and assuming that the initial values of $\{ \V(s) \mid s \in \mathcal{S} \}$
  already satisfy \eqref{eq:positive_bound}. Recall that for each $s \in
  \mathcal{S}$, the value of $\V(s)$ is updated by applying
  \eqref{eq:update_rule}. Note that both the natural logarithm and the
  exponential function are increasing, $\gamma > 0$, and the $\mathcal{T}$
  function gives a probability (a non-negative number). Thus,
  \begin{align*}
    \V(s) &\le \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s')\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma} \right) \\
          &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma}\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s') \right) \\
          &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right)
  \end{align*}
  by the definition of $\mathcal{T}$. Then
  \begin{align*}
    \V(s) &\le \log \left( |\mathcal{A}| \exp\left( r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
          &= \log \left( \exp\left( \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
          &= \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \\
          &= \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 - \gamma)(\log|\mathcal{A}| + r(s))}{1 - \gamma} \\
          &\le \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 - \gamma)(\log|\mathcal{A}| + \rinf)}{1 - \gamma} \\
          &= \vbound
  \end{align*}
  by the definition of $\rinf$.

  The proof for
  \begin{equation} \label{eq:negative_bound}
    \V(s) \ge \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1}
  \end{equation}
  follows the same argument until we get to
  \begin{align*}
    \V(s) &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma - 1)(\log|\mathcal{A}| + r(s))}{\gamma - 1} \\
          &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma - 1)(-\log|\mathcal{A}| -\rinf)}{\gamma - 1} \\
          &= \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1},
  \end{align*}
  where we use the fact that $r(s) \ge -\rinf - 2\log|\mathcal{A}|$. Combining
  \eqref{eq:positive_bound} and \eqref{eq:negative_bound} gives
  \eqref{eq:bound}.
\end{proof}

\begin{lemma} \label{lemma:integral_of_r}
  \[
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r} \mid \mathbf{u})\,d\mathbf{r} \le a +
    \lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1,
  \]
  where $a$ is a constant independent of $\mathbf{u}$.
\end{lemma}
\begin{proof}
  Since $\rinf \le \lVert \mathbf{r} \rVert_1$,
  \[
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r} \mid \mathbf{u})\,d\mathbf{r} \le \int
    \lVert \mathbf{r} \rVert_1 q(\mathbf{r} \mid \mathbf{u})\,d\mathbf{r} =
    \sum_{i=1}^{|\mathcal{S}|} \mathbb{E}[|r_i|].
  \]
  As each $\mathbb{E}[|r_i|]$ is a mean of a folded Gaussian distribution,
  \[
    \mathbb{E}[|r_i|] = \sigma_i \sqrt{\frac{2}{\pi}} \exp
    \left(-\frac{\xi_i^2}{2\sigma_i^2} \right) + \xi_i \left( 1 - 2\Phi \left(
        -\frac{\xi_1}{\sigma_1} \right) \right),
  \]
  where $\xi_i = \left[\Kru^\intercal\Kuu^{-1}\mathbf{u}\right]_i$, $\sigma_i =
  \sqrt{[\Krr - \Kru^\intercal\Kuu^{-1}\Kru]_{i,i}}$\footnote{The expression
    under the square root sign is non-negative because $\Krr -
    \Kru^\intercal\Kuu^{-1}\Kru$ is a covariance matrix of a Gaussian
    distribution, hence also positive semi-definite, which means that its
    diagonal entries are non-negative.}, and $\Phi$ is the cumulative
  distribution function of the standard Gaussian. Furthermore,
  \[
    \mathbb{E}[|r_i|] \le \sigma_i\sqrt{\frac{2}{\pi}} + |\xi_i|,
  \]
  as $\sigma_i$ is non-negative, and $\Phi(x) \in [0, 1]$ for all $x$. Since
  \[ \sum_{i=1}^{|\mathcal{S}|} |\xi_i| = \lVert \Kru^\intercal \Kuu^{-1}
    \mathbf{u} \rVert_1, \]
  we can set
  \[ a = \sum_{i=1}^{|\mathcal{S}|} \sigma_i \sqrt{\frac{2}{\pi}} \]
  to get the desired result.
\end{proof}

Our main theorem is a specialised version of an integral differentiation result
by Timoney \cite{lecture_notes}.
\begin{theorem} \label{thm:main}
  Whenever the derivative exists,
  \[
    \dt\iint
    \V(s)q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})\dx
    = \iint
    \dt[\V(s)q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})]\dx,
  \]
  where $t$ is any scalar part of $\bm\mu$, $\bm\Sigma$, or $\bm\lambda$.
\end{theorem}
\begin{proof}
  Let
  \begin{align*}
    \f &= \V(s)q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u}), \\
    F(t) &= \iint \f\dx,
  \end{align*}
  and fix the value of $t$. Let $(t_n)_{n=1}^\infty$ be any sequence such that
  $\lim_{n \to \infty} t_n = t$, but $t_n \ne t$ for all $n$. We want to show
  that
  \begin{equation} \label{eq:to_prove}
    F'(t) = \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \iint \df\dx.
  \end{equation}
  We have
  \[
    \begin{split}
      \frac{F(t_n) - F(t)}{t_n - t} &= \iint \frac{\ftn - \f}{t_n - t}\dx \\
      &= \iint \fn\dx,
    \end{split}
  \]
  where
  \[
    \fn = \frac{\ftn - \f}{t_n - t}.
  \]
  Since
  \[
    \lim_{n \to \infty} \fn = \df,
  \]
  \eqref{eq:to_prove} follows from Theorem \ref{thm:lebesgue} as soon as we show
  that both $f$ and $f_n$ are measurable and find a non-negative integrable
  function $g$ such that for all $n$, $\mathbf{r}$, $\mathbf{u}$,
  \[
    |\fn| \le \g.
  \]
  The MDP value function is measurable by Proposition \ref{thm:measurability}.
  The result of multiplying or adding measurable functions (e.g., PDFs) to a
  measurable function is still measurable. Thus, both $f$ and $f_n$ are
  measurable.

  It remains to find $g$. For notational simplicity and without loss of
  generality, we will temporarily assume that $t$ is a parameter of
  $q(\mathbf{r} \mid \mathbf{u})$. Then
  \[
    |\fn| = |\V(s)| \left| \frac{q(\mathbf{r} \mid \mathbf{u})|_{t =
          t_n} - q(\mathbf{r} \mid \mathbf{u})}{t_n - t} \right| q(\mathbf{u}),
  \]
  since PDFs are non-negative. An upper bound for
  $|\V(s)|$ is given by Proposition \ref{thm:bound}, while
  \[
    \frac{q(\mathbf{r} \mid \mathbf{u})|_{t = t_n} - q(\mathbf{r} \mid \mathbf{u})}{t_n - t} = \left.
      \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial t} \right|_{t = c(\mathbf{r},
      \mathbf{u})}
  \]
  for some function $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to
  (\min\{t, t_n\}, \max\{t, t_n\})$ due to the mean value theorem (since $q$ is
  a continuous and differentiable function of $t$, regardless of the specific
  choices of $q$ and $t$).

  We then have that
  \[
    |\fn| \le \vbound \left| \left. \frac{\partial
          q(\mathbf{r} \mid \mathbf{u})}{\partial t} \right|_{t=c(\mathbf{r}, \mathbf{u})}
    \right| q(\mathbf{u}).
  \]
  The bound is clearly non-negative and measurable. It remains to show that it
  is also integrable. Depending on what $t$ represents, we can use one of the
  Lemmas \ref{lemma:bound1}, \ref{lemma:bound2}, and \ref{lemma:bound3}, which
  gives us two polynomials $p_1(\mathbf{u}), p_2(\mathbf{u}) \in
  \mathbb{R}_2[\mathbf{u}]$ such that
  \[
    p_1(\mathbf{u})q(\mathbf{r} \mid \mathbf{u}) < \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial
        t} \right|_{t=c(\mathbf{r}, \mathbf{u})} < p_2(\mathbf{u})q(\mathbf{r} \mid \mathbf{u}).
  \]
  Then
  \[
    \left| \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial t}
      \right|_{t=c(\mathbf{r}, \mathbf{u})} \right| < q(\mathbf{r} \mid \mathbf{u}) \max \{
    |p_1(\mathbf{u})|, |p_2(\mathbf{u})| \}.
  \]
  We can now apply Lemma \ref{lemma:integral_of_r}, which allows us to integrate
  out $\mathbf{r}$, and we are left with showing the existence of
  \begin{equation} \label{eq:remaining_integral}
    \int \left( a + \lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1 \right) \max \{|p_1(\mathbf{u})|, |p_2(\mathbf{u})| \} q(\mathbf{u})\,d\mathbf{u},
  \end{equation}
  where $a$ is a constant. The integral
  \[
    \int \max \left\{
      \begin{aligned}
        &|p_1(\mathbf{u})|, \\
        &|p_2(\mathbf{u})|
      \end{aligned}
    \right\} q(\mathbf{u})\,d\mathbf{u} = \int \max \left\{
      \begin{aligned}
        &|p_1(\mathbf{u})q(\mathbf{u})|, \\
        &|p_2(\mathbf{u})q(\mathbf{u})|
      \end{aligned}
    \right\}\,d\mathbf{u}
  \]
  exists because $p_1(\mathbf{u})q(\mathbf{u})$ and
  $p_2(\mathbf{u})q(\mathbf{u})$ are both integrable, hence their absolute
  values are integrable, and the maximum of two integrable functions is also
  integrable. Since $\lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1 \in
  \mathbb{R}_1[\mathbf{u}]$, a similar argument can be applied to the rest of
  \eqref{eq:remaining_integral} as well.
\end{proof}

\section{Evaluation} \label{sec:evaluation}
% showing deep understanding

In order to fully understand the model's behaviour, we focus on a three-state
MDP where the agent can deterministically move from any state to any other
state. More formally, we set $\mathcal{S} = \{ s_1, s_2, s_3 \}$, $\mathcal{A} =
\{ a_1, a_2 \}$,
\begin{align*}
  &\mathcal{T}(s_1, a_1, s_2) = 1, \quad \mathcal{T}(s_1, a_2, s_3) = 1, \\
  &\mathcal{T}(s_2, a_1, s_1) = 1, \quad \mathcal{T}(s_1, a_2, s_3) = 1, \\
  &\mathcal{T}(s_3, a_1, s_1) = 1, \quad \mathcal{T}(s_1, a_2, s_2) = 1,
\end{align*}
all other values of $\mathcal{T}$ to zero, and $\gamma = 0.9$. We
also set the inducing points to be equal to the three states in $\mathcal{S}$,
add a single feature $f : \mathcal{S} \to \mathbb{R}$ such that
\[
  f(s_1) = 1, \quad f(s_2) = 2, \quad f(s_3) = 3,
\]
and create two demonstrations $\zeta_1 = \{ (s_1, a_1) \}$ and $\zeta_2 = \{
(s_3, a_2) \}$ that correspond to moving from $s_1$ and $s_3$ to $s_2$.
Therefore, we would expect the reward of $s_2$ to be higher than the other two
rewards in order to reflect this.

\begin{figure*}
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=0.8\columnwidth]{sage1}
    \includegraphics[width=0.8\columnwidth]{sage2}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=0.8\columnwidth]{elbo_over_B_short}
  \end{minipage}
  \caption{How a well-defined problem with a correct solutions becomes a
    well-defined problem with an incorrect solution. In each plot, the function
    we are trying to optimise is in blue, and its derivative is in red. The
    plots on the left are for $q(\mathbf{u})$ and its derivatives with respect
    to $B_{1,1}$ (at the top) and $B_{2,1}$ (at the bottom), whereas the plots
    on the right are for $\mathbb{E}[v]$ and its corresponding derivatives.}
  \label{fig:B_gradient}
\end{figure*}

\begin{figure*}
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=0.8\columnwidth]{convergence1}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=0.8\columnwidth]{convergence2}
  \end{minipage}
  \caption{The convergence of $\mathcal{L}$ (at the top) and several example
    policies (at the bottom) over a number of iterations for Scenario~1 on the
    left and Scenario~2 on the right.}
  \label{fig:convergence1}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{parameter_convergence1}
  \includegraphics[width=\textwidth]{parameter_convergence2}
  \caption{Convergence of all optimised parameters for Scenario~1 at the top and
  Scenario~2 at the bottom. In order to represent different variables on the
  same scale, some variables have been $\log$-transformed. Colours denote which
  vector or matrix each scalar comes from: black for $\bm\lambda$,
  \textcolor{blue}{blue} for $\bm\mu$, \textcolor{red}{red} for diagonal
  elements of $\mathbf{B}$, and \textcolor{green}{green} for its non-diagonal
  elements.}
  \label{fig:convergence2}
\end{figure*}

Unfortunately, we are not able to use $\frac{\partial \mathcal{L}}{\partial
  \mathbf{B}}$ in order to optimise $\mathbf{B}$. We illustrate the problem in
Figure \ref{fig:B_gradient}. On the left side of the figure, we plot how
$q(\mathbf{u})$ behaves as a function of a diagonal and a non-diagonal element
of $\mathbf{B}$. Both functions have maximum values that can be attained by
following the corresponding derivatives. However, when these derivatives are
used to estimate $\frac{\partial \mathbb{E}[v]}{\partial \mathbf{B}}$, the
resulting derivatives no longer match their corresponding functions, although
the functions themselves still have optimal values: at or below $1$ for
$B_{1,1}$ and at $0$ for $B_{2,1}$. This leads us to consider two restrictions
of the model in order to investigate its convergence behaviour:
\begin{itemize}
\item In \textbf{Scenario 1}, we remove $\frac{\partial \mathbb{E}[v]}{\partial
    \mathbf{B}}$ from $\frac{\partial \mathcal{L}}{\partial \mathbf{B}}$. This
  essentially optimises $\mathbf{B}$ to match $\Kuu$, because then
  $\frac{\partial \mathcal{L}}{\partial \mathbf{B}}$ becomes
  $-\frac{\partial}{\partial \mathbf{B}} \DKL{q(\mathbf{u})}{p(\mathbf{u})}$,
  i.e., we are optimising $\mathbf{B}$ to minimise the difference between the
  prior and the posterior of $\mathbf{u}$.
\item In \textbf{Scenario 2}, we set $\mathbf{B} = \mathbf{I}_m$, and do not
  optimise it at all.
\end{itemize}

We plot how $\mathcal{L}$ as well as policies $\pi(a_1 \mid s_1)$, $\pi(a_2 \mid
s_3)$, and $\pi(a_1 \mid s_2)$ converge over a number of iterations in Figure
\ref{fig:convergence1}. The first two policies correspond to actions taken in
the set of demonstrations $\mathcal{D}$, so we would expect to see their
probabilities converge to values above $0.5$. Note that due to the stochastic
nature of our MDP model, we do not expect to see any probability reach exactly
$1$. The third policy, however, has no relevant data in $\mathcal{D}$, so the
maximum causal entropy framework would put the probability at around $0.5$.

As we ran the algorithm with two stopping conditions---$300$ iterations and the
$\ell_1$-norm of the change in parameter values being below $0.01$---note that
the algorithm terminated early in Scenario~2 but not in Scenario~1, although
there is clear convergence in both cases. An important difference is that
$\pi(a_1 \mid s_1)$ and $\pi(a_2 \mid s_3)$ converge closer and closer to $1$
with the slightly-restricted model of Scenario~1, but stabilise at $0.6$ in
Scenario~2 due to an important part of the model being completely fixed. In both
cases, $\pi(a_1 \mid s_2)$ converges to $0.5$ as expected.

Figure \ref{fig:convergence2} shows how the parameters of the model converge in
both scenarios. The algorithm seems to converge fine in Scenario~2, but most of
the parameters fail to stabilise in Scenario~1. Most importantly, the diagonal
values of the $\mathbf{B}$ matrix diverge to positive infinity, leading to higher
variance in ELBO estimates seen in Figure \ref{fig:convergence1}. The
non-diagonal values, however, converge just fine. Both $\lambda_0$ and
$\lambda_1$ also diverge to positive infinity, while the elements of $\bm\mu$
diverge, but in `the right' direction: as $\mu_2$ increases while $\mu_1$ and
$\mu_3$ decrease, the policies in Figure \ref{fig:convergence1} converge to
their optimal values.

This leaves us with two models: one that converges to reasonable-but-suboptimal
values, and one that diverges to infinite variance but also provides correct
policies.

\begin{figure*}
  \begin{tikzpicture}[
    /pgfplots/colormap/viridis,
    ellipse2/.style={
      draw,
      ellipse,
      /utils/exec={
        \pgfplotscolormapdefinemappedcolor{#1}},
      fill=mapped color},
    edge2/.style={
      ultra thick,
      /utils/exec={
        \pgfplotscolormapdefinemappedcolor{#1}},
      draw=mapped color}]
    \csvreader[
    no head,
    column count=11
    ]{covariance_and_policy_transformed.csv}{1=\x, 2=\y, 3=\Soneone, 4=\Stwotwo, 5=\Sthreethree, 6=\Stwoone, 7=\Sthreeone, 8=\Sthreetwo, 9=\ponetwo, 10=\ptwoone, 11=\pthreetwo}{%
      \node[ellipse2=\Soneone] (s1) at (3*\x, 2*\y) {$s_1$};
      \node[ellipse2=\Stwotwo] (s2) at (3*\x+1, 2*\y+1) {$s_2$};
      \node[ellipse2=\Sthreethree] (s3) at (3*\x+2, 2*\y) {$s_3$};
      \draw[edge2=\Stwoone] (s1) -- (s2);
      \draw[edge2=\Sthreeone] (s1) to (s3);
      \draw[edge2=\Sthreetwo] (s2) to (s3);
    }
    \draw [<->] (2.5, 7.5) node (yaxis) [above] {$\zeta_2$}
    |- (11.5, 1.5) node (xaxis) [right] {$\zeta_1$};
    \foreach \x in {1, 2, 3} {
      \node (x\x) at (1+3*\x, 1.3) {$\x$};
    }
    \foreach \y in {1, 2, 3} {
      \node (y\y) at (2.3, 0.5+2*\y) {$\y$};
    }
  \end{tikzpicture}
  \begin{tikzpicture}
    \csvreader[
    no head,
    column count=11
    ]{covariance_and_policy_transformed.csv}{1=\x, 2=\y, 3=\Soneone, 4=\Stwotwo, 5=\Sthreethree, 6=\Stwoone, 7=\Sthreeone, 8=\Sthreetwo, 9=\ponetwo, 10=\ptwoone, 11=\pthreetwo}{%
      \begin{axis}[
        ybar=0,
        symbolic x coords={a,b,c},
        ticks=none,
        ymax=1,
        ymin=0,
        x=20,
        bar width=20,
        enlarge x limits=0.25,
        at={(350*\x, 110*\y)},
        height=100,
        bar shift=0pt,
        %legend pos=outer south,
        legend style={
          legend columns=-1,
          at={(1.7,-0.4)}
        }
        ]
        \addplot coordinates {(a, \ponetwo)};
        \addplot coordinates {(b, \ptwoone)};
        \addplot coordinates {(c, \pthreetwo)};
        \draw[dashed] (-50,50) -- (270,50);
        \ifthenelse{\x=2 \AND \y=1}{
          \legend{$\pione$,$\pitwo$,$\pithree$};
        }{}
      \end{axis}
    }
    \draw [<->] (2.3, 8.5) node (yaxis) [above] {$\zeta_2$}
    |- (10, 2) node (xaxis) [right] {$\zeta_1$};
    \foreach \x in {1, 2, 3} {
      \node (x\x) at (1+2.5*\x, 1.7) {$\x$};
    }
    \foreach \y in {1, 2, 3} {
      \node (y\y) at (2.1, 1+2.1*\y) {$\y$};
    }
  \end{tikzpicture}
\end{figure*}

\section{Related Work} % 0.5-1 page

The IRL problem itself was originally proposed by Russell in 1998
\cite{DBLP:conf/colt/Russell98}. Most of the early approaches had the
aforementioned reward linearity assumption. One of the first papers on the
subject by Ng and Russell \cite{DBLP:conf/icml/NgR00} introduced several linear
programming algorithms and identified an important issue: there are typically
many reward functions that can explain the data equally well. This problem was
solved by Ziebart et al. \cite{ziebart2008maximum} with the introduction of IRL
based on the principles of maximum causal entropy in a linearly-solvable MDP.

Levine et al. \cite{DBLP:conf/nips/LevinePK11} were the first to lift the
linearity assumption without imposing additional restrictions on the problem.
They do, however, model rewards as having no variance---our work removes this
restriction without any compromises.

Recently, Jin et al. \cite{DBLP:conf/uai/JinDAS17} have adapted the model
proposed by Levine et al. \cite{DBLP:conf/nips/LevinePK11} to use deep GPs,
harnessing the power of deep learning to make the model less dependent on what
features are provided. Although they use VI, their approximating distribution
for rewards at inducing points is simply the Dirac $\delta$ function, which is
essentially equivalent to the assumption of no variance.

An alternative to GPs for modelling nonlinear functions is, of course, neural
networks. Wulfmeier et al. \cite{wulfmeier2015maximum} have shown how they can
be used in the IRL setting. While this approach benefits from constant-time
inference and the ability to learn complex features from data, neural
networks often need significantly more data for the weights across all layers to
stabilise.

\subsection{Variational Inference}
% none of these approaches really work in our situation because they tend to
% depend on properties of how GPs are used in regression that are no longer true
% in IRL

Since the focus of our work was on proving feasibility rather than ensuring
performance, we simply used a combination of Gaussians for our variational
approximation. However, while VI was initially focused on approximating
distributions using simplistic models where all variables are independent
\cite{blei2017variational}, the last few years have brought many advances in
approximating more complex distributions and greatly reducing the computational
complexity of the task. Adapting some of them to IRL is a nontrivial but highly
valuable undertaking.

For approximating complex distributions, Rezende and Mohamed
\cite{DBLP:conf/icml/RezendeM15} suggest using \emph{normalising flows}, i.e., a
collection of invertible functions---parametrised by additional variational
parameters---that are applied to latent variables. A major challenge in applying
their work to IRL is related to variational parameters being used to compute the
MDP value function, i.e., how can we take the derivative of the value function
with respect to such a variational parameter? Alternatively, perhaps one can
construct a different model that would make this question moot.

Another approach to flexibility in modelling could come from considering
different GP kernels. For instance, Wilson and Adams \cite{pmlr-v28-wilson13}
show how all \emph{stationary} (i.e., invariant to translations) kernels can be
generated (or at least approximated) from a mixture of Gaussians in their
spectral representation using Bochner's theorem
\cite{bochner1959lectures,DBLP:journals/technometrics/Woodard00}. It looks
promising to combine these kernels with the variational Fourier features
approach by Hensman et al. \cite{DBLP:journals/jmlr/HensmanDS17} that leverages
the same spectral representations for efficient VI.

\section{Conclusion}
% TODO: write
% Show how we achieved our claimed contributions

\begin{itemize}
\item Why my theoretical contributions are important...
\item Reasonable results with convergence: policies do converge to optimal
  values, even at the extremes (i.e., $0$ and $1$).
\item Shown how to eliminate the deterministic training conditional
  assumption---a common weakness of previous approaches.
\item Being able to model rewards using full probability distributions without
  limiting assumptions opens up many new research directions.
\item My excuse for the buggy implementation...
\end{itemize}

\subsection{Further Work}

An interesting extension to our work would be to consider IRL in the context
of a reinforcement learning (RL) agent. Suppose we have an agent whose purpose
is to learn optimal behaviour from observing other agents using IRL. It could
then take reward variance estimates into account when choosing what states to
visit next. It would have to handle the balance between exploration and
exploitation similarly to many RL agents, but the information about rewards
would come from observing (presumably near-optimal) behaviour exhibited by other
agents rather than directly from the environment.

It is also worth noting the approach presented in this paper requires solving
$S$ MDPs for every iteration of optimising the parameters (where $S$ is the
number of samples drawn from $\approximation$). There are at least two ways to
reduce or eliminate this performance bottleneck:
\begin{itemize}
\item The MDP value function could be approximated, allowing for some minor
  mistakes in the resulting policy.
\item Perhaps there is a good way to use information about previously computed
  value functions for similar rewards to hasten the current computation. One
  simple way to do this would be by initialising the current values to the
  optimal values of the previous MDP value function computation.
\end{itemize}

Finally, building on the idea that variance estimates can be used to judge
whether the model has learned optimal policy, an interesting question for
MDP (or, perhaps, dynamical systems) research would be: how much does a reward
have to change in order to affect the deterministic policy? A simple answer to
this question would allow us to use variance estimates in order to quantify the
model's confidence regarding optimal behaviour.

%\vskip8pt \noindent
%{\bf Acknowledgments.}
%This is optional; it is a location for you to thank people
\bibliographystyle{abbrv}
\bibliography{paper}

\appendix
\section{Proofs} \label{appendix:proofs}

\derivatives*
\begin{proof}
  \leavevmode
  \begin{enumerate}
  \item
    \[
      \begin{split}
        \frac{\partial q(\mathbf{u})}{\partial m} &=
        q(\mathbf{u})\dm\left[-\frac{Q}{2}\right]
        \\
        &= -\frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
        \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu)\dm[\mathbf{u} -
        \bm\mu] \\
        &= \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
        \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu).
      \end{split}
    \]
  \item An online tool by Laue et
    al.\footnote{\url{http://www.matrixcalculus.org/}}
    \cite{DBLP:conf/nips/LaueMG18} can be used to find both derivatives.
  \item
    \begin{enumerate}
    \item Since
    \begin{align*}
      q(\mathbf{r} \mid \mathbf{u}) &= \mathcal{N}(\mathbf{r};
      \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr - \Kru^\intercal \Kuu^{-1}\Kru) \\
      &= \mathcal{N}(\mathbf{r}; \mathbf{Su}, \bm\Gamma),
    \end{align*}
    we have
    \[
      \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i} =
      -\frac{1}{2}q(\mathbf{r} \mid \mathbf{u}) \dl[(\mathbf{r} -
      \mathbf{Su})^\intercal\bm\Gamma^{-1}(\mathbf{r} - \mathbf{Su}) +
      \log|\bm\Gamma|].
    \]
    The same online tool can be used to show that
    \[
      \dl \log|\bm\Gamma| = -|\bm\Gamma|^{-1}\tr(\mathbf{R}\adj(\bm\Gamma)),
    \]
    and
    \[
      \dl \bm\Gamma^{-1} = \bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}.
    \]
  \item
    If $i=0$, then
    \[
      \frac{\partial \mathbf{K}}{\partial \lambda_i} =
      \frac{1}{\lambda_i}\mathbf{K}
    \]
    by the structure of each element of $\mathbf{K}$. If $i \ne 0$, then each
    element of $\frac{\partial \mathbf{K}}{\partial \lambda_i}$ is
    \begin{align*}
      L_{j,k} &= \frac{\partial k(\mathbf{x}_j, \mathbf{x}_k)}{\partial \lambda_i} \\
      &\begin{aligned}
        = k(\mathbf{x}_j, \mathbf{x}_k) \dl \left[ \vphantom{\frac{1}{2}} \right. &-\frac{1}{2}(\mathbf{x}_j - \mathbf{x}_k)^\intercal \bm\Lambda (\mathbf{x}_j - \mathbf{x}_k) \\
        &- \mathbbm{1}[j \ne k]\sigma^2\tr(\bm\Lambda) \left. \vphantom{\frac{1}{2}} \right]
      \end{aligned} \\
      &\begin{aligned}
        = k(\mathbf{x}_j, \mathbf{x}_k) \dl \left[ \vphantom{\sum_{k=1}^d} \right. &-\frac{1}{2} \sum_{l=1}^d \lambda_l (x_{j,l} - x_{k,l})^2 \\
        &- \mathbbm{1}[j \ne k]\sigma^2\sum_{l=1}^d \lambda_l \left. \vphantom{\sum_{l=1}^d} \right]
      \end{aligned} \\
      &= k(\mathbf{x}_j, \mathbf{x}_k) \left( -\frac{1}{2}(x_{j,i} - x_{k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
    \end{align*}
    \end{enumerate}
  \end{enumerate}
\end{proof}

\section{Derivatives of the ELBO}

\subsection{\texorpdfstring{$\partial/\partial\bm\mu$}{Derivative w.r.t. mu}}

We begin by removing terms independent of $\bm\mu$:
\[
  \frac{\partial\mathcal{L}}{\partial\bm\mu} =
  \dm[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu] - \frac{1}{2} \dm
  \left[ \bm\mu^\intercal \Kuu^{-1} \bm\mu \right] - \dm\mathbb{E}[v].
\]
Here
\[
  \dm \left[ \bm\mu^\intercal \Kuu^{-1} \bm\mu \right] = (\Kuu^{-1} +
  \Kuu^{-\intercal}) \bm\mu
\]
by Petersen and Pedersen \cite{petersen2008matrix}, and
\[
  \begin{split}
    \dm\mathbb{E}[\V(s)] &= \dm\iint \V(s) q(\mathbf{r} \mid \mathbf{u})
    q(\mathbf{u})\dx \\
    &= \iint \V(s) q(\mathbf{r} \mid \mathbf{u}) \frac{\partial
      q(\mathbf{u})}{\partial \bm\mu}\dx \\
    &= \frac{1}{2}\mathbb{E}[\V(s) (\bm\Sigma^{-1} +
    \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu)]
  \end{split}
\]
by Theorem \ref{thm:main} and Lemma \ref{lemma:derivatives}. Hence,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\bm\mu} &=
    \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1} - \frac{1}{2} (\Kuu^{-1} +
    \Kuu^{-\intercal}) \bm\mu \\
    &- \frac{1}{2}\mathbb{E} \left[(\bm\Sigma^{-1} +
      \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu) v \right].
  \end{split}
\]

\subsection{\texorpdfstring{$\partial/\partial\mathbf{B}$}{Derivative w.r.t. B}}

\[
  \frac{\partial\mathcal{L}}{\partial\mathbf{B}} =
  \frac{1}{2} \left( \dB\log|\bm\Sigma| - \dB\tr \left( \Kuu^{-1} \bm\Sigma
    \right) \right)
  - \dB\mathbb{E}[v].
\]
By Theorem \ref{thm:main},
\[
  \dB\mathbb{E}[\V(s)] = \iint \V(s) q(\mathbf{r} \mid \mathbf{u})
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{B}}\dx.
\]
Then, using the aforementioned tool by Laue et al.
\cite{DBLP:conf/nips/LaueMG18}, we get
\[
  \dB\log|\bm\Sigma| = 2\bm\Sigma^{-1}\mathbf{B}, \quad \dB \tr \left( \Kuu^{-1}
    \bm\Sigma \right) = 2\Kuu^{-1}\mathbf{B},
\]
and Lemma \ref{lemma:derivatives} gives
\begin{gather*}
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
  q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
  |\bm\Sigma|^{-1}\adj(\bm\Sigma))\mathbf{B}.
\end{gather*}
Therefore,
\begin{gather*}
  \frac{\partial \mathcal{L}}{\partial \mathbf{B}} =
  \left( \bm\Sigma^{-1} - \Kuu^{-1} \right) \mathbf{B} - \mathbb{E}
  [(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
  |\bm\Sigma|^{-1}\adj(\bm\Sigma))\mathbf{B}v].
\end{gather*}

\subsection{\texorpdfstring{$\partial/\partial \lambda_j$}{Derivative w.r.t.
    Lambda}}

For $j = 0, \dots, d$,
\[
  \begin{split}
    \frac{\partial \mathcal{L}}{\partial \lambda_j} &= \mathbf{t}^\intercal\dlj
    \left[ \Kru^\intercal\Kuu^{-1} \right] \bm\mu - \dlj\mathbb{E}[v] \\
    &- \frac{1}{2} \left(\dlj \tr \left(\Kuu^{-1}\bm\Sigma \right) +
      \bm\mu^\intercal \frac{\partial \Kuu^{-1}}{\partial \lambda_j} \bm\mu +
      \dlj \log |\Kuu| \right),
  \end{split}
\]
where
\begin{align*}
  \frac{\partial \Kuu^{-1}}{\partial \lambda_j} &= -\Kuu^{-1}\frac{\partial \Kuu}{\partial \lambda_j}\Kuu^{-1}, \\
  \dlj \left[ \Kru^\intercal\Kuu^{-1} \right] &= \frac{\partial \Kru^\intercal}{\partial \lambda_j} \Kuu^{-1} + \Kru^\intercal \frac{\partial \Kuu^{-1}}{\partial \lambda_j} \\
                                                &= \left( \frac{\partial \Kru^\intercal}{\partial \lambda_j} - \Kru^\intercal\Kuu^{-1}\frac{\partial \Kuu}{\partial \lambda_j} \right) \Kuu^{-1}, \\
  \dlj \tr(\Kuu^{-1}\bm\Sigma) &= \tr \left( \dlj \left[ \Kuu^{-1}\bm\Sigma \right] \right) = \tr \left( \frac{\partial \Kuu^{-1}}{\partial \lambda_j} \bm\Sigma \right) \\
                                                &= -\tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j} \Kuu^{-1} \bm\Sigma \right), \\
  \dlj\log|\Kuu| &= \tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j} \right)
\end{align*}
by Petersen and Pedersen \cite{petersen2008matrix}, and
\[
  \begin{split}
    \dlj \mathbb{E}[\V(s)] &= \iint\V(s)\frac{\partial q(\mathbf{r} \mid
      \mathbf{u})}{\partial \lambda_j}q(\mathbf{u})\dx \\
    &= \!\begin{multlined}[t]
      \frac{1}{2}\mathbb{E}[\V(s) (|\bm\Gamma|^{-1} \tr(\mathbf{R}
          \adj(\bm\Gamma)) \\
          - (\mathbf{r} -
          \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r}
          - \mathbf{Su}))]
    \end{multlined}
  \end{split}
\]
by Theorem \ref{thm:main} and Lemma \ref{lemma:derivatives}. Thus,
\begin{align*}
  \frac{\partial \mathcal{L}}{\partial \lambda_j} &=
  \mathbf{t}^\intercal \left( \frac{\partial \Kru^\intercal}{\partial
      \lambda_j} - \Kru^\intercal\Kuu^{-1}\frac{\partial
      \Kuu}{\partial \lambda_j} \right) \Kuu^{-1} \bm\mu \\
  &\begin{aligned}
    + \frac{1}{2} \left[ \vphantom{\frac{\partial \Kuu}{\partial \lambda_j}}
    \right. &\tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j}
      \Kuu^{-1} \bm\Sigma \right) + \bm\mu^\intercal \Kuu^{-1} \frac{\partial
      \Kuu}{\partial \lambda_j} \Kuu^{-1} \bm\mu \\
    &- \tr \left. \left(\Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j}
      \right) \right]
  \end{aligned} \\
  &\begin{aligned}
    - \frac{1}{2} \mathbb{E} [ (&|\bm\Gamma|^{-1} \tr(\mathbf{R} \adj(\bm\Gamma)) \\
    &- (\mathbf{r} - \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r} - \mathbf{Su})) v],
  \end{aligned} 
\end{align*}
where the remaining derivatives can be found in Lemma \ref{lemma:derivatives}.

\end{document}