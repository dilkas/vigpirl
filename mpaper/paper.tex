\documentclass{mpaper}
\usepackage{bm}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\tr}{tr}

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}

\begin{document} % 14 pages

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian Processes}
\author{Paulius Dilkas}
\matricnum{2146879}
\maketitle

\begin{abstract}
According to Simon Peyton Jones, an abstract should address
four key questions. First, what is the problem that this
paper tackles? Second, why is this an interesting problem?
Third, what is the solution this paper proposes?
Finally, why is the proposed solution a good one?
\end{abstract}

\section{Introduction}
% problem description/statement

\section{Background}
% motivation, relevance (whatever that means)
% comprehensive literature survey

\subsection{Linear Algebra and Numerical Analysis}

\begin{definition}[Norms]
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} is
  \[
    \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|
  \]
  whereas its \emph{taxicab} (or \emph{Manhattan}) \emph{norm} is
  \[
    \lVert \mathbf{x} \rVert_1 = \sum_{i = 1}^n |x_i|.
  \]
  Let $\mathbf{A}$ be a matrix. For any vector norm $\lVert
  \cdot \rVert_p$, we can also define its \emph{induced norm} for matrices as
  \[
    \lVert \mathbf{A} \rVert_p = \sup_{\mathbf{x} \ne \mathbf{0}} \frac{\lVert
      \mathbf{Ax} \rVert_p}{\lVert \mathbf{x} \rVert_p}.
  \]
  In particular, for $p = \infty$, we have
  \[
    \lVert \mathbf{A} \rVert_\infty = \max_i \sum_{j} |A_{i,j}|.
  \]
\end{definition}

\begin{lemma}[Perturbation Lemma
  \cite{layton2014numerical}] \label{prop:condition_number}
  Let $\lVert \cdot \rVert$ be any matrix norm, and let $\mathbf{A}$ and
  $\mathbf{E}$ be matrices such that $\mathbf{A}$ is invertible and $\lVert
  \mathbf{A}^{-1} \rVert \lVert \mathbf{E} \rVert < 1$, then $\mathbf{A} +
  \mathbf{E}$ is invertible, and
  \[
    \lVert (\mathbf{A} + \mathbf{E})^{-1} \rVert \le \frac{\lVert
      \mathbf{A}^{-1} \rVert}{1 - \lVert \mathbf{A}^{-1} \rVert \lVert
      \mathbf{E} \rVert}.
  \]
\end{lemma}

\section{The WizWoz System}
% solution
% TODO: express all lambdas as exponentials (update the derivatives or sth)

\subsection{Notation}

For any matrix $\mathbf{A}$, we will use either $A_{i,j}$ or
$[\mathbf{A}]_{i,j}$ to denote the element of $\mathbf{A}$ in row $i$ and column
$j$. Moreover, we use $\tr(\mathbf{A})$ to denote its \emph{trace} and
$\adj(\mathbf{A})$ for its \emph{adjugate} (or \emph{classical adjoint}).

For any vector $\mathbf{x}$, we write $\mathbb{R}_d[\mathbf{x}]$ to denote a
vector space of polynomials with degree at most $d$, where variables are
elements of $\mathbf{x}$, and coefficients are in $\mathbb{R}$.

\subsection{Preliminaries}

In this paper, all references to measurability are with respect to the Lebesgue
measure. Similarly, whenever we consider the existence of an integral, we use
the Lebesgue definition of integration.

As recently suggested by Ong et al. \cite{ong2018gaussian}, we use a
decomposition $\bm\Sigma = \mathbf{B}\mathbf{B}^\intercal + \mathbf{D}^2$, where
$\mathbf{B}$ is a lower triangular $m \times p$ matrix with positive diagonal
entries, and $\mathbf{D} = \diag(d_1, \dots, d_m)$. Typically, we would set $p$
so that $p \ll m$ to get an efficient approximation, but it is also worth
pointing out that we can retain full accuracy by setting $p = m$ and $\mathbf{D}
= \mathbf{O}$. Moreover, we define a few matrices that will simplify expressions
for the derivatives:
\begin{align*}
  \mathbf{U} &= (\mathbf{u} - \bm\mu)(\mathbf{u} - \bm\mu)^\intercal \\
  \mathbf{S} &= \Kru^\intercal\Kuu^{-1}, \\
  \bm\Gamma &= \Krr - \mathbf{S}\Kru, \\
  \mathbf{R} &= \mathbf{S}\frac{\partial \Kru}{\partial \lambda_i} - \frac{\partial \Krr}{\partial \lambda_i} + \left( \frac{\partial \Kru^\intercal}{\partial \lambda_i} - \mathbf{S}\frac{\partial \Kuu}{\partial \lambda_i} \right) \Kuu^{-1}\Kru.
\end{align*}

Derivatives such as $\frac{\partial \Kuu}{\partial \lambda_i}$ can be expressed
as
\[
  \frac{\partial \Kuu}{\partial \lambda_i} = \frac{1}{\lambda_i}\Kuu
\]

if $i = 0$, and

\[
  \begin{split}
    \left[ \frac{\partial \Kuu}{\partial \lambda_i} \right]_{j,k} =
    k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
    \left( \vphantom{\frac{1}{2}} \right. &-\frac{1}{2}(x_{\mathbf{u},j,i} -
    x_{\mathbf{u},k,i})^2 \\
    &- \mathbbm{1}[j \ne k]\sigma^2 \left. \vphantom{\frac{1}{2}} \right)
  \end{split}
\]
otherwise.

\begin{lemma}[Derivatives of probability
  distributions] \label{lemma:derivatives}
  \leavevmode
  \begin{enumerate}
  \item $\frac{\partial q(\mathbf{u})}{\partial \bm\mu} =
    \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u}
    - \bm\mu)$.
  \item
    \begin{enumerate}
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} =
      \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-\intercal}\mathbf{U}\bm\Sigma^{-\intercal}
      - \bm\Sigma^{-\intercal})$.
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
      q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
      |\bm\Sigma|^{-1}\adj(\bm\Sigma))\mathbf{B}$.
%    \item
%      $\frac{\partial q(\mathbf{u})}{\partial \mathbf{D}} =
%      \frac{1}{2}q(\mathbf{u}) \{ \bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1}\mathbf{D}^\intercal +
%      \mathbf{D}^\intercal\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
%      |\bm\Sigma|^{-1}(\adj(\bm\Sigma^{-1})\mathbf{D}^\intercal +
%      \mathbf{D}^\intercal \adj(\bm\Sigma^{-1})) \}$.
    \end{enumerate}
  \item For $i = 0, \dots, d$,
    \[
      \begin{split}
        \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i} &=
        \frac{1}{2}q(\mathbf{r} \mid \mathbf{u}) (|\bm\Gamma|^{-1}
          \tr(\mathbf{R} \adj(\bm\Gamma)) \\
          &- (\mathbf{r} -
          \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r}
          - \mathbf{Su})),
      \end{split}
    \]
  \end{enumerate}
\end{lemma}

\section{Evaluation} % showing deep understanding

\section{Conclusions}

\subsection{Future Work}

%\vskip8pt \noindent
%{\bf Acknowledgments.}
%This is optional; it is a location for you to thank people
\bibliographystyle{abbrv}
\bibliography{paper}
\end{document}