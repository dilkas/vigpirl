\documentclass{mpaper}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{thmtools, thm-restate}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{tikz}
\usepackage{layouts}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{pgfplots}
\usepackage{blkarray}
\usepackage[boxed]{algorithm2e}

\usetikzlibrary{shapes,bayesnet,arrows.meta,arrows,calc,pgfplots.colormaps}
%\tikzstyle{myarrow} = [-{Latex[length=2mm,width=2mm]}]

% Makes the legend have a single rectangle per colour
\pgfplotsset{
  /pgfplots/ybar legend/.style={
    /pgfplots/legend image code/.code={%
      \draw[##1,/tikz/.cd,bar width=3pt,yshift=-0.2em,bar shift=0pt]
      plot coordinates {(0cm,0.8em)};},
  },
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{problem}[theorem]{Open Problem}
\newtheorem{example}[theorem]{Example}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\DKL}{D_{\mathrm{KL}}\infdivx}
\newcommand{\V}{V_{\mathbf{r}}}
\newcommand{\dx}{\,\mathrm{d}\mathbf{r}\,\mathrm{d}\mathbf{u}}
\newcommand{\vbound}{\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma}}
\newcommand{\rinf}{\lVert \mathbf{r} \rVert_\infty}

\newcommand{\approximationS}{q(\mathbf{u}_s, \mathbf{r}_s)}
\newcommand{\pfullS}{p(\mathcal{D}, \mathbf{u}_s, \mathbf{r}_s)}

\newcommand{\f}{f(\mathbf{r}, \mathbf{u}, t)}
\newcommand{\fn}{f_n(\mathbf{r}, \mathbf{u})}
\newcommand{\ftn}{f(\mathbf{r}, \mathbf{u}, t_n)}
\newcommand{\g}{g(\mathbf{r}, \mathbf{u})}

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}
\newcommand{\Luu}{\mathbf{L}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Euu}{\mathbf{E}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Err}{\mathbf{E}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Eru}{\mathbf{E}_{\mathbf{r},\mathbf{u}}}

\newcommand{\pfull}{p(\mathcal{D}, \mathbf{u}, \mathbf{r})}
\newcommand{\approximation}{q(\mathbf{u}, \mathbf{r})}
\newcommand{\posterior}{p(\mathbf{u}, \mathbf{r} \mid \mathcal{D})}

\newcommand{\dm}{\frac{\partial}{\partial\bm\mu}}
\newcommand{\dB}{\frac{\partial}{\partial\mathbf{B}}}
\newcommand{\dl}{\frac{\partial}{\partial \lambda_i}}
\newcommand{\dlj}{\frac{\partial}{\partial \lambda_j}}
\newcommand{\dt}{\frac{\partial}{\partial t}}
\newcommand{\df}{\left.\frac{\partial f}{\partial t}\right|_{(\mathbf{r},
    \mathbf{u}, t)}}

\newcommand{\pione}{\pi(a_1 \mid s_1)}
\newcommand{\pitwo}{\pi(a_1 \mid s_2)}
\newcommand{\pithree}{\pi(a_2 \mid s_3)}

\begin{document} % 14 pages

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian Processes}
\author{Paulius Dilkas}
\matricnum{2146879}
\maketitle

\begin{abstract}
  The inverse reinforcement learning (IRL) problem asks us to find a reward
  function of a Markov decision process that explains observed behaviour. Many
  approaches are only able to express reward functions as linear combinations
  of state features. Out of those that can handle nonlinearity, none can provide
  a full posterior distribution of rewards. Providing variance estimates for
  rewards would allow one to judge how well the model has learned its policy and
  discover any weak spots the model may have. We show how to perform variational
  inference (VI) on a Gaussian process-based IRL model in order to approximate
  the posterior distribution of rewards. We prove the correctness of the
  approach and demonstrate the model's behaviour in practice. Being able to
  provide full posterior probability distributions in IRL unlocks many new
  research frontiers ranging from integrating recent developments in VI to make
  the models more efficient and flexible, to developing complex reinforcement
  learning agents that can explicitly search for opportunities to fix their
  weaknesses.
\end{abstract}

\section{Introduction} % first page
% 1. Problem (with examples, intuitive)
% 2. Its importance/relevance/motivation
% 3. My idea
% 4. List of contributions
% Forward-reference which section deals with what

Imagine using a machine learning (ML) algorithm to teach a robot how to move
around people so that it learns to predict where people are going and adjust its
path accordingly. The ML algorithm would learn from data about proper actions in
a range of different situations. But do we have enough data to ensure reasonably
optimal behaviour? Perhaps the robot behaves well in most situations, but fails
in less common scenarios. Can the ML model itself describe its weaknesses so
that we could ensure it is exposed to sufficiently many uncommon or difficult
situations?

This learning problem \cite{DBLP:journals/ijrr/KretzschmarSSB16} as well as
many others have benefited from an approach called \emph{inverse reinforcement
  learning} (IRL), also known as apprenticeship learning and inverse optimal
control. IRL proposes a way to learn behaviour from demonstrations that
typically come from human actions. More formally, the IRL problem asks us to
find a reward function for a Markov decision process (MDP), where demonstrations
are encoded as sets of state-action pairs, showing observed actions in a number
of states.

IRL is an important problem because adjusting the reward function by hand is
often unwieldy, since human behaviour often depends on many factors in
complicated ways
\cite{DBLP:conf/icml/PieterN04}. Moreover,
learning the reward function rather than the policy itself makes the model more
transferable to new environments: a minor change in the environment can
reorganise the whole policy but only have a local effect in the reward structure
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}.
IRL has been used to teach helicopters how to perform tricks
\cite{DBLP:conf/nips/AbbeelCQN06}, predict taxi destinations
\cite{DBLP:conf/huc/ZiebartMDB08}, and make driving safer and more efficient by
predicting pedestrian movement \cite{DBLP:conf/iros/ZiebartRGMPBHDS09} and the
driver's intentions \cite{DBLP:conf/aaai/VogelRGR12}.

Most IRL models in the literature make a convenient yet unjustified
assumption that the reward function can be expressed as a linear combination of
features
\cite{DBLP:conf/icml/PieterN04,DBLP:conf/icml/NgR00,ziebart2008maximum}.
This assumption severely restricts what reward structures can be modelled. Out
of the non-linear models proposed to date, none can answer the questions posed
in the first paragraph. More specifically, without knowing the ground truth for
the learning problem, we have no reliable way to know whether the learned
behaviour is optimal and which parts of the model could benefit from more data.
Quite often, the models assume that rewards have no variance
\cite{DBLP:conf/nips/LevinePK11,DBLP:conf/uai/JinDAS17}.

In this paper, we show how that assumption can be lifted by switching from
maximum likelihood estimation to \emph{variational inference} (VI), i.e., we
approximate the posterior distribution of the model by optimising the parameters
of a simpler distribution to make it similar to the posterior. This approach can
prove useful in three major ways:
\begin{enumerate}
\item Variance estimates can be used to guide what data should be collected
  next, i.e., if the rewards of some states have abnormally high variance, we
  might want to expose the model to more data visiting those and surrounding
  states.
\item Variances estimates can also be used to judge whether we can trust the
  predictions of the model or, perhaps, the model could benefit from some
  adjustments or more data.
\item By adopting a more Bayesian approach we can set prior distributions on
  random variables. This can help prevent overfitting as well as encode
  additional information about the reward function
  \cite{DBLP:conf/uai/JinDAS17}.
\end{enumerate}

No fully Bayesian nonlinear IRL model has been proposed yet. We will uncover one
reason for this when we set up our IRL model for VI in Section~\ref{sec:model}.
Applying VI to this model in a theoretically sound way (and without introducing
unjustified simplifying assumptions) requires us to use Lebesgue's dominated
convergence theorem in order to derive the gradient of the expectation of the
MDP value function---we provide a complete proof for this (and some other
results) in Section~\ref{sec:proof}. Finally, in Section~\ref{sec:evaluation} we
demonstrate the model's correctness and properties experimentally using several
example scenarios.

\paragraph{Notation and Conventions}

For any matrix $\mathbf{A}$, we write either $A_{i,j}$ or
$[\mathbf{A}]_{i,j}$ to denote the element of $\mathbf{A}$ in row $i$ and column
$j$. We use $\tr(\mathbf{A})$ to denote its \emph{trace} and $\adj(\mathbf{A})$
for its \emph{adjugate} (or \emph{classical adjoint}). For any vector
$\mathbf{x}$, we write $\mathbb{R}_d[\mathbf{x}]$ to denote a vector space of
polynomials with degree at most $d$, where variables are elements of
$\mathbf{x}$, and coefficients are in $\mathbb{R}$.

Throughout the paper, all integrals should be interpreted as definite integrals
over the entire sample space. When referencing measurability, we assume Lebesgue
measure in a Euclidean space with a suitable number of dimensions. Similarly,
whenever we consider the existence of an integral, we use the Lebesgue
definition of integration.

\section{Background} % 0.5-1 page

The IRL problem itself was originally proposed by Russell in 1998
\cite{DBLP:conf/colt/Russell98}. Most of the early approaches had the
aforementioned reward linearity assumption. One of the first papers on the
subject by Ng and Russell \cite{DBLP:conf/icml/NgR00} introduced several linear
programming algorithms and identified an important issue: there are typically
many reward functions that can explain the data equally well. This problem was
solved by Ziebart et al. \cite{ziebart2008maximum} with the introduction of IRL
based on the principle of maximum causal entropy in a linearly-solvable MDP.

Levine et al. \cite{DBLP:conf/nips/LevinePK11} were the first to lift the
linearity assumption without imposing additional restrictions on the problem by
modelling rewards with a Gaussian process (GP). They do, however, assume that
rewards have no variance---our work removes this restriction without any
compromises.

An alternative to GPs for modelling nonlinear functions is, of course, neural
networks. Wulfmeier et al. \cite{wulfmeier2015maximum} have shown how they can
be used in the IRL setting. While this approach benefits from constant-time
inference and the ability to learn complex features from data, neural
networks often need significantly more data for the weights across all layers to
stabilise.

Recently, Jin et al. \cite{DBLP:conf/uai/JinDAS17} have adapted the model
proposed by Levine et al. \cite{DBLP:conf/nips/LevinePK11} to use deep GPs,
harnessing the power of deep learning to make the model less dependent on what
features are provided. Although they use VI, their approximating distribution
for rewards at inducing points is simply the Dirac $\delta$ function, which is
essentially equivalent to the assumption of no variance.

We argue that developing an IRL model with a variational approximation of the
full posterior distribution has eluded previous research in the area for two
reasons. First, it is not directly relevant to the goal of producing accurate
point estimates of rewards. However, we described several key benefits of
variance prediction in the previous section, and we postulate how this work
could affect the wider fields of reinforcement learning and artificial
intelligence in Section~\ref{sec:conclusions}. Second, as it will become
apparent in Sections~\ref{sec:model} and \ref{sec:proof}, optimising such a VI
model involves estimating gradients, and establishing correctness of these
estimates requires a significant amount of theoretical groundwork.

\subsection{Variational Inference}

VI has come a long way from the initial \emph{mean field} approach where each
variable is approximated by a univariate Gaussian, independent of all other
variables. Recent years have brought improvements in both computational
complexity and flexibility of the variational approximation
\cite{blei2017variational}. In this section, we will mention some of the most
important and relevant developments in the field, while referring the interested
reader to recent survey articles
\cite{blei2017variational,DBLP:journals/corr/abs-1711-05597} for a broader
overview of the area.

A major development in approximating complex distributions came in the form of
\emph{normalizing flows}, i.e., a collection of invertible
functions---parametrised by additional variational parameters---that are applied
to latent variables \cite{DBLP:conf/icml/RezendeM15}. A sequence of such
functions can transform a simple initial distribution (e.g., uniform or
Gaussian) into an arbitrarily complex one.

Another approach to flexibility in modelling can be achieved by considering
different GP kernels, i.e., functions that model similarity between pairs of
points. For instance, Wilson and Adams \cite{pmlr-v28-wilson13} show how all
\emph{stationary} (i.e., invariant to translations) kernels can be generated (or
at least approximated) from a mixture of Gaussians in their spectral
representation using Bochner's theorem \cite{bochner1959lectures}. It looks
promising to combine these kernels with the variational Fourier features
approach by Hensman et al. \cite{DBLP:journals/jmlr/HensmanDS17} that leverages
the same spectral representations for efficient VI. This algorithm, however, is
currently restricted to Mat\'ern kernels, which could be restrictive,
considering how IRL models tend to use kernels that can adjust the importance of
each feature \cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}.

While these and many other advancements in VI promise great improvements, there
are hidden challenges when it comes to applying them to IRL. Many suggested
improvements implicitly impose restrictions on the model that are unlikely to be
satisfied in the context of IRL, e.g., the evidence of the model being a
Gaussian. Nevertheless, the work mentioned here seems adaptable to the IRL
setting and can make variational approximations both more accurate and faster.

\section{Problem Description} \label{sec:problem}

In this section, we describe the problem in more detail, introducing the
notation and definitions that will be used throughout the paper.

\begin{definition}[MDP]
  A \emph{Markov decision process} is a set $\mathcal{M} = \{ \mathcal{S},
  \mathcal{A}, \mathcal{T}, \gamma, \mathbf{r} \}$, where $\mathcal{S}$ and
  $\mathcal{A}$ are sets of states and actions, respectively; $\mathcal{T}
  \colon \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is a
  function defined so that $\mathcal{T}(s, a, s')$ is the probability of moving
  to state $s'$ after taking action $a$ in state $s$; $\gamma \in [0, 1)$ is the
  discount factor; and $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$ is the reward
  vector.
\end{definition}

While it is more common to represent rewards as a function $r \colon \mathcal{S}
\to \mathbb{R}$, the vector notation is generally more appropriate in our
situation. However, we switch between the two notations as needed.

\begin{definition}[IRL]
  Given an MDP without rewards $\mathcal{M} \setminus \{ \mathbf{r} \}$, an
  $|\mathcal{S}| \times d$ feature matrix $\mathbf{X}$ (where $d$ is the number
  of features), and a set of \emph{demonstrations} $\mathcal{D} = \{\zeta_i
  \}_{i=1}^N$, where each demonstration $\zeta_i = \{ (s_{i,t}, a_{i,t})
  \}_{t=1}^T$ is a multiset of state-action pairs that represents optimal (or
  near-optimal) actions, find a reward function that maximises the probability
  of observing the demonstrations, i.e.,
  \[
    \argmax_{\mathbf{r}} p(\mathcal{D} \mid \mathbf{r}).
  \]
\end{definition}

\begin{example}
  Figure~\ref{fig:example} shows a possible setup for IRL. In this case,
  $\mathcal{S} = \{ s_1, s_2, s_3, s_4 \}$, $\mathcal{A} = \{ l, r, u, d \}$,
  \begin{align*}
    \mathcal{T}(s, a, s') &= \begin{cases}
      1 & \text{if } (s, a, s') = (s_1, r, s_2) \\
      1 & \text{if } (s, a, s') = (s_1, d, s_4) \\
      1 & \text{if } (s, a, s') = (s_2, l, s_1) \\
      1 & \text{if } (s, a, s') = (s_2, d, s_3) \\
      1 & \text{if } (s, a, s') = (s_3, l, s_4) \\
      1 & \text{if } (s, a, s') = (s_3, u, s_2) \\
      1 & \text{if } (s, a, s') = (s_4, r, s_3) \\
      1 & \text{if } (s, a, s') = (s_4, u, s_1) \\
      0 & \text{otherwise,}
    \end{cases} \\
      \mathbf{X} &= \begin{blockarray}{ccc}
        & f_1 & f_2 \\
        \begin{block}{c[cc]}
          s_1 & 1 & 2 \\
          s_2 & 2 & 2 \\
          s_3 & 2 & 1 \\
          s_4 & 1 & 1 \\
        \end{block}
      \end{blockarray},
  \end{align*}
  and $\gamma \in [0, 1)$ can be assigned any value. The distribution of example
  state-action pairs into separate demonstrations is irrelevant to our model
  (and presumably all other models) and is only a relic of the practical
  motivation behind the problem's initial definition. Hence, we can keep each
  pair in its own set as follows:
  \[
    \mathcal{D} = \{ \{(s_1, d)\}, \{(s_2, l)\}, \{(s_4, u)\}, \{(s_4, u)\} \}.
  \]

  A solution to this problem is then an assignment of rewards to states, e.g.,
  \[
    \mathbf{r} = \begin{bmatrix}
      2 \\
      0 \\
      -1 \\
      1
    \end{bmatrix} \quad \text{or} \quad r(s) = \begin{cases}
      2 & \text{if } s = s_1 \\
      0 & \text{if } s = s_2 \\
      -1 & \text{if } s = s_3 \\
      1 & \text{if } s = s_4
    \end{cases}.
  \]
\end{example}

\begin{figure}
  \centering
  \begin{tikzpicture}[state/.style={circle,draw,fill=lightgray}]
    \node [state] (s1) {$s_1$};
    \node [state,right=2 of s1] (s2) {$s_2$};
    \node [state,below=2 of s2] (s3) {$s_3$};
    \node [state,left=2 of s3] (s4) {$s_4$};

    \draw[->,bend left,dashed] (s1) edge node[above] {$r$} (s2);
    \draw[->,bend left] (s2) edge node[below] {$l(1)$} (s1);
    \draw[->,bend left] (s1) edge node[right] {$d(1)$} (s4);
    \draw[->,bend left] (s4) edge node[left] {$u(2)$} (s1);
    \draw[->,bend left,dashed] (s4) edge node[above] {$r$} (s3);
    \draw[->,bend left,dashed] (s3) edge node[below] {$l$} (s4);
    \draw[->,bend left,dashed] (s2) edge node[right] {$d$} (s3);
    \draw[->,bend left,dashed] (s3) edge node[left] {$u$} (s2);

    \draw [<->] (-1.3, 0.8) node (yaxis) [above] {$f_2$}
    |- (3.5, -3.6) node (xaxis) [right] {$f_1$};
    \draw[shift={(0,-3.6)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$1$};
    \draw[shift={(2.65,-3.6)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$2$};
    \draw[shift={(-1.3,-2.6)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$1$};
    \draw[shift={(-1.3,0)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$2$};
  \end{tikzpicture}
  \caption{An example MDP with deterministic actions. States are represented by
    grey circles, actions by directed labelled edges between them. The numbers
    in parentheses next to some actions' names denote how many times that
    state-action pair appears in $\mathcal{D}$. If that number is zero, the edge
    is dashed. Finally, the axes are used to assign two features to each state.}
  \label{fig:example}
\end{figure}

The optimal (deterministic) \emph{policy} $\pi \colon \mathcal{S} \to
\mathcal{A}$ (i.e., a choice of actions for each state that maximises reward
over time) is usually constructed by defining a \emph{value (utility) function}
$\V \colon \mathcal{S} \to \mathbb{R}$ that measures how good a state is based
on the reward $\mathbf{r}$ as well as the structure of the MDP. One can then
find $\V$ by applying the Bellman backup operator until convergence to all
states $s \in \mathcal{S}$ (this algorithm is known as \emph{value iteration})
\cite{DBLP:books/daglib/0023820}:
\[
  V_{\mathbf{r}}(s) \coloneqq r(s) + \gamma \max_{a \in \mathcal{A}} \sum_{s' \in
    \mathcal{S}} \mathcal{T}(s, a, s')V_{\mathbf{r}}(s').
\]

However, we follow previous work on IRL with GPs
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11} and use a
\emph{linearly solvable} (or \emph{maximum causal entropy}) MDP with a
stochastic policy that defines probability distributions over actions instead
of suggesting a single action for each state \cite{ziebart2008maximum}. This
type of MDP can be solved by applying the `soft' version of the operator
\cite{DBLP:conf/nips/LevinePK11,supplementary_material}:
\begin{equation} \label{eq:update_rule}
  \V(s) \coloneqq \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s'
      \in \mathcal{S}} \mathcal{T}(s, a, s')\V(s') \right).
\end{equation}
With this model, we can express the likelihood as
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}
\begin{align*}
  p(\mathcal{D} \mid \mathbf{r}) &= \prod_{i=1}^N \prod_{t=1}^T p(a_{i,t} \mid s_{i,t}) \\
                                 &= \exp\left( \sum_{i=1}^N \sum_{t=1}^T Q_{\mathbf{r}}(s_{i,t}, a_{i,t}) - \V(s_{i,t}) \right),
\end{align*}
where
\[
  Q_{\mathbf{r}}(s, a) = r(s) + \gamma\sum_{s' \in \mathcal{S}}
  \mathcal{T}(s, a, s')\V(s').
\]

A significant part of learning is about being able to generalise to previously
unseen input states (otherwise the problem becomes knowledge representation
instead of learning). For this reason, we represent each state by a
$d$-dimensional feature vector. This way, similar states are represented by
similar features, and the ML model can predict rewards for new states based on
their similarity to training data.

In this paper, we model rewards as a function from feature space to
$\mathbb{R}$ using a \emph{Gaussian process}. A GP represents an infinite
collection of random variables from a joint (infinitely-dimensional) probability
distribution, any finite number of which has a Gaussian distribution
\cite{DBLP:books/lib/RasmussenW06}. A GP is defined by its mean function (which
is always $\mathbf{x} \mapsto 0$ in our case) and covariance function, which we
denote by $k$. \emph{Covariance functions} (also known as \emph{kernels}) take
two state feature vectors as input and quantify how similar the two states are,
in a sense that we would expect high covariance scores to be associated with
similar rewards.

A common way to scale GPs to larger data sets is by selecting $m$ points in the
feature space--called \emph{inducing points}---and focus most of the training
effort on them \cite{DBLP:journals/corr/abs-1807-01065}. Let $\mathbf{X_u}$ be
the $m \times d$ matrix of features at inducing points, and let $\mathbf{u}$ be
the rewards at those states. Then the full joint probability distribution can be
factorised as
\begin{equation} \label{eq:full}
  \pfull = p(\mathbf{u}) \times p(\mathbf{r} \mid \mathbf{u}) \times
  p(\mathcal{D} \mid \mathbf{r}),
\end{equation}
where $p(\mathbf{u}) = \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu)$ is the GP
prior \cite{DBLP:books/lib/RasmussenW06}. The GP posterior is then a
multivariate Gaussian \cite{DBLP:conf/nips/LevinePK11} defined as
\begin{equation} \label{eq:r}
  p(\mathbf{r} \mid \mathbf{u}) =
  \mathcal{N}(\mathbf{r}; \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr - \Kru^\intercal\Kuu^{-1}\Kru).
\end{equation}
The matrices such as
$\Kru$ are called \emph{covariance matrices} (also \emph{kernel matrices} and
\emph{Gram matrices}) and are defined as
$[\Kru]_{i,j} = k(\mathbf{x}_{\mathbf{r},i},
\mathbf{x}_{\mathbf{u},j})$, where $\mathbf{x}_{\mathbf{r},i}$ and
$\mathbf{x}_{\mathbf{u},j}$ denote feature vectors for the $i$th state in
$\mathcal{S}$ and the $j$th inducing point, respectively
\cite{DBLP:conf/uai/JinDAS17}.

We can then use VI in order to approximate
$p(\mathbf{u},~\mathbf{r}~\mid~\mathcal{D})$. This is done by minimising the
\emph{Kullback-Leibler} (KL) divergence between the original probability
distribution and our approximation. KL divergence (asymmetrically) measures the
difference between two probability distributions. Let $\approximation$ denote
our approximation. The KL divergence is then defined as
\cite{blei2017variational}
\begin{align*}
  \DKL{\approximation}{\posterior} &= \mathbb{E}_{\approximation}[\log\approximation - \log\posterior ] \\
                                   &= \mathbb{E}_{\approximation}[\log\approximation - \log\pfull] \\
                                   &+ \mathbb{E}_{\approximation}[\log p(\mathcal{D})].
\end{align*}
The last term is both hard to compute and constant with respect to
$\approximation$, so we can remove it from our optimisation objective. The
negation of what remains is known as the \emph{evidence lower bound} (ELBO) and
is defined as \cite{DBLP:books/lib/Bishop07,blei2017variational}
\begin{equation} \label{eq:elbo}
  \begin{split}
    \mathcal{L} &= \mathbb{E}_{\approximation}\left[ \log \frac{\pfull}{\approximation} \right] \\
    &= \iint \log \frac{\pfull}{\approximation} \approximation\dx.
  \end{split}
\end{equation}
Thus, instead of minimising KL divergence, we focus on maximising $\mathcal{L}$
by optimising the values of parameters that we define in
Section~\ref{sec:model}. Note that hereafter we drop the subscript notation, as
all expected values will be with respect to $\approximation$.

\section{The Model} \label{sec:model}

In order to have a fully functional model, we still need to make several key
design decisions. In this section, we describe the covariance function used to
define the GP, consider what parameters should be used to optimise $\mathcal{L}$
and how they should be initialised, and in Section~\ref{sec:elbo} we derive our
final expression for $\mathcal{L}$.

We stick with the same covariance function as in the work by Levine et al.
\cite{DBLP:conf/nips/LevinePK11}, which is a version of the \emph{automatic
  relevance detection kernel}:
\begin{align*}
  k(\mathbf{x}_i, \mathbf{x}_j) = \lambda_0\exp\left( \vphantom{\frac{1}{2}} \right. &-\frac{1}{2}(\mathbf{x}_i - \mathbf{x}_j)^\intercal\bm\Lambda(\mathbf{x}_i - \mathbf{x}_j) \\
                                                                                     &- \mathbbm{1}[i \ne j]\sigma^2\tr(\bm\Lambda) \left. \vphantom{\frac{1}{2}} \right).
\end{align*}
Here, $\lambda_0$ is the overall `scale' factor, $\bm\Lambda = \diag(\lambda_1,
\dots, \lambda_d)$ is a diagonal matrix that determines the importance of each
feature, $\mathbbm{1}$ is defined as
\[
  \mathbbm{1}[b] = \begin{cases}
    1 & \mbox{if $b$ is true} \\
    0 & \mbox{otherwise},
  \end{cases}
\]
and $\sigma^2$ is fixed as $10^{-2}/2$, since this value has little influence on
the behaviour of the algorithm and is here as a noise factor used to avoid
singular covariance matrices \cite{DBLP:conf/nips/LevinePK11}. We will write
$\bm\lambda = (\lambda_0, \dots, \lambda_d)^\intercal$ to refer to both
$\lambda_0$ and $\bm\Lambda$ at the same time.

Ideally, we would like to model $\bm\lambda$ with an approximating distribution.
However, due to how the prior probability density function (PDF) for
$\mathbf{u}$ involves $\Kuu^{-1}$, and $\mathcal{L}$ has an expected value that
cannot be eliminated (see Section~\ref{sec:elbo}), we are unable to show that
such an $\mathcal{L}$ is well-defined. More generally, we pose the following
problem, which, to the best of our knowledge, is currently open:
\begin{problem}
  Let $\mathbf{A}$ be a $n \times n$ matrix of coefficients, $X$ be a random
  variable, and $\mathbf{M}$ be an $n \times n$ matrix such that $M_{i,j} = f(X,
  A_{i,j})$, where $f$ is an arbitrary function. Under what circumstances does
  $\mathbb{E}[\mathbf{M}^{-1}]$ exist?
\end{problem}
While there are some obvious examples where the expected value exists (e.g.,
$f(X, A_{i,j}) = A_{i,j}X$ for an invertible $\mathbf{A}$ and many distributions
of $X$), it would be particularly interesting to know whether the answer is
`always'. A proof of such a result would allow us to model $\bm\lambda$ instead
of treating it as a variational parameter, which would make the model fully
Bayesian. For now, $\bm\lambda$ will have to be treated as a variational
parameter.

It remains to decide on the approximating distribution for $\mathbf{u}$ and
$\mathbf{r}$. As is commonly done when applying VI to GPs
\cite{DBLP:conf/nips/ChengB17}, we set
\begin{equation} \label{eq:approximation}
  \approximation = q(\mathbf{u}) q(\mathbf{r} \mid \mathbf{u}),
\end{equation}
where $q(\mathbf{r} \mid \mathbf{u}) = p(\mathbf{r} \mid \mathbf{u})$ and
$q(\mathbf{u}) = \mathcal{N}(\mathbf{u}; \bm\mu, \bm\Sigma)$.

Ong et al. \cite{ong2018gaussian} have recently suggested that, in order to make
variational approximation of a multivariate Gaussian more scalable, the
covariance matrix should be decomposed as $\bm\Sigma =
\mathbf{B}\mathbf{B}^\intercal + \mathbf{D}^2$, where $\mathbf{B}$ is a lower
triangular $m \times p$ matrix with positive diagonal entries, and $\mathbf{D}$
is a diagonal matrix. Typically, we would set $p$ so that $p \ll m$ to get an
efficient approximation, but in this case we will simply set $p = m$ and
$\mathbf{D} = \mathbf{O}_m$ in order to retain full covariance structure.

The resulting model is summarised in Figure~\ref{fig:graphical_model}. We rely
on $p(\mathcal{D} \mid \mathbf{r})$ as the only link between data and our model.
Since the expression for $q(\mathbf{r} \mid \mathbf{u})$ has both $\mathbf{u}$
and covariance matrices in it, $\mathbf{r}$ depends on both $\mathbf{u}$ and the
parameters of the kernel, $\bm\lambda$. The two remaining dependencies stem from
the fact that the approximating distribution for $\mathbf{u}$ is
$\mathcal{N}(\bm\mu, \mathbf{BB}^\intercal)$.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node[latent] (u) {$\mathbf{u}$};
    \node[latent, right=of u] (r) {$\mathbf{r}$};
    \node[obs, right=of r] (D) {$\mathcal{D}$};
    \node[const, below=of r] (lambda) {$\bm\lambda$};
    \node[const, below=of u] (mu) {$\bm\mu$};
    \node[const, left=of mu] (B) {$\mathbf{B}$};
    \edge {mu, B} {u};
    \edge {lambda, u} {r};
    \edge {r} {D};
  \end{tikzpicture}
  \caption{Our VI problem expressed as a (simplified)
    Bayesian network. The only observed variable (representing the
    demonstrations) is in a grey circle, modelled latent variables are in white
    circles, and the variational parameters are at the bottom.}
  \label{fig:graphical_model}
\end{figure}

As we want to restrict some parameters (namely, $\bm\lambda$ and the diagonal of
$\mathbf{B}$) to positive values, we express them as exponentials and later
adjust their derivatives accordingly. Specifically, we can set $\lambda_i =
e^{\lambda'_i}$ and optimise $\lambda'_i$ using the chain rule:
\begin{equation} \label{eq:transformation}
  \frac{\partial \mathcal{L}}{\partial \lambda'_i} = e^{\lambda'_i}
  \frac{\partial \mathcal{L}}{\partial \lambda_i}.
\end{equation}
This way, we restrict $\lambda_i$ to positive values while allowing $\lambda'_i$
to range over $\mathbb{R}$.

Finally, the parameters can be initialised as follows\footnote{In practice, it
  is often easier to start with a fixed $\mathbf{B} = \mathbf{I}_m$, as random
  values of $\mathbf{B}$ can often result in a near-singular $\bm\Sigma$.}:
%\begin{equation} \label{eq:initialisation}
\begin{alignat}{3}
  \mu_i &\sim \mathcal{U}(0, 1) \quad &&\text{for } i = 1, \dots, m, \label{eq:first_initialisation} \\
  \lambda_0 &\sim \chi^2_5, && \\
  \lambda_i &\sim \chi^2_1 \quad &&\text{for } i = 1, \dots, d, \\
  \diag(\mathbf{B}) &\sim \chi^2_4, && \\
  \text{the rest of } \mathbf{B} &\sim \mathcal{N}(0, 1). \label{eq:last_initialisation} &&
\end{alignat}
%\end{equation}
The initialisation of $\bm\mu$ mirrors the initialisation of $\mathbf{r}$ in
previous work \cite{DBLP:conf/nips/LevinePK11}. In contrast, while they have
constant initial values for $\bm\lambda$, we sample from $\chi^2$ distributions
centred around those values ($5$ for $\lambda_0$ and $1$ for any other
$\lambda_i$). The distributions for initial values of $\mathbf{B}$ are simply
set to provide a reasonable spread of positive values for the diagonal, and both
positive and negative values for all other entries in the matrix.

\subsection{Evidence Lower Bound} \label{sec:elbo}

It remains to express $\mathcal{L}$ for our (now fully specified) model. Note
that in order to keep the derivation simple, we drop all constant terms in the
expression of $\mathcal{L}$, i.e., equality is taken to mean `equality up to an
additive constant'.

Firstly, let us return to Equation~\eqref{eq:elbo} and write
\[
  \mathcal{L} = \mathbb{E}[\log\pfull] - \mathbb{E}[\log\approximation].
\]
By substituting in Equations~\eqref{eq:full} and \eqref{eq:approximation}, we get
\begin{align*}
  \mathcal{L} &= \mathbb{E}[\log p(\mathbf{u}) + \log p(\mathbf{r} \mid \mathbf{u}) + \log p(\mathcal{D} \mid \mathbf{r})] \\
              &- \mathbb{E}[\log q(\mathbf{u}) + \log q(\mathbf{r} \mid \mathbf{u})].
\end{align*}
Since $q(\mathbf{r} \mid \mathbf{u}) = p(\mathbf{r} \mid \mathbf{u})$, they
cancel each other out. Also notice that
\begin{align*}
  \mathbb{E}[\log p(\mathbf{u}) - \log q(\mathbf{u})] &= -\DKL{q(\mathbf{u})}{p(\mathbf{u})} \\
                                                      &= -\frac{1}{2} (\tr (\Kuu^{-1}\bm\Sigma) + \bm\mu^\intercal\Kuu^{-1}\bm\mu - m \\
                                                      &+ \log |\Kuu| - \log |\bm\Sigma|),
\end{align*}
by the definition of KL divergence between two multivariate Gaussians
\cite{kl}. Hence,
\begin{align*}
  \mathcal{L} &= \mathbb{E}\left[ \sum_{i=1}^N \sum_{t=1}^T Q_{\mathbf{r}}(s_{i,t}, a_{i,t}) - \V(s_{i,t}) \right] \\
              &- \frac{1}{2} \left(\tr \left( \Kuu^{-1}\bm\Sigma \right) + \bm\mu^\intercal\Kuu^{-1}\bm\mu + \log |\Kuu| - \log |\bm\Sigma| \right).
\end{align*}
Using the expressions for $Q_{\mathbf{r}}$ we get
\begin{align*}
  \mathcal{L} &= \mathbb{E}\left[\sum_{i=1}^N \sum_{t=1}^T r(s_{i,t}) - \V(s_{i,t}) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\V(s') \right] \\
              &- \frac{1}{2} \left(\tr \left( \Kuu^{-1}\bm\Sigma \right) + \bm\mu^\intercal\Kuu^{-1}\bm\mu + \log |\Kuu| - \log |\bm\Sigma| \right).
\end{align*}
We can simplify $\sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t})$ by defining a new vector
$\mathbf{t} = (t_1, \dots, t_{|\mathcal{S}|})^\intercal$, where $t_i$ is the
number of times the state associated with reward $r_i$ has been visited
across all demonstrations. Then,
\begin{align*}
  \mathbb{E} \left[ \sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t}) \right] &= \mathbb{E}[\mathbf{t}^\intercal\mathbf{r}] = \mathbf{t}^\intercal\mathbb{E}[\mathbf{r}] \\
                                                               &= \mathbf{t}^\intercal\mathbb{E}\left[\Kru^\intercal\Kuu^{-1}\mathbf{u}\right] = \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu.
\end{align*}
This allows us to simplify $\mathcal{L}$ to
\begin{align*}
  \mathcal{L} &= \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu - \mathbb{E}[v] \\
              &- \frac{1}{2} \left(\tr \left( \Kuu^{-1}\bm\Sigma \right) + \bm\mu^\intercal\Kuu^{-1}\bm\mu + \log |\Kuu| - \log |\bm\Sigma| \right),
\end{align*}
where
\[
  v = \sum_{i=1}^N \sum_{t=1}^T \V(s_{i,t}) - \gamma\sum_{s' \in \mathcal{S}}
  \mathcal{T}(s_{i,t}, a_{i,t}, s')\V(s').
\]
We will need to optimise this expression in order to train our model.

\section{Theoretical Justification} \label{sec:proof}

The typical way to optimise a quantity (the ELBO, in this case) involves
computing its gradient. Unfortunately, the term $\mathbb{E}[v]$ in $\mathcal{L}$
complicates the situation. Indeed, in order to take the derivative of an
expected value, certain conditions must be satisfied. Without proving the
validity of our algorithm, we risk building it on faulty assumptions and having
it misbehave---perhaps all the time, perhaps only in certain situations. The
goal of this section is to show how Lebesgue's dominated convergence theorem can
be applied to our model in order to derive $\nabla\mathbb{E}[v]$, i.e., the
derivative of $\mathbb{E}[v]$ with respect to our variational parameters
$\bm\mu$, $\mathbf{B}$, and $\bm\lambda$. This technique is inspired by black
box VI \cite{DBLP:conf/aistats/RanganathGB14}, but takes a more detailed look at
the problem and requires significantly more work to prove correctness. After
showing that the theorem applies to our situation, we can then estimate
$\nabla\mathbb{E}[v]$ with
\begin{align*}
  \nabla\mathbb{E}[v] &= \nabla \iint v q(\mathbf{r} \mid \mathbf{u}) q(\mathbf{u})\dx \\
                      &= \iint \nabla[v q(\mathbf{r} \mid \mathbf{u}) q(\mathbf{u})]\dx \\
                      &= \iint \frac{\nabla[v q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})]}{q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})} q(\mathbf{r} \mid \mathbf{u}) q(\mathbf{u})\dx \\
                      &\approx \frac{1}{S} \sum_{s=1}^S \frac{\nabla[v q(\mathbf{r}_s \mid \mathbf{u}_s)q(\mathbf{u}_s)]}{q(\mathbf{r}_s \mid \mathbf{u}_s)q(\mathbf{u}_s)},
\end{align*}
which can be computed by drawing $S$ samples $\{(\mathbf{u}_s,
\mathbf{r}_s)\}_{s=1}^S$ from $\approximation$.

Our main goal is Theorem~\ref{thm:main}, which allows us to move differentiation
inside the integral. In order to prove it, we use a number of intermediate
results. We begin by introducing a few important tools and techniques in
Section~\ref{sec:math_preliminaries}. In Section~\ref{sec:derivatives}, we state
a few derivatives of PDFs and covariance matrices and bound their values with
some easy-to-work-with polynomials. In Section~\ref{sec:propositions}, we
provide a sketch proof of the measurability of the MDP value function as well as
new upper and lower bounds on its values. After another quick lemma, the main
proof of the paper is in Section~\ref{sec:the_proof}. See
Figure~\ref{fig:theorems} for an overview of how these results fit together.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node [rectangle, draw, fill=lightgray] (perturbation) at (2, 0) {\textsc{Perturbation Lemma}};
    \node [rectangle, draw, fill=lightgray] (lebesgue) at (-1, -3) {\textsc{Dominated Convergence Theorem}};
    \node [rectangle, draw] (derivatives) at (-2, 0) {\textsc{Derivatives of PDFs}};
    \node [rectangle, draw] (measurability) at (-3, -2) {\textsc{Measurability}};
    \node [rectangle, draw] (boundedness) at (3, -2) {\textsc{Boundedness}};
    \node [rectangle, draw] (lemma1) at (-2, -1) {\textsc{Lemma~\ref{lemma:bound1}}};
    \node [rectangle, draw] (lemma2) at (0, -1) {\textsc{Lemma~\ref{lemma:bound2}}};
    \node [rectangle, draw] (lemma3) at (2, -1) {\textsc{Lemma~\ref{lemma:bound3}}};
    \node [rectangle, draw] (lemma4) at (3, -3) {\textsc{Lemma~\ref{lemma:integral_of_r}}};
    \node [rectangle, draw] (theorem) at (0, -2) {\textsc{Theorem~\ref{thm:main}}};

    \draw [->] (perturbation) edge (lemma3);
    \draw [->] (lebesgue) edge (theorem);
    \draw [->] (derivatives) edge (lemma1);
    \draw [->] (derivatives) edge (lemma2);
    \draw [->] (derivatives) edge (lemma3);
    \draw [->] (measurability) edge (theorem);
    \draw [->] (boundedness) edge (theorem);
    \draw [->] (lemma1) edge (theorem);
    \draw [->] (lemma2) edge (theorem);
    \draw [->] (lemma3) edge (theorem);
    \draw [->] (lemma4) edge (theorem);
  \end{tikzpicture}
  \caption{A graphical representation of dependencies between our theoretical
    results. An arrow from $A$ to $B$ means that $A$ was used to prove $B$.
    Results from the literature are in grey.}
  \label{fig:theorems}
\end{figure}

\subsection{Mathematical Preliminaries} \label{sec:math_preliminaries}

We introduce a few definitions and results from linear algebra, numerical
analysis, and measure theory that will be used later in the paper. Namely, we
will use several different vector and matrix norms, consider how an inverse of a
matrix changes with a small perturbation, and introduce Lebesgue's dominated
convergence theorem which plays a major role in Section~\ref{sec:the_proof}.

\begin{definition}[Norms]
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} (\emph{$\ell_\infty$-norm}) is
  \[
    \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|
  \]
  whereas its \emph{$\ell_1$-norm} is
  \[
    \lVert \mathbf{x} \rVert_1 = \sum_{i = 1}^n |x_i|.
  \]
  Let $\mathbf{A}$ be a matrix. For any vector norm $\lVert
  \cdot \rVert_p$, we can also define its \emph{induced norm} for matrices as
  \[
    \lVert \mathbf{A} \rVert_p = \sup_{\mathbf{x} \ne \mathbf{0}} \frac{\lVert
      \mathbf{Ax} \rVert_p}{\lVert \mathbf{x} \rVert_p}.
  \]
  In particular, for $p = \infty$, we have
  \[
    \lVert \mathbf{A} \rVert_\infty = \max_i \sum_{j} |A_{i,j}|.
  \]
\end{definition}

\begin{lemma}[Perturbation Lemma
  \cite{layton2014numerical}] \label{lemma:perturbation}
  Let $\lVert \cdot \rVert$ be any matrix norm, and let $\mathbf{A}$ and
  $\mathbf{E}$ be matrices such that $\mathbf{A}$ is invertible and $\lVert
  \mathbf{A}^{-1} \rVert \lVert \mathbf{E} \rVert < 1$. Then $\mathbf{A} +
  \mathbf{E}$ is invertible, and
  \[
    \lVert (\mathbf{A} + \mathbf{E})^{-1} \rVert \le \frac{\lVert
      \mathbf{A}^{-1} \rVert}{1 - \lVert \mathbf{A}^{-1} \rVert \lVert
      \mathbf{E} \rVert}.
  \]
\end{lemma}

\begin{theorem}[Dominated Convergence Theorem
  \cite{royden2010real}] \label{thm:lebesgue}
  Let $(X, \mathcal{M}, \mu)$ be a measure space and $\{ f_n \}$ a sequence of
  measurable functions on $X$ for which $\{ f_n \} \to f$ pointwise almost
  everywhere on $X$, and the function $f$ is measurable. Assume there is a
  non-negative function $g$ that is integrable over $X$ and dominates the
  sequence $\{ f_n \}$ on $X$ in the sense that
  \[
    |f_n| \le g \text{ almost everywhere on $X$ for all $n$.}
  \]
  Then $f$ is integrable over $X$ and
  \[
    \lim_{n \to \infty} \int_X f_n\,\mathrm{d}\mu = \int_X f\,\mathrm{d}\mu.
  \]
\end{theorem}

\subsection{Derivatives of PDFs and Their Bounds} \label{sec:derivatives}

The goal of this section is to find derivatives of $q(\mathbf{u})$ and
$q(\mathbf{r} \mid \mathbf{u})$ with respect to variational parameters and
bound their values. Note that throughout this section the word `constant' means
`constant with respect to $\mathbf{u}$ and $\mathbf{r}$'. We begin by
introducing a few extra variables in order to simplify expressions of
derivatives:
\begin{align*}
  \mathbf{U} &= (\mathbf{u} - \bm\mu)(\mathbf{u} - \bm\mu)^\intercal, \\
  \mathbf{S} &= \Kru^\intercal\Kuu^{-1}, \\
  \bm\Gamma &= \Krr - \mathbf{S}\Kru, \\
  \mathbf{R} &= \mathbf{S}\frac{\partial \Kru}{\partial \lambda_i} - \frac{\partial \Krr}{\partial \lambda_i} + \left( \frac{\partial \Kru^\intercal}{\partial \lambda_i} - \mathbf{S}\frac{\partial \Kuu}{\partial \lambda_i} \right) \Kuu^{-1}\Kru.
\end{align*}
Next, we state a few key derivatives, derivations of which can be found in
Appendix~\ref{appendix:proofs}.

\begin{restatable}[Derivatives of PDFs]{lemma}{derivatives} \label{lemma:derivatives}
  \leavevmode
  \begin{enumerate}
  \item $\frac{\partial q(\mathbf{u})}{\partial \bm\mu} =
    \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u}
    - \bm\mu)$.
  \item
    \begin{enumerate}
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} =
      \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
      \bm\Sigma^{-1})$.
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
      q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
      \bm\Sigma^{-1})\mathbf{B}$.
    \end{enumerate}
  \item For $i = 0, \dots, d$,
    \begin{enumerate}
    \item
      \begin{align*}
        \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i} &= \frac{1}{2}q(\mathbf{r} \mid \mathbf{u}) (|\bm\Gamma|^{-1} \tr(\mathbf{R} \adj(\bm\Gamma)) \\
                                                                          &- (\mathbf{r} - \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r} - \mathbf{Su})).
      \end{align*}
    \item For any covariance matrix $\mathbf{K}$,
      \[
        \frac{\partial \mathbf{K}}{\partial \lambda_i} =
        \begin{cases}
          \frac{1}{\lambda_i}\mathbf{K} & \text{if } i = 0, \\
          \mathbf{L} & \text{otherwise,}
        \end{cases}
      \]
      where
      \[
        L_{j,k} = k(\mathbf{x}_j, \mathbf{x}_k) \left( -\frac{1}{2}(x_{j,i} -
          x_{k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
      \]
    \end{enumerate}
  \end{enumerate}
\end{restatable}

\begin{lemma} \label{lemma:bound1}
  Let $i \in \{ 0, \dots, d \}$ be arbitrary, and let $c \colon
  \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (\lambda_i - \epsilon,
  \lambda_i + \epsilon) \subset \mathbb{R}$ be a function with a codomain
  arbitrarily close to $\lambda_i$. Then
  \[
    \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i}
    \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{r} \mid
  \mathbf{u})d(\mathbf{u})$, where $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Let $\mathbf{K}$ be any covariance matrix and set
  \[
    \mathbf{A} = \frac{1}{\lambda_0} \mathbf{K}.
  \]

  % K and its derivative
  First, we can easily deduce that\footnote{Note that we often switch between
    proving limits and bounds. We are fundamentally interested in proving
    constant bounds, but sometimes it is more convenient to show convergence
    instead. In this situation, convergence as $\epsilon \to 0$ implies constant
    upper and lower bounds.}
  \begin{equation} \label{eq:convergence1}
    \lim_{\epsilon \to 0} \mathbf{K}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} = \mathbf{K},
  \end{equation}
  and
  \begin{equation} \label{eq:convergence2}
    \lim_{\epsilon \to 0} \left. \frac{\partial \mathbf{K}}{\partial \lambda_i} \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} = \frac{\partial \mathbf{K}}{\partial \lambda_i}.
  \end{equation}
  For Equation~\eqref{eq:convergence1}, the result is obvious if $i = 0$.
  Otherwise, it follows from the continuity of the exponential function. For
  Equation~\eqref{eq:convergence2}, observe that if $i=0$, then
  \[
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_0} \right|_{\lambda_0 =
      c(\mathbf{r}, \mathbf{u})}
  \]
  as an expression contains no $\lambda_0$, so
  \[
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_0} \right|_{\lambda_0 =
      c(\mathbf{r}, \mathbf{u})} = \frac{\partial \mathbf{K}}{\partial
      \lambda_0}.
  \]
  Finally, if $i>0$, then each element of 
  \[
    \left. \frac{\partial \mathbf{K}}{\partial \lambda_i}
    \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}
  \]
  is a constant multiple of the corresponding element of
  \[
    \mathbf{K}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})},
  \]
  so the same reasoning applies as in case of Equation~\eqref{eq:convergence1}.

  % Inverse of a covariance matrix
  Next, we will show that $\mathbf{K}^{-1}|_{\lambda_i = c(\mathbf{r},
    \mathbf{u})}$ exists and
  \begin{equation} \label{eq:limit3}
    \lim_{\epsilon \to 0} \mathbf{K}^{-1}|_{\lambda_i = c(\mathbf{r},
      \mathbf{u})} = \mathbf{K}^{-1}.
  \end{equation}
  If $i = 0$, then $\mathbf{K}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
  c(\mathbf{r}, \mathbf{u})\mathbf{A}$. Therefore\footnote{Note that since
    $\lambda_0 > 0$, $c(\mathbf{r}, \mathbf{u}) \ne 0$ for small enough
    $\epsilon$.},
  \begin{align*}
    \mathbf{K}^{-1}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} &=
                                                               \frac{1}{c(\mathbf{r}, \mathbf{u})}\mathbf{A}^{-1} \\
                                                             &\to \frac{1}{\lambda_0}\mathbf{A}^{-1} = \frac{1}{\lambda_0} \left( \frac{1}{\lambda_0}\mathbf{K} \right)^{-1} = \mathbf{K}^{-1}
  \end{align*}
  as $\epsilon \to 0$. For $i > 0$, by Equation~\eqref{eq:convergence1} and
  continuity of $\mathbf{A} \mapsto \mathbf{A}^{-1}$ we immediate get
  Equation~\eqref{eq:limit3}.

  This is enough to prove constant upper and lower bounds on
  $\mathbf{S}$, $\bm\Gamma$, and $\mathbf{R}$ (all with $\lambda_i$ replaced
  with $c(\mathbf{r}, \mathbf{u})$), which means that $(\mathbf{r} -
  \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r} -
  \mathbf{Su})|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}$ has upper and lower
  bounds in $\mathbb{R}_2[\mathbf{u}]$. Furthermore, having convergence results
  for arbitrary covariance matrices and their inverses means that
  \begin{equation} \label{eq:gamma_convergence}
    \lim_{\epsilon \to 0} \bm\Gamma|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
    \bm\Gamma
  \end{equation}
  and, by continuity of the determinant function,
  \begin{equation} \label{eq:det_gamma}
    \lim_{\epsilon \to 0} \det(\bm\Gamma)|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
    \det(\bm\Gamma).
  \end{equation}
  Assuming that
  $\bm\Gamma$ is invertible so that $q(\mathbf{r} \mid \mathbf{u})$ exists,
  \[
    \det(\bm\Gamma)|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} \ne 0
  \]
  for small enough $\epsilon$, and, thus, $\det(\bm\Gamma)^{-1}|_{\lambda_i =
    c(\mathbf{r}, \mathbf{u})}$ exists and is bounded.
  
  From Equation~\eqref{eq:gamma_convergence} and continuity of $\mathbf{A} \mapsto
  \mathbf{A}^{-1}$ we can immediately deduce that
  \begin{equation} \label{eq:gamma_inverse}
    \lim_{\epsilon \to 0} \bm\Gamma^{-1}|_{\lambda_i = c(\mathbf{r},
      \mathbf{u})} = \bm\Gamma^{-1}.
  \end{equation}
  Similarly, from Equations~\eqref{eq:det_gamma} and \eqref{eq:gamma_inverse} we
  get that
  \[
    \adj(\bm\Gamma)|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} =
    \det(\bm\Gamma)\bm\Gamma^{-1}|_{\lambda_i = c(\mathbf{r}, \mathbf{u})} \to
    \det(\bm\Gamma)\bm\Gamma^{-1}
  \]
  as $\epsilon \to 0$. This gives us constant bounds on
  \[
    |\bm\Gamma|^{-1}\tr(\mathbf{R}\adj(\bm\Gamma))|_{\lambda_i = c(\mathbf{r},
      \mathbf{u})},
  \]
  which completes the proof that
  \[
    \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i}
    \right|_{\lambda_i = c(\mathbf{r}, \mathbf{u})}
  \]
  has the required quadratic bounds.
\end{proof}

\begin{remark}
  In order to find a derivative such as $\frac{\partial q(\mathbf{u})}{\partial
    \mu_i}$, we can find $\frac{\partial q(\mathbf{u})}{\partial \bm\mu}$ and
  simply take the $i$th element. A similar line of reasoning applies to matrices
  as well. Thus, we only need to consider derivatives with respect to $\bm\mu$
  and $\bm\Sigma$.
\end{remark}

\begin{lemma} \label{lemma:bound2}
  Let $c \colon \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (a, b) \subset
  \mathbb{R}$ be an arbitrary bounded function. Then, for $i = 1, \dots, m$,
  every element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$,
  where $d(\mathbf{u}) \in \mathbb{R}_1[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma~\ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})} = \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
    \bm\Sigma^{-\intercal})(\mathbf{u} - \mathbf{c}(\mathbf{r}, \mathbf{u})),
  \]
  where $\mathbf{c}(\mathbf{r}, \mathbf{u}) = (\mu_1, \dots, \mu_{i - 1},
  c(\mathbf{r}, \mathbf{u}), \mu_{i + 1} \dots, \mu_m)^\intercal$. Since
  $c(\mathbf{r}, \mathbf{u})$ is bounded and $\bm\Sigma^{-1} +
  \bm\Sigma^{-\intercal}$ is a constant matrix, we can use the bounds on
  $c(\mathbf{r}, \mathbf{u})$ to manufacture both upper and lower bounds on
  \[
     \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  of the required form.
\end{proof}

\begin{lemma} \label{lemma:bound3}
  Let $i, j = 1, \dots, m$, and let
  \[
    c \colon \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (\Sigma_{i,j} -
    \epsilon, \Sigma_{i,j} + \epsilon) \subset \mathbb{R}
  \]
  be a function with a codomain arbitrarily close to $\Sigma_{i,j}$. Then every
  element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} \right|_{\Sigma_{i,j} =
    c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$, where
  $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma~\ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} \right|_{\Sigma_{i,j} =
    c(\mathbf{r}, \mathbf{u})} =
    \frac{1}{2}q(\mathbf{u})(\mathbf{C}(\mathbf{r},
    \mathbf{u})^{-1}\mathbf{UC}(\mathbf{r}, \mathbf{u})^{-1} -
    \mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}),
  \]
  where
  \[
    [\mathbf{C}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) & \text{if } (k, l) = (i, j), \\
      \Sigma_{k,l} & \text{otherwise.}
    \end{cases}
  \]
  We can also express $\mathbf{C}(\mathbf{r},\mathbf{u})$ as
  $\mathbf{C}(\mathbf{r}, \mathbf{u}) = \bm\Sigma + \mathbf{E}(\mathbf{r},
  \mathbf{u})$, where
  \[
    [\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) - \Sigma_{i,j} & \text{if } (k, l) = (i, j), \\
      0 & \text{otherwise.}
    \end{cases}
  \]
  We will show how we can establish bounds on $\mathbf{C}(\mathbf{r},
  \mathbf{u})^{-1}$ without using the continuity of matrix inversion. For this,
  we use the maximum norm $\lVert \cdot \rVert_\infty$ on both vectors and
  matrices. We can apply Lemma~\ref{lemma:perturbation} to $\bm\Sigma$ and
  $\mathbf{E}(\mathbf{r}, \mathbf{u})$ since
  \[
    \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty = \max_k \sum_l
    |[\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l}| = |c(\mathbf{r}, \mathbf{u}) -
    \Sigma_{i,j}| < \epsilon \to 0,
  \]
  ensuring that $\lVert \bm\Sigma^{-1} \rVert_\infty
  \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty < 1$. Then
  $\mathbf{C}(\mathbf{r}, \mathbf{u})$ is invertible, and
  \[
    \lVert \mathbf{C}(\mathbf{r}, \mathbf{u})^{-1} \rVert_\infty \le
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \lVert \bm\Sigma^{-1}
      \rVert_\infty \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty} <
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \epsilon \lVert
      \bm\Sigma^{-1} \rVert_\infty},
  \]
  which means that
  \[
    \max_k \sum_l \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right|
    < \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \epsilon \lVert
      \bm\Sigma^{-1} \rVert_\infty},
  \]
  i.e., for any row $k$ and column $l$,
  \[
    \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right| <
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \epsilon \lVert
      \bm\Sigma^{-1} \rVert_\infty},
  \]
  which bounds all elements of $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ as
  required. Since every element of $\mathbf{U} = (\mathbf{u} - \bm\mu)(\mathbf{u} -
  \bm\mu)^\intercal$ is in $\mathbb{R}_2[\mathbf{u}]$, and the elements of
  $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ are bounded, the desired result
  follows.
\end{proof}

\subsection{The MDP Value Function} \label{sec:propositions}

In this section, we present two results that are necessary for the main proof in
Section \ref{sec:the_proof} but also interesting in their own right. Firstly, we
sketch a proof for the measurability of the MDP value function. Afterwards, we
establish upper and lower bounds on said function.

\begin{remark}
  MDP values are characterised by both a state and a reward function/vector. In
  this section, we will think of the value function as $V \colon \mathcal{S} \to
  \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$, i.e., $V$ takes a state $s \in
  \mathcal{S}$ and returns a function $V(s) \colon \mathbb{R}^{|\mathcal{S}|}
  \to \mathbb{R}$ that takes a reward vector $\mathbf{r} \in
  \mathbb{R}^{|\mathcal{S}|}$ and returns a value of the state $s$, $\V(s) \in
  \mathbb{R}$. Given a reward vector, the function $V(s)$ computes the values of
  all states and returns the value of state $s$.
\end{remark}

\begin{proposition}[Measurability] \label{thm:measurability}
  MDP value functions $V(s) \colon \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$
  (for $s \in \mathcal{S}$) are Lebesgue measurable.
\end{proposition}
\begin{proof}
  For any reward vector $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$, the
  set of converged value functions $\{ \V(s) \mid s \in
  \mathcal{S} \}$ satisfy
  \begin{equation} \label{eq:linear_mdp}
    \V(s) = \log \sum_{a \in \mathcal{A}}
    \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
      s')\V(s') \right)
  \end{equation}
  for all $s \in \mathcal{S}$. Let $s_0 \in \mathcal{S}$ be an arbitrary state.
  In order to prove that $V(s_0)$ is measurable, it is enough to show that for
  any $\alpha \in \mathbb{R}$, the set
  \[
    \left\{ \mathbf{r} \in \mathbb{R}^{|\mathcal{S}|} \; \middle|
    \begin{aligned}
      \;&\V(s_0) \in (-\infty, \alpha); \\
      \;&\V(s) \in \mathbb{R} \text{ for all } s \in \mathcal{S} \setminus \{ s_0 \}; \\
      \;&\text{Equation~\eqref{eq:linear_mdp} is satisfied by all } s \in
      \mathcal{S}
    \end{aligned}
    \right\}
  \]
  is measurable. Since this set can be constructed in Zermelo-Fraenkel set
  theory \emph{without} the axiom of choice, it is measurable
  \cite{herrlich2006axiom}, which proves that $V(s)$ is a measurable function
  for any $s \in \mathcal{S}$.
\end{proof}

\begin{proposition}[Boundedness] \label{thm:bound}
  If the initial values of the MDP value function satisfy the following
  bound, then the bound remains satisfied throughout value iteration:
  \[
    |\V(s)| \le \vbound.
  \]
\end{proposition}
\begin{proof}
  We begin by considering the desired inequality without taking the absolute
  value of $\V(s)$, i.e.,
  \begin{equation} \label{eq:positive_bound}
    \V(s) \le \vbound,
  \end{equation}
  and assuming that the initial values of $\{ \V(s) \mid s \in \mathcal{S} \}$
  already satisfy Inequality~\eqref{eq:positive_bound}. We will show that it
  remains satisfied using an inductive argument. Recall that for each $s \in
  \mathcal{S}$, the value of $\V(s)$ is updated by applying
  Equation~\eqref{eq:update_rule}. Note that both the natural logarithm and the
  exponential function are increasing, $\gamma > 0$, and the $\mathcal{T}$
  function gives a probability (a non-negative number). Thus,
  \begin{align*}
    \V(s) &\le \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s')\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma} \right) \\
          &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma}\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s') \right) \\
          &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right)
  \end{align*}
  by the definition of $\mathcal{T}$. Then
  \begin{align*}
    \V(s) &\le \log \left( |\mathcal{A}| \exp\left( r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
          &= \log \left( \exp\left( \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
          &= \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \\
          &= \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 - \gamma)(\log|\mathcal{A}| + r(s))}{1 - \gamma} \\
          &\le \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 - \gamma)(\log|\mathcal{A}| + \rinf)}{1 - \gamma} \\
          &= \vbound
  \end{align*}
  by the definition of $\rinf$.

  The proof for
  \begin{equation} \label{eq:negative_bound}
    \V(s) \ge \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1}
  \end{equation}
  follows the same argument until we get to
  \begin{align*}
    \V(s) &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma - 1)(\log|\mathcal{A}| + r(s))}{\gamma - 1} \\
          &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma - 1)(-\log|\mathcal{A}| -\rinf)}{\gamma - 1} \\
          &= \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1},
  \end{align*}
  where we use the fact that $r(s) \ge -\rinf - 2\log|\mathcal{A}|$. Combining
  Inequalities~\eqref{eq:positive_bound} and \eqref{eq:negative_bound} gives
  the desired result.
\end{proof}

\begin{lemma} \label{lemma:integral_of_r}
  \[
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r} \mid
    \mathbf{u})\,\mathrm{d}\mathbf{r} \le a + \lVert \mathbf{Su} \rVert_1,
  \]
  where $a$ is a constant independent of $\mathbf{u}$.
\end{lemma}
\begin{proof}
  Since $\rinf \le \lVert \mathbf{r} \rVert_1$,
  \[
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r} \mid
    \mathbf{u})\,\mathrm{d}\mathbf{r} \le \int \lVert \mathbf{r} \rVert_1
    q(\mathbf{r} \mid \mathbf{u})\,\mathrm{d}\mathbf{r} =
    \sum_{i=1}^{|\mathcal{S}|} \mathbb{E}[|r_i|].
  \]
  As each $\mathbb{E}[|r_i|]$ is a mean of a folded Gaussian distribution,
  \[
    \mathbb{E}[|r_i|] = \sigma_i \sqrt{\frac{2}{\pi}} \exp
    \left(-\frac{\xi_i^2}{2\sigma_i^2} \right) + \xi_i \left( 1 - 2\Phi \left(
        -\frac{\xi_1}{\sigma_1} \right) \right),
  \]
  where $\xi_i = \left[\mathbf{Su}\right]_i$, $\sigma_i =
  \sqrt{\bm\Gamma_{i,i}}$\footnote{The expression
    under the square root sign is non-negative because $\bm\Gamma$ is a
    covariance matrix of a Gaussian distribution, hence also positive
    semi-definite, which means that its diagonal entries are non-negative.}, and
  $\Phi$ is the cumulative distribution function of the standard Gaussian.
  Furthermore,
  \[
    \mathbb{E}[|r_i|] \le \sigma_i\sqrt{\frac{2}{\pi}} + |\xi_i|,
  \]
  as $\sigma_i$ is non-negative, and $\Phi(x) \in [0, 1]$ for all $x$. Since
  \[
    \sum_{i=1}^{|\mathcal{S}|} |\xi_i| = \lVert \mathbf{Su} \rVert_1,
  \]
  we can set
  \[ a = \sum_{i=1}^{|\mathcal{S}|} \sigma_i \sqrt{\frac{2}{\pi}} \]
  to get the desired result.
\end{proof}

\subsection{Differentiability} \label{sec:the_proof}

Our main theorem is a specialised version of an integral differentiation result
by Timoney \cite{lecture_notes}.
\begin{theorem} \label{thm:main}
  Whenever the derivative exists,
  \[
    \dt\iint
    \V(s)q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})\dx
    = \iint
    \dt[\V(s)q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u})]\dx,
  \]
  where $t$ is any scalar part of $\bm\mu$, $\bm\Sigma$, or $\bm\lambda$.
\end{theorem}
\begin{proof}
  Let
  \begin{align*}
    \f &= \V(s)q(\mathbf{r} \mid \mathbf{u})q(\mathbf{u}), \\
    F(t) &= \iint \f\dx,
  \end{align*}
  and fix the value of $t$. Let $(t_n)_{n=1}^\infty$ be any sequence such that
  $\lim_{n \to \infty} t_n = t$, but $t_n \ne t$ for all $n$. We want to show
  that
  \begin{equation} \label{eq:to_prove}
    F'(t) = \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \iint \df\dx.
  \end{equation}
  We have
  \[
    \begin{split}
      \frac{F(t_n) - F(t)}{t_n - t} &= \iint \frac{\ftn - \f}{t_n - t}\dx \\
      &= \iint \fn\dx,
    \end{split}
  \]
  where
  \[
    \fn = \frac{\ftn - \f}{t_n - t}.
  \]
  Since
  \[
    \lim_{n \to \infty} \fn = \df,
  \]
  Equation~\eqref{eq:to_prove} follows from Theorem~\ref{thm:lebesgue} as soon
  as we show that both $f$ and $f_n$ are measurable and find a non-negative
  integrable function $g$ such that for all $n$, $\mathbf{r}$, $\mathbf{u}$,
  \[
    |\fn| \le \g.
  \]
  The MDP value function is measurable by Proposition~\ref{thm:measurability}.
  The result of multiplying or adding measurable functions (e.g., PDFs) to a
  measurable function is still measurable. Thus, both $f$ and $f_n$ are
  measurable.

  It remains to find $g$. For notational simplicity and without loss of
  generality, we will temporarily assume that $t$ is a parameter of
  $q(\mathbf{r} \mid \mathbf{u})$. Then
  \[
    |\fn| = |\V(s)| \left| \frac{q(\mathbf{r} \mid \mathbf{u})|_{t =
          t_n} - q(\mathbf{r} \mid \mathbf{u})}{t_n - t} \right| q(\mathbf{u}),
  \]
  since PDFs are non-negative. An upper bound for
  $|\V(s)|$ is given by Proposition~\ref{thm:bound}, while
  \[
    \frac{q(\mathbf{r} \mid \mathbf{u})|_{t = t_n} - q(\mathbf{r} \mid
      \mathbf{u})}{t_n - t} = \left. \frac{\partial q(\mathbf{r} \mid
        \mathbf{u})}{\partial t} \right|_{t = c(\mathbf{r}, \mathbf{u})}
  \]
  for some function $c \colon \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to
  (\min\{t, t_n\}, \max\{t, t_n\})$ due to the mean value theorem (since $q$ is
  a continuous and differentiable function of $t$, regardless of the specific
  choices of $q$ and $t$).

  We then have that
  \[
    |\fn| \le \vbound \left| \left. \frac{\partial
          q(\mathbf{r} \mid \mathbf{u})}{\partial t} \right|_{t=c(\mathbf{r}, \mathbf{u})}
    \right| q(\mathbf{u}).
  \]
  The bound is clearly non-negative and measurable. It remains to show that it
  is also integrable. Depending on what $t$ represents, we can use one of the
  Lemmas~\ref{lemma:bound1}, \ref{lemma:bound2}, and \ref{lemma:bound3}, which
  gives us two polynomials $p_1(\mathbf{u}), p_2(\mathbf{u}) \in
  \mathbb{R}_2[\mathbf{u}]$ such that
  \[
    p_1(\mathbf{u})q(\mathbf{r} \mid \mathbf{u}) < \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial
        t} \right|_{t=c(\mathbf{r}, \mathbf{u})} < p_2(\mathbf{u})q(\mathbf{r} \mid \mathbf{u}).
  \]
  Then
  \[
    \left| \left. \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial t}
      \right|_{t=c(\mathbf{r}, \mathbf{u})} \right| < q(\mathbf{r} \mid \mathbf{u}) \max \{
    |p_1(\mathbf{u})|, |p_2(\mathbf{u})| \}.
  \]
  We can now apply Lemma~\ref{lemma:integral_of_r}, which allows us to integrate
  out $\mathbf{r}$, and we are left with showing the existence of
  \begin{equation} \label{eq:remaining_integral}
    \int \left( a + \lVert \mathbf{Su} \rVert_1 \right) \max \{|p_1(\mathbf{u})|, |p_2(\mathbf{u})| \} q(\mathbf{u})\,\mathrm{d}\mathbf{u},
  \end{equation}
  where $a$ is a constant. The integral
  \[
    \int \max \left\{
      \begin{aligned}
        &|p_1(\mathbf{u})|, \\
        &|p_2(\mathbf{u})|
      \end{aligned}
    \right\} q(\mathbf{u})\,\mathrm{d}\mathbf{u} = \int \max \left\{
      \begin{aligned}
        &|p_1(\mathbf{u})q(\mathbf{u})|, \\
        &|p_2(\mathbf{u})q(\mathbf{u})|
      \end{aligned}
    \right\}\,\mathrm{d}\mathbf{u}
  \]
  exists because $p_1(\mathbf{u})q(\mathbf{u})$ and
  $p_2(\mathbf{u})q(\mathbf{u})$ are both integrable, hence their absolute
  values are integrable, and the maximum of two integrable functions is also
  integrable. Since $\lVert \mathbf{Su} \rVert_1 \in \mathbb{R}_1[\mathbf{u}]$,
  a similar argument can be applied to the rest of
  Integral~\eqref{eq:remaining_integral} as well.
\end{proof}

Having proven that differentiation can be moved inside the integral allows us to
use $\nabla\mathcal{L}$ for optimisation, knowing that our approach is sound.
This allows us to implement a simple optimisation algorithm, the description of
which is in the next section.

\section{Experiments} \label{sec:evaluation}
% showing deep understanding

The main goal of this section is to use experimental evidence to show that the
implementation agrees with the theory and to investigate its properties such as
convergence and how the learned model changes with varying amounts and types of
data. We begin with an outline of the implementation in
Algorithm~\ref{alg:optimisation}. The algorithm was implemented in MATLAB to be
compatible with the
toolkit\footnote{\url{https://graphics.stanford.edu/projects/gpirl/}} for
evaluating IRL algorithms developed by Levine et al.
\cite{DBLP:conf/nips/LevinePK11}. We decided to use a constant step size because
the stochastic nature of the gradient makes dynamic step size algorithms such as
AdaGrad \cite{DBLP:journals/jmlr/DuchiHS11} and AdaDelta
\cite{DBLP:journals/corr/abs-1212-5701} ineffective, as spikes in the gradient
estimates drive the step size down to zero. Similarly, although we check for
convergence using the $\ell_\infty$-norm, the algorithm usually only stops when
the maximum number of iterations is exhausted. Throughout the experiments, we
discard runs that result in a near-singular $\bm\Sigma$ or $\bm\Gamma$,
repeatedly testing each combination of parameters as many times as necessary
until a non-failing run is achieved.

\begin{algorithm}
  \SetKwData{parameters}{parameters}
  \SetKwData{stepSize}{stepSize}
  \SetKwData{ttolerance}{tolerance}
  \SetKwData{numIterations}{numIterations}
  \SetKwFunction{estimatedPart}{estimatedPart}
  \SetKwFunction{exactPart}{exactPart}
  \SetKwFunction{pack}{pack}
  \SetKwFunction{unpack}{unpack}
  \SetKwFunction{valueIteration}{valueIteration}
  \SetKwFunction{transformGradient}{transformGradient}
  \SetKwProg{Function}{Function}{}{end}
  \KwData{number of samples $S$, constant step size \stepSize, convergence
    criteria \ttolerance and \numIterations}
  \KwResult{optimised \parameters}
  initialise \parameters as in Equations~\eqref{eq:first_initialisation}--\eqref{eq:last_initialisation}\;
  \For{$i \gets 1$ \KwTo \numIterations}{
    $\{ \mathbf{u}_s \}_{s=1}^S \sim \mathcal{N}(\parameters.\bm\mu, \parameters.\bm\Sigma)$\;
    $\nabla\mathcal{L} \gets \exactPart{\parameters} + \frac{1}{S} \sum_{s=1}^S\estimatedPart{\parameters, $\mathbf{u}_s$}$\;
    $\mathbf{v} \gets$ \pack{\parameters}\;
    $\mathbf{v}' \gets \mathbf{v} + \stepSize \times \transformGradient{$\nabla\mathcal{L}$}$\;
    $\parameters \gets \unpack{$\mathbf{v}'$}$\;
    \lIf{$\lVert \mathbf{v}' - \mathbf{v} \rVert_\infty < \ttolerance$}{break}
  }
  \Function{\estimatedPart{\parameters, $\mathbf{u}$}}{
    $\mathbf{r} \sim \mathcal{N}(\parameters.\mathbf{Su}, \parameters.\bm\Gamma)$\;
    $\V \gets \valueIteration{$\mathbf{r}$}$\;
    \Return{the estimated part of $\nabla\mathcal{L}$ according to $\mathbf{u}$,
      $\mathbf{r}$, $\V$, and \parameters\;}
  }
  \caption{The optimisation procedure. All parameters are kept in a single variable
    \textsf{parameters}. Converting it to/from vector form and taking logarithms,
    exponentiating, and using Equation~\eqref{eq:transformation} to handle the
    optimisation of parameters restricted to positive values is handled by
    functions \texttt{pack}, \texttt{unpack}, and \texttt{transformGradient}.
    Each derivative from Appendix~\ref{appendix:derivatives} is split into parts
    outside and inside $\mathbb{E}[\cdot]$ that are handled by functions
    \texttt{exactPart} and \texttt{estimatedPart}, respectively. The function
    \texttt{valueIteration} simply applies Equation~\eqref{eq:update_rule} until
    convergence.}
  \label{alg:optimisation}
\end{algorithm}

We will first focus on a simple three-state MDP where the agent can
deterministically move from any state to any other state. More formally, we set
$\mathcal{S} = \{s_1, s_2, s_3 \}$, $\mathcal{A} = \{ a_1, a_2 \}$,
\begin{align*}
  &\mathcal{T}(s_1, a_1, s_2) = 1, \quad \mathcal{T}(s_1, a_2, s_3) = 1, \\
  &\mathcal{T}(s_2, a_1, s_1) = 1, \quad \mathcal{T}(s_1, a_2, s_3) = 1, \\
  &\mathcal{T}(s_3, a_1, s_1) = 1, \quad \mathcal{T}(s_1, a_2, s_2) = 1,
\end{align*}
all other values of $\mathcal{T}$ to zero, and $\gamma = 0.9$. We
also set the inducing points to be equal to the three states in $\mathcal{S}$,
add a single feature $f \colon \mathcal{S} \to \mathbb{R}$ such that
\[
  f(s_1) = 1, \quad f(s_2) = 2, \quad f(s_3) = 3,
\]
and create two demonstrations $\zeta_1 = \{ (s_1, a_1) \}$ and $\zeta_2 = \{
(s_3, a_2) \}$ that correspond to moving from $s_1$ and $s_3$ to $s_2$. With
this setup, we would expect the reward of $s_2$ to be higher than the other two
rewards.

\paragraph{Convergence}

We plot how $\mathcal{L}$ as well as policies $\pi(a_1 \mid s_1)$, $\pi(a_2 \mid
s_3)$, and $\pi(a_1 \mid s_2)$ converge over a number of iterations in
Figure~\ref{fig:convergence1}. The first two policies
correspond to actions taken in the set of demonstrations $\mathcal{D}$, so we
would expect to see their probabilities converge to values near $1$. The third
policy, however, has no data in $\mathcal{D}$ to guide its value, so the maximum
causal entropy framework would put the probability at around $0.5$. The first
thing to note is that while $\mathcal{L}$ converges in just a few iterations,
policies continue to improve for much longer. In addition, all policies behave
as expected: $\pitwo$ stays around $0.5$, while both $\pione$ and $\pithree$
keep increasing. Although they stay at around $0.75$ (and not 1), this may be
optimal for the maximum causal entropy model (and this question will be
addressed in more detail with a different experiment).

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/convergence_new}
  \caption{Convergence of $\mathcal{L}$ (at the top) as well as several example
    policies (at the bottom)}
  \label{fig:convergence1}
\end{figure}

We also plot how the parameters of the model converge in
Figure~\ref{fig:convergence2}. While some parameters could benefit from a higher
number of iterations\footnote{The limit of 300 iterations was chosen so that the
  initial increase in $\mathcal{L}$ would not be overshadowed by a long straight
  line afterwards.}, none look alarmingly wrong. Note that $\mu_2$, a variable
closely related to $r(s_2)$, stabilises at a positive value whereas both $\mu_1$
and $\mu_2$ move to negative values as expected.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/parameter_convergence_new}
  \caption{Convergence of variational parameters. In order to represent
    different variables on the same scale, some variables have been
    $\log$-transformed. Colours denote which vector or matrix each scalar comes
    from: black for $\bm\lambda$, \textcolor{blue}{blue} for $\bm\mu$,
    \textcolor{red}{red} for diagonal elements of $\mathbf{B}$, and
    \textcolor{green}{green} for its non-diagonal elements.}
  \label{fig:convergence2}
\end{figure*}

\paragraph{Adding More Data}

We would expect policies to converge closer to extreme values (i.e., 0 and 1) as
we add more data to the model. We test this hypothesis with a small-scale
experiment in Figure~\ref{fig:policies}. The policy $\pitwo$, in particular,
confirms our prediction. As this policy denotes the probability of going from
$s_2$ to $s_1$, we expect it to stay around $0.5$ as long as we have equal
amounts of $\zeta_1$ and $\zeta_2$---this corresponds to plots at coordinates
$(1, 1)$, $(2, 2)$, and $(3, 3)$. Moreover, as we increase the number of times
state $s_1$ appears in our demonstrations (going to the right across any row of
plots), the model recognises the increasing value of state $s_1$, and the
probability increases. In contrast, going up across the plots reduces the
probability, as state $s_3$ becomes more valuable, and the model prefers it over
state $s_1$. Interestingly, the other two policies seem to increase regardless
of what data is added. This is likely due to the fact that a higher number of
demonstrations results in higher gradient values, and thus leads to
better-converged policies.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \csvreader[
    no head,
    column count=11
    ]{data/covariance_and_policy_transformed.csv}{1=\x, 2=\y, 3=\Soneone, 4=\Stwotwo, 5=\Sthreethree, 6=\Stwoone, 7=\Sthreeone, 8=\Sthreetwo, 9=\ponetwo, 10=\ptwoone, 11=\pthreetwo}{%
      \begin{axis}[
        ybar=0,
        symbolic x coords={a,b,c},
        ticks=none,
        ymax=1,
        ymin=0,
        x=20,
        bar width=20,
        enlarge x limits=0.25,
        at={(350*\x, 110*\y)},
        height=100,
        bar shift=0pt,
        legend style={
          legend columns=-1,
          at={(1.7,-0.4)}
        }
        ]
        \addplot coordinates {(a, \ponetwo)};
        \addplot coordinates {(b, \ptwoone)};
        \addplot coordinates {(c, \pthreetwo)};
        \draw[dashed] (-50,50) -- (270,50);
        \ifthenelse{\x=2 \AND \y=1}{
          \legend{$\pione$,$\pitwo$,$\pithree$};
        }{}
      \end{axis}
    }
    \draw [<->] (2.3, 8.5) node (yaxis) [above] {$\zeta_2$}
    |- (10, 2) node (xaxis) [right] {$\zeta_1$};
    \foreach \x in {1, 2, 3} {
      \node (x\x) at (1+2.5*\x, 1.7) {$\x$};
    }
    \foreach \y in {1, 2, 3} {
      \node (y\y) at (2.1, 1+2.1*\y) {$\y$};
    }
  \end{tikzpicture}
  \caption{Changes in policies after adding more demonstrations. The $x$-axis
    ($y$-axis) shows how many copies of $\zeta_1$ ($\zeta_2$) are in
    $\mathcal{D}$. Each plot shows the values of three policies after 300
    iterations (averaged out over ten runs). The bottom (top) of each bar plot
    corresponds to probability 0 (1), and dashed lines mark probability
    0.5.}
  \label{fig:policies}
\end{figure}

We would also like to see similar changes in reward covariances. Firstly, we
will show how to derive the covariance matrix for $\mathbf{r}$ from the
posterior distributions of $\mathbf{u}$ and $\mathbf{r} \mid \mathbf{u}$.
Remember that $\mathbf{u} \sim \mathcal{N}(\bm\mu, \bm\Sigma)$ and $\mathbf{r}
\mid \mathbf{u} \sim \mathcal{N}(\mathbf{Su}, \bm\Gamma)$, and let $\mathbf{r}
\sim \mathcal{N}(\bm\mu', \bm\Sigma')$, where our goal is to find $\bm\mu'$ and
$\bm\Sigma'$. Then,
\[
  \bm\mu' = \mathbb{E}[\mathbf{r}] = \mathbb{E}[\mathbf{Su}] = \mathbf{S}\bm\mu.
\]
Furthermore,
\begin{align*}
  \mathbb{E}_{\mathbf{r}}[\mathbf{rr}^\intercal] &= \bm\Gamma + \mathbb{E}_{\mathbf{u}}[\mathbf{Suu}^\intercal\mathbf{S}^\intercal] = \bm\Gamma + \mathbf{S}\mathbb{E}_{\mathbf{u}}[\mathbf{uu}^\intercal]\mathbf{S}^\intercal \\
                                                 &= \bm\Gamma + \mathbf{S}(\bm\Sigma + \bm\mu\bm\mu^\intercal)\mathbf{S}^\intercal
\end{align*}
by an identity for $\mathbb{E}[\mathbf{xx}^\intercal]$ by Petersen and Pedersen
\cite{petersen2008matrix}. Now we can find the covariance matrix as follows:
\begin{align*}
  \bm\Sigma' &= \mathbb{E}[(\mathbf{r} - \mathbf{S}\bm\mu)(\mathbf{r} -
               \mathbf{S}\bm\mu)^\intercal] \\
             &= \mathbb{E}[\mathbf{rr}^\intercal - \mathbf{r}\bm\mu^\intercal\mathbf{S}^\intercal - \mathbf{S}\bm\mu\mathbf{r}^\intercal + \mathbf{S}\bm\mu\bm\mu^\intercal\mathbf{S}^\intercal] \\
             &= \bm\Gamma + \mathbf{S}\bm\Sigma\mathbf{S}^\intercal.
\end{align*}
Therefore, $\mathbf{r} \sim \mathcal{N}(\mathbf{S}\bm\mu, \bm\Gamma +
\mathbf{S}\bm\Sigma\mathbf{S}^\intercal)$.

Secondly, we normalise the elements of this new covariance matrix into the
interval $[-1, 1]$ for visualisation purposes. Diagonal (state) and non-diagonal
(edge) covariances are normalised separately in order to represent the full
range of values more clearly. All covariances are normalised according to this
rule:
\[
  x \mapsto
  \begin{cases*}
    x/M & if $x \ge 0$ \\
    -x/m & otherwise,
  \end{cases*}
\]
where $M$ is the maximum (diagonal or non-diagonal) covariance across all data,
and $m$ is the minimum. This way, all covariances are scaled to $[-1, 1]$, with
both extreme points guaranteed to be reached unless all values are on one side
of zero on the real number axis.

However, the resulting plot in Figure~\ref{fig:covariance} shows no clear
pattern, suggesting that adding more of the same demonstrations does not result
in lower covariances. The only other observation from this plot is that there
are only two instances of negative covariance, and both of them are between
states $s_2$ and $s_3$.

\begin{figure}[!t]
  \centering
  \begin{tikzpicture}[
    /pgfplots/colormap/violet,
    ellipse2/.style={
      draw,
      circle,
      /utils/exec={
        \pgfplotscolormapdefinemappedcolor{#1}},
      fill=mapped color},
    edge2/.style={
      ultra thick,
      /utils/exec={
        \pgfplotscolormapdefinemappedcolor{#1}},
      draw=mapped color}]
    \csvreader[
    no head,
    column count=11
    ]{data/median_covariance_transformed.csv}{1=\x, 2=\y, 3=\Soneone, 4=\Stwotwo, 5=\Sthreethree, 6=\Stwoone, 7=\Sthreeone, 8=\Sthreetwo}{%
      \node[ellipse2=\Soneone] (s1) at (2*\x, 2*\y) {$s_1$};
      \node[ellipse2=\Stwotwo] (s2) at (2*\x+0.5, 2*\y+1) {$s_2$};
      \node[ellipse2=\Sthreethree] (s3) at (2*\x+1, 2*\y) {$s_3$};
      \draw[edge2=\Stwoone] (s1) -- (s2);
      \draw[edge2=\Sthreeone] (s1) to (s3);
      \draw[edge2=\Sthreetwo] (s2) to (s3);
    }
    \draw [<->] (1.5, 7.5) node (yaxis) [above] {$\zeta_2$}
    |- (8, 1.5) node (xaxis) [right] {$\zeta_1$};
    \foreach \x in {1, 2, 3} {
      \draw[shift={(0.5+2*\x, 1.5)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\x$};
    }
    \foreach \y in {1, 2, 3} {
      \draw[shift={(1.5, 0.5+2*\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\y$};
    }
    \begin{axis}[
      colormap/violet,
      scale only axis,
      width=0pt,
      height=0pt,
      hide axis,
      colorbar horizontal,
      point meta min=-1,
      point meta max=1,
      xshift=40,
      yshift=30,
      colorbar style={
        width=6.5cm,
        xtick={-1,0,1},
        xlabel=normalised covariance,
      }]
      \addplot [draw=none] coordinates {(0,0)};
    \end{axis}
  \end{tikzpicture}
  \caption{Changes in reward covariances after adding more demonstrations. The
    meaning of external axes is the same as in Figure~\ref{fig:policies}.
    Colours denote reward covariance values, e.g., the colour of node $s_1$
    denotes the variance of $r(s_1)$, and the colour of the edge between nodes
    $s_1$ and $s_2$ denotes the covariance between $r(s_1)$ and $r(s_2)$. The
    colours represent median covariances across ten runs, normalised to the
    interval $[-1, 1]$ while preserving their positivity/negativity.}
  \label{fig:covariance}
\end{figure}

\paragraph{Structure versus Randomness}

If having more copies of the same demonstrations has no effect on covariances,
we can test if covariances tend to be lower in situations that exhibit more
structure, i.e., where demonstrations provide a clearer picture of which states
are more valuable. For this, we consider a completely deterministic MDP with ten
states and nine actions that allow the agent to move from any state to any other
state---the \emph{clique MDP}. We denote the states by $\mathcal{S} = \{ s_i
\}_{i=0}^9$, and, similarly to the previous example, set up a single feature $f$
such that $f(s_i) = i$ for $i = 0, \dots, 9$. Finally, $\gamma = 0.9$ as before.
We consider two scenarios, with 100 randomly generated demonstrations in each.
The scenarios are visualised in Figure~\ref{fig:cliques}, although we only show
a small subset of demonstrations. In the first scenario, we draw both the
starting state and the action uniformly at random. In the second scenario, we
draw the starting state uniformly from $\mathcal{S} \setminus \{ s_0 \}$, and
the action always points to $s_0$. We would expect at least variances of state
rewards to be lower in the second scenario in order to reflect the more
structured, certain behavioural pattern expressed by the demonstrations. Indeed,
Figure~\ref{fig:covariance_boxplots} shows exactly that. The first two box plots
show that in roughly $3/4$ of the runs reward variances were higher in the
random scenario\footnote{Note that outliers were removed from the plot. However,
  all outliers were positive.}. Interestingly, the same applies to covariances.

\begin{figure}[!t]
  \centering
  \begin{tikzpicture}
    \foreach \i in {0,...,9}
    \node[draw,circle] (v\i) at (360/10 * \i:3cm) {$s_{\i}$};
    \foreach \i in {0,...,9}
    \foreach \j in {0,...,\i}
    \draw[gray] (v\i) -- (v\j);

    \draw[->,thick,blue] (v9) -- (v2);
    \draw[->,thick,blue] (v7) -- (v8);
    \draw[->,thick,blue] (v9) -- (v4);
    \draw[->,thick,blue] (v3) -- (v0);
    \draw[->,thick,blue] (v2) -- (v4);
    \draw[->,thick,blue] (v4) -- (v6);
    \draw[->,thick,blue] (v6) -- (v5);

    \begin{scope}[yshift=-7cm]
    \foreach \i in {0,...,9}
    \node[draw,circle] (v\i) at (360/10 * \i:3cm) {$s_{\i}$};
    \foreach \i in {0,...,9}
    \foreach \j in {0,...,\i}
    \draw[gray] (v\i) -- (v\j);

    \draw[->,thick,blue] (v3) -- (v0);
    \draw[->,thick,blue] (v4) -- (v0);
    \draw[->,thick,blue] (v6) -- (v0);
    \draw[->,thick,blue] (v2) -- (v0);
    \draw[->,thick,blue] (v5) -- (v0);
    \draw[->,thick,blue] (v8) -- (v0);
    \draw[->,thick,blue] (v9) -- (v0);
    \end{scope}
  \end{tikzpicture}
  \caption{The clique MDP with two sets of demonstrations denoted by blue arrows}
  \label{fig:cliques}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/boxplots}
  \caption{Box plots of the difference between (absolute values of) reward
    covariances in random and semi-structured scenarios from
    Figure~\ref{fig:cliques}, averaged out over 100 runs and grouped into four
    key categories. `First state' refers to the variance of $r(s_0)$, the reward
    of the state targeted by all demonstrations in the semi-structured case.
    `Other states' are variances of other states of the MDP, i.e., diagonal
    entries of the covariance matrix. `Incident edges' refers to covariances
    between $r(s_0)$ and all other states, i.e., the first row or column of the
    covariance matrix. Finally, `other edges' is the category for the remaining
    covariances between pairs of rewards.}
  \label{fig:covariance_boxplots}
\end{figure}

\section{Conclusions and Further Work} \label{sec:conclusions}
% Show how we achieved our claimed contributions

In this paper, we showed how VI can be used to solve the IRL problem. We
proved the validity of our approach, establishing subsidiary results such as new
bounds for the linearly solvable MDP model along the way. The implementation of
the model was well-behaved and showed that covariance can indeed be used to
recognise a well-learned model from a model which is less sure about what
policies are optimal.

% FURTHER WORK
Building on that, an interesting question for MDP (or, perhaps, dynamical
systems) research would be: how much does a reward have to change in order to
affect the deterministic policy? A simple answer to this question would allow us
to use variance estimates in order to quantify the model's confidence regarding
optimal behaviour.

It is also worth noting that the approach presented in this paper requires solving
$S$ MDPs for every iteration of optimisation (where $S$ is the
number of samples drawn from $\approximation$). Thus, any improvements in MDP
solving speed are highly desirable. Two simple improvements can come from
requiring less precision when running value iteration and from reusing the
solutions of previous MDPs to initialise the new (unsolved) MDP.

Finally, an interesting extension to our work would be to consider IRL in the
context of a reinforcement learning (RL) agent. Suppose we have an agent whose
purpose is to learn optimal behaviour from observing other agents using IRL. It
could then take reward variance estimates into account when choosing what states
to visit next. It would have to handle the balance between exploration and
exploitation similarly to many RL agents, but the information about rewards
would come from observing (presumably near-optimal) behaviour exhibited by other
agents rather than directly from the environment.

%\vskip8pt \noindent
%{\bf Acknowledgments.}
%This is optional; it is a location for you to thank people
\bibliographystyle{abbrv}
\bibliography{paper}

\appendix
\section{Proofs} \label{appendix:proofs}

\derivatives*
\begin{proof}
  \leavevmode
  \begin{enumerate}
  \item
    \begin{align*}
      \frac{\partial q(\mathbf{u})}{\partial m} &= q(\mathbf{u})\dm\left[-\frac{(\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)}{2}\right] \\
                                                &= -\frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu)\dm[\mathbf{u} - \bm\mu] \\
                                                &= \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu).
    \end{align*}
  \item An online tool by Laue et
    al.\footnote{\url{http://www.matrixcalculus.org/}}
    \cite{DBLP:conf/nips/LaueMG18} can be used to find both derivatives.
  \item
    \begin{enumerate}
    \item Since
    \begin{align*}
      q(\mathbf{r} \mid \mathbf{u}) &= \mathcal{N}(\mathbf{r};
      \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr - \Kru^\intercal \Kuu^{-1}\Kru) \\
      &= \mathcal{N}(\mathbf{r}; \mathbf{Su}, \bm\Gamma),
    \end{align*}
    we have
    \begin{align*}
      \frac{\partial q(\mathbf{r} \mid \mathbf{u})}{\partial \lambda_i} =
      -\frac{1}{2}q(\mathbf{r} \mid \mathbf{u}) \dl[&(\mathbf{r} -
      \mathbf{Su})^\intercal\bm\Gamma^{-1}(\mathbf{r} - \mathbf{Su}) \\
      &+ \log|\bm\Gamma|].
    \end{align*}
    The same online tool can be used to show that
    \[
      \dl \log|\bm\Gamma| = -|\bm\Gamma|^{-1}\tr(\mathbf{R}\adj(\bm\Gamma)),
    \]
    and
    \[
      \dl \bm\Gamma^{-1} = \bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}.
    \]
  \item
    If $i=0$, then
    \[
      \frac{\partial \mathbf{K}}{\partial \lambda_i} =
      \frac{1}{\lambda_i}\mathbf{K}
    \]
    by the structure of each element of $\mathbf{K}$. If $i \ne 0$, then each
    element of $\frac{\partial \mathbf{K}}{\partial \lambda_i}$ is
    \begin{align*}
      L_{j,k} &= \frac{\partial k(\mathbf{x}_j, \mathbf{x}_k)}{\partial \lambda_i} \\
      &\begin{aligned}
        = k(\mathbf{x}_j, \mathbf{x}_k) \dl \left[ \vphantom{\frac{1}{2}} \right. &-\frac{1}{2}(\mathbf{x}_j - \mathbf{x}_k)^\intercal \bm\Lambda (\mathbf{x}_j - \mathbf{x}_k) \\
        &- \mathbbm{1}[j \ne k]\sigma^2\tr(\bm\Lambda) \left. \vphantom{\frac{1}{2}} \right]
      \end{aligned} \\
      &\begin{aligned}
        = k(\mathbf{x}_j, \mathbf{x}_k) \dl \left[ \vphantom{\sum_{k=1}^d} \right. &-\frac{1}{2} \sum_{l=1}^d \lambda_l (x_{j,l} - x_{k,l})^2 \\
        &- \mathbbm{1}[j \ne k]\sigma^2\sum_{l=1}^d \lambda_l \left. \vphantom{\sum_{l=1}^d} \right]
      \end{aligned} \\
      &= k(\mathbf{x}_j, \mathbf{x}_k) \left( -\frac{1}{2}(x_{j,i} - x_{k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
    \end{align*}
    \end{enumerate}
  \end{enumerate}
\end{proof}

\section{Derivatives of the ELBO} \label{appendix:derivatives}

\subsection{\texorpdfstring{$\partial/\partial\bm\mu$}{Derivative w.r.t. mu}}

We begin by removing terms independent of $\bm\mu$:
\[
  \frac{\partial\mathcal{L}}{\partial\bm\mu} =
  \dm[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu] - \frac{1}{2} \dm
  \left[ \bm\mu^\intercal \Kuu^{-1} \bm\mu \right] - \dm\mathbb{E}[v].
\]
Here
\[
  \dm \left[ \bm\mu^\intercal \Kuu^{-1} \bm\mu \right] = (\Kuu^{-1} +
  \Kuu^{-\intercal}) \bm\mu
\]
by Petersen and Pedersen \cite{petersen2008matrix}, and
\[
  \begin{split}
    \dm\mathbb{E}[\V(s)] &= \dm\iint \V(s) q(\mathbf{r} \mid \mathbf{u})
    q(\mathbf{u})\dx \\
    &= \iint \V(s) q(\mathbf{r} \mid \mathbf{u}) \frac{\partial
      q(\mathbf{u})}{\partial \bm\mu}\dx \\
    &= \frac{1}{2}\mathbb{E}[\V(s) (\bm\Sigma^{-1} +
    \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu)]
  \end{split}
\]
by Theorem~\ref{thm:main} and Lemma~\ref{lemma:derivatives}. Hence,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\bm\mu} &=
    \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1} - \frac{1}{2} (\Kuu^{-1} +
    \Kuu^{-\intercal}) \bm\mu \\
    &- \frac{1}{2}\mathbb{E} \left[(\bm\Sigma^{-1} +
      \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu) v \right].
  \end{split}
\]

\subsection{\texorpdfstring{$\partial/\partial\mathbf{B}$}{Derivative w.r.t. B}}

\[
  \frac{\partial\mathcal{L}}{\partial\mathbf{B}} =
  \frac{1}{2} \left( \dB\log|\bm\Sigma| - \dB\tr \left( \Kuu^{-1} \bm\Sigma
    \right) \right)
  - \dB\mathbb{E}[v].
\]
By Theorem~\ref{thm:main},
\[
  \dB\mathbb{E}[\V(s)] = \iint \V(s) q(\mathbf{r} \mid \mathbf{u})
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{B}}\dx.
\]
Then, using the aforementioned tool by Laue et al.
\cite{DBLP:conf/nips/LaueMG18}, we get
\[
  \dB\log|\bm\Sigma| = 2\bm\Sigma^{-1}\mathbf{B}, \quad \dB \tr \left( \Kuu^{-1}
    \bm\Sigma \right) = 2\Kuu^{-1}\mathbf{B},
\]
and Lemma~\ref{lemma:derivatives} gives
\begin{gather*}
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
  q(\mathbf{u})(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
  |\bm\Sigma|^{-1}\adj(\bm\Sigma))\mathbf{B}.
\end{gather*}
Therefore,
\begin{gather*}
  \frac{\partial \mathcal{L}}{\partial \mathbf{B}} =
  \left( \bm\Sigma^{-1} - \Kuu^{-1} \right) \mathbf{B} - \mathbb{E}
  [(\bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} -
  |\bm\Sigma|^{-1}\adj(\bm\Sigma))\mathbf{B}v].
\end{gather*}

\subsection{\texorpdfstring{$\partial/\partial \lambda_j$}{Derivative w.r.t.
    Lambda}}

For $j = 0, \dots, d$,
\[
  \begin{split}
    \frac{\partial \mathcal{L}}{\partial \lambda_j} &= \mathbf{t}^\intercal\dlj
    \left[ \Kru^\intercal\Kuu^{-1} \right] \bm\mu - \dlj\mathbb{E}[v] \\
    &- \frac{1}{2} \left(\dlj \tr \left(\Kuu^{-1}\bm\Sigma \right) +
      \bm\mu^\intercal \frac{\partial \Kuu^{-1}}{\partial \lambda_j} \bm\mu +
      \dlj \log |\Kuu| \right),
  \end{split}
\]
where
\begin{align*}
  \frac{\partial \Kuu^{-1}}{\partial \lambda_j} &= -\Kuu^{-1}\frac{\partial \Kuu}{\partial \lambda_j}\Kuu^{-1}, \\
  \dlj \left[ \Kru^\intercal\Kuu^{-1} \right] &= \frac{\partial \Kru^\intercal}{\partial \lambda_j} \Kuu^{-1} + \Kru^\intercal \frac{\partial \Kuu^{-1}}{\partial \lambda_j} \\
                                                &= \left( \frac{\partial \Kru^\intercal}{\partial \lambda_j} - \Kru^\intercal\Kuu^{-1}\frac{\partial \Kuu}{\partial \lambda_j} \right) \Kuu^{-1}, \\
  \dlj \tr(\Kuu^{-1}\bm\Sigma) &= \tr \left( \dlj \left[ \Kuu^{-1}\bm\Sigma \right] \right) = \tr \left( \frac{\partial \Kuu^{-1}}{\partial \lambda_j} \bm\Sigma \right) \\
                                                &= -\tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j} \Kuu^{-1} \bm\Sigma \right), \\
  \dlj\log|\Kuu| &= \tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j} \right)
\end{align*}
by Petersen and Pedersen \cite{petersen2008matrix}, and
\[
  \begin{split}
    \dlj \mathbb{E}[\V(s)] &= \iint\V(s)\frac{\partial q(\mathbf{r} \mid
      \mathbf{u})}{\partial \lambda_j}q(\mathbf{u})\dx \\
    &= \!\begin{multlined}[t]
      \frac{1}{2}\mathbb{E}[\V(s) (|\bm\Gamma|^{-1} \tr(\mathbf{R}
          \adj(\bm\Gamma)) \\
          - (\mathbf{r} -
          \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r}
          - \mathbf{Su}))]
    \end{multlined}
  \end{split}
\]
by Theorem~\ref{thm:main} and Lemma~\ref{lemma:derivatives}. Thus,
\begin{align*}
  \frac{\partial \mathcal{L}}{\partial \lambda_j} &=
  \mathbf{t}^\intercal \left( \frac{\partial \Kru^\intercal}{\partial
      \lambda_j} - \Kru^\intercal\Kuu^{-1}\frac{\partial
      \Kuu}{\partial \lambda_j} \right) \Kuu^{-1} \bm\mu \\
  &\begin{aligned}
    + \frac{1}{2} \left[ \vphantom{\frac{\partial \Kuu}{\partial \lambda_j}}
    \right. &\tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j}
      \Kuu^{-1} \bm\Sigma \right) + \bm\mu^\intercal \Kuu^{-1} \frac{\partial
      \Kuu}{\partial \lambda_j} \Kuu^{-1} \bm\mu \\
    &- \tr \left. \left(\Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j}
      \right) \right]
  \end{aligned} \\
  &\begin{aligned}
    - \frac{1}{2} \mathbb{E} [ (&|\bm\Gamma|^{-1} \tr(\mathbf{R} \adj(\bm\Gamma)) \\
    &- (\mathbf{r} - \mathbf{Su})^\intercal\bm\Gamma^{-1}\mathbf{R}\bm\Gamma^{-1}(\mathbf{r} - \mathbf{Su})) v],
  \end{aligned} 
\end{align*}
where the remaining derivatives can be found in Lemma~\ref{lemma:derivatives}.

\end{document}