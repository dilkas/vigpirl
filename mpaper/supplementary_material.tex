\documentclass{article}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newenvironment{proofsketch}{%
  \renewcommand{\proofname}{Proof sketch}\proof}{\endproof}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareMathOperator{\tr}{tr}

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Luu}{\mathbf{L}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}
\newcommand{\pr}{\mathcal{N}(\mathbf{r}; \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr
  - \Kru^\intercal\Kuu^{-1}\Kru)}

\newcommand{\dm}{\frac{\partial}{\partial\mathbf{m}}}
\newcommand{\dS}{\frac{\partial}{\partial\mathbf{S}}}
\newcommand{\da}{\frac{\partial}{\partial\alpha_j}}
\newcommand{\db}{\frac{\partial}{\partial\beta_j}}
\newcommand{\dt}{\frac{\partial}{\partial t}}
\newcommand{\dlz}{\frac{\partial}{\partial \lambda_0}}
\newcommand{\dl}{\frac{\partial}{\partial \lambda_i}}

\newcommand{\f}{f(\mathbf{r}, \mathbf{u}, t)}
\newcommand{\ftn}{f(\mathbf{r}, \mathbf{u}, t_n)}
\newcommand{\fn}{f_n(\mathbf{r}, \mathbf{u})}
\newcommand{\dx}{\,d\mathbf{r}\,d\mathbf{u}}
\newcommand{\df}{\left.\frac{\partial f}{\partial t}\right|_{(\mathbf{r},
    \mathbf{u}, t)}}
\newcommand{\g}{g(\mathbf{r}, \mathbf{u})}
\newcommand{\rinf}{\lVert \mathbf{r} \rVert_\infty}
\newcommand{\vbound}{\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma}}

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian
  Processes: Supplementary Material}
\author{Paulius Dilkas (2146879)}

\begin{document}
\maketitle

\section{Preliminaries}

For any matrix $\mathbf{A}$, we will use either $A_{i,j}$ or
$[\mathbf{A}]_{i,j}$ to denote the element of $\mathbf{A}$ in row $i$ and column
$j$.

For any vector $\mathbf{x}$, we write $\mathbb{R}_d[\mathbf{x}]$ to denote a
vector space of polynomials with degree at most $d$, where variables are
elements of $\mathbf{x}$, and coefficients are in $\mathbb{R}$.

In this paper, all references to measurability are with respect to the Lebesgue
measure. Similarly, whenever we consider the existence of an integral, we use
the Lebesgue definition of integration.

\begin{lemma}[Derivatives of probability
  distributions] \label{lemma:derivatives}
  \begin{enumerate}
    \leavevmode
  \item $\frac{\partial q(\mathbf{u})}{\partial \mathbf{m}} =
    q(\mathbf{u})\frac{1}{2}(\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})$.
  \item $\frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} =
    -\frac{1}{2}\mathbf{S}^{-\intercal}q(\mathbf{u}) +
    \frac{1}{2}q(\mathbf{u})\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal}$.
  \item For $i = 0, \dots, d$,
    \[
      \frac{\partial q(\mathbf{r})}{\partial \lambda_i} =
      q(\mathbf{r})\frac{1}{2}\tr
      \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} -
        \Kuu^{-1}) \frac{\partial \Kuu}{\partial \lambda_i} \right),
    \]
    where
    \[
      \frac{\partial \Kuu}{\partial \lambda_i} = \frac{1}{\lambda_i}\Kuu
    \]
    if $i = 0$, and
    \[
      \left[ \frac{\partial \Kuu}{\partial \lambda_i} \right]_{j,k} =
      k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
      \left( -\frac{1}{2}(x_{\mathbf{u},j,i} - x_{\mathbf{u},k,i})^2 -
        \mathbbm{1}[j \ne k]\sigma^2 \right)
    \]
    otherwise.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \leavevmode
  \begin{enumerate}
  \item
    \[
      \begin{split}
        \frac{\partial q(\mathbf{u})}{\partial m} &=
        q(\mathbf{u})\dm\left[-\frac{1}{2}(\mathbf{u} -
          \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})\right]
        \\
        &= q(\mathbf{u})\left(-\frac{1}{2}\right)(\mathbf{S}^{-1} +
        \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})\dm[\mathbf{u} -
        \mathbf{m}] \\
        &= q(\mathbf{u})\frac{1}{2}(\mathbf{S}^{-1} +
        \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m}).
      \end{split}
    \]
  \item
    \[
      \begin{split}
        \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} &=
        \dS\left[\frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\exp \left( -\frac{1}{2}
            (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
            \mathbf{m})\right)\right] \\
        &= \dS\left[\frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\right]\exp \left( -\frac{1}{2}
          (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
          \mathbf{m})\right) \\
        &+ \frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\dS\left[\exp\left( -\frac{1}{2}
            (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
            \mathbf{m})\right)\right] \\
        &=
        \frac{1}{(2\pi)^{m/2}}\dS\left[\frac{1}{|\mathbf{S}|^{1/2}}\right]\exp
        \left( -\frac{1}{2} (\mathbf{u} -
          \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})\right) \\
        &- \frac{1}{2}q(\mathbf{u})\dS[(\mathbf{u}
        - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})]. \\
      \end{split}
    \]
    The two remaining derivatives can be taken with the help of \emph{The Matrix
      Cookbook} \cite{petersen2008matrix}:
    \begin{gather*}
      \dS\left[\frac{1}{|\mathbf{S}|^{1/2}}\right] =
      -\frac{1}{2}|\mathbf{S}|^{-3/2}\frac{\partial |\mathbf{S}|}{\partial \mathbf{S}} =
      -\frac{1}{2}|\mathbf{S}|^{-3/2}|\mathbf{S}|\mathbf{S}^{-\intercal} = -\frac{1}{2|\mathbf{S}|^{1/2}}\mathbf{S}^{-\intercal}, \\
      \dS[(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
      \mathbf{m})] = -\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
                     \mathbf{m})^\intercal\mathbf{S}^{-\intercal}.
    \end{gather*}
    Substituting them back in gives
    \[
      \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} =
      -\frac{1}{2}\mathbf{S}^{-\intercal}q(\mathbf{u}) +
      \frac{1}{2}q(\mathbf{u})\mathbf{S}^{-\intercal}(\mathbf{u} -
      \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal}.
    \]
  \item Using a result by Rasmussen and Williams
    \cite{DBLP:books/lib/RasmussenW06},
    \[
      \frac{\partial q(\mathbf{r})}{\partial \lambda_i} = q(\mathbf{r}) \dl
      \left[-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} -
        \frac{1}{2}\log|\Kuu| \right] = q(\mathbf{r})\frac{1}{2}\tr
      \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} - \Kuu^{-1})
        \frac{\partial \Kuu}{\partial \lambda_i} \right).
    \]
    The remaining derivative is
    \[
      \frac{\partial \Kuu}{\partial \lambda_i} =
      \begin{cases}
        \frac{1}{\lambda_i}\Kuu & \text{if } i = 0, \\
        \Luu & \text{otherwise,}
      \end{cases}
    \]
    where
    \[
      \begin{split}
        [\Luu]_{j,k} &= \dl k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j},
        \mathbf{x}_{\mathbf{u},k}) \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \dl \left[-\frac{1}{2}(\mathbf{x}_{\mathbf{u},j} -
          \mathbf{x}_{\mathbf{u},k})^\intercal \bm\Lambda
          (\mathbf{x}_{\mathbf{u},j} - \mathbf{x}_{\mathbf{u},k}) -
          \mathbbm{1}[j \ne k]\sigma^2\tr(\bm\Lambda) \right] \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \dl \left[-\frac{1}{2}\sum_{l=1}^d \lambda_l
          (x_{\mathbf{u},j,l} - x_{\mathbf{u},k,l})^2 -
          \mathbbm{1}[j \ne k]\sigma^2\sum_{l=1}^d \lambda_l \right] \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \left( -\frac{1}{2}(x_{\mathbf{u},j,i} -
        x_{\mathbf{u},k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
      \end{split}
    \]
  \end{enumerate}
\end{proof}

\subsection{Linear Algebra and Numerical Analysis}

\begin{definition}[Norms]
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} is
  \[
    \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|
  \]
  whereas its \emph{taxicab} (or \emph{Manhattan}) \emph{norm} is
  \[
    \lVert \mathbf{x} \rVert_1 = \sum_{i = 1}^n |x_i|.
  \]
  Let $\mathbf{A}$ be a matrix. For any vector norm $\lVert
  \cdot \rVert_p$, we can also define its \emph{induced norm} for matrices as
  \[
    \lVert \mathbf{A} \rVert_p = \sup_{\mathbf{x} \ne \mathbf{0}} \frac{\lVert
      \mathbf{Ax} \rVert_p}{\lVert \mathbf{x} \rVert_p}.
  \]
  In particular, for $p = \infty$, we have
  \[
    \lVert \mathbf{A} \rVert_\infty = \max_i \sum_{j} |A_{i,j}|.
  \]
\end{definition}

\begin{lemma}[Perturbation Lemma
  \cite{layton2014numerical}] \label{prop:condition_number}
  Let $\lVert \cdot \rVert$ be any matrix norm, and let $\mathbf{A}$ and
  $\mathbf{E}$ be matrices such that $\mathbf{A}$ is invertible and $\lVert
  \mathbf{A}^{-1} \rVert \lVert \mathbf{E} \rVert < 1$, then $\mathbf{A} +
  \mathbf{E}$ is invertible, and
  \[
    \lVert (\mathbf{A} + \mathbf{E})^{-1} \rVert \le \frac{\lVert
      \mathbf{A}^{-1} \rVert}{1 - \lVert \mathbf{A}^{-1} \rVert \lVert
      \mathbf{E} \rVert}.
  \]
\end{lemma}

\section{Proofs}

We primarily think of rewards as a vector $\mathbf{r} \in
\mathbb{R}^{|\mathcal{S}|}$, but sometimes we use a function notation $r(s)$ to
denote the reward of a particular state $s \in \mathcal{S}$. The functional
notation is purely a notational convenience.

MDP values are characterised by both a state and a reward function/vector. In
order to prove the next theorem, we think of the value function as $V :
\mathcal{S} \to \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$, i.e., $V$ takes a
state $s \in \mathcal{S}$ and returns a function $V(s) :
\mathbb{R}^{\mathcal{S}} \to \mathbb{R}$ that takes a reward vector $\mathbf{r}
\in \mathbb{R}^{|\mathcal{S}|}$ and returns a value of the state $s$,
$V_{\mathbf{r}}(s) \in \mathbb{R}$. The function $V(s)$ computes the values of
all states and returns the value of state $s$.

\begin{proposition} \label{thm:measurability}
  MDP value functions $V(s) : \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$ (for $s
  \in \mathcal{S}$) are Lebesgue measurable.
\end{proposition}
\begin{proofsketch}
  For any reward vector $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$, the
  collection of converged value functions $\{ V_{\mathbf{r}}(s) \mid s \in
  \mathcal{S} \}$ satisfy
  \[
    V_{\mathbf{r}}(s) = \log \sum_{a \in \mathcal{A}}
    \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
      s')V_{\mathbf{r}}(s') \right)
  \]
  for all $s \in \mathcal{S}$. Let $s_0 \in \mathcal{S}$ be an arbitrary state.
  In order to prove that $V(s_0)$ is measurable, it is enough to show that for
  any $\alpha \in \mathbb{R}$, the set
  \[
    \begin{split}
      \left\{ \vphantom{\sum_{a \in \mathcal{A}}} \right. \mathbf{r} \in
      \mathbb{R}^{|\mathcal{S}|} \left. \vphantom{\sum_{a \in \mathcal{A}}} \;
        \middle| \; \right. &V_{\mathbf{r}}(s_0) \in (-\infty, \alpha); \\
      &V_{\mathbf{r}}(s) \in
      \mathbb{R} \text{ for all } s \in \mathcal{S} \setminus \{ s_0 \}; \\
      &V_{\mathbf{r}}(s) = \left. \log \sum_{a \in \mathcal{A}} \exp\left( r(s)
          + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
          s')V_{\mathbf{r}}(s') \right) \text{ for all } s \in
        \mathcal{S}\right\}
    \end{split}
  \]
  is measurable. Since this set can be constructed in Zermelo-Fraenkel set
  theory \emph{without} the axiom of choice, it is measurable
  \cite{herrlich2006axiom}, which proves that $V(s)$ is a measurable function
  for any $s \in \mathcal{S}$.
\end{proofsketch}

\begin{proposition} \label{thm:bound}
  If the initial values of the MDP value function satisfy the following
  bound, then the bound remains satisfied throughout value iteration:
  \begin{equation} \label{eq:bound}
    |V_{\mathbf{r}}(s)| \le \vbound.
  \end{equation}
\end{proposition}
\begin{proof}
  We begin by considering \eqref{eq:bound} without taking the absolute value of
  $V_{\mathbf{r}}(s)$, i.e.,
  \begin{equation} \label{eq:positive_bound}
    V_{\mathbf{r}}(s) \le \vbound,
  \end{equation}
  and assuming that the initial values of $\{ V_{\mathbf{r}}(s) \mid s \in
  \mathcal{S} \}$ already satisfy \eqref{eq:positive_bound}. For each $s \in
  \mathcal{S}$, the value of $V_{\mathbf{r}}(s)$ is updated via this rule:
  \[
    V_{\mathbf{r}}(s) \coloneqq \log \sum_{a \in \mathcal{A}} \exp\left( r(s) +
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s')V_{\mathbf{r}}(s')
    \right).
  \]
  Note that both $\log$ and $\exp$ are increasing functions, $\gamma > 0$, and
  the $\mathcal{T}$ function gives a probability (a non-negative number).
  Thus
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\le \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s'
          \in \mathcal{S}} \mathcal{T}(s, a, s')\frac{\rinf +
          \log|\mathcal{A}|}{1 - \gamma} \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma}\sum_{s' \in \mathcal{S}}
        \mathcal{T}(s, a, s') \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma} \right)
    \end{split}
  \]
  by the definition of $\mathcal{T}$. Then
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\le \log \left( |\mathcal{A}| \exp\left( r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log \left( \exp\left( \log|\mathcal{A}| + r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \\
      &= \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + r(s))}{1 - \gamma} \\
      &\le \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + \rinf)}{1 - \gamma} \\
      &= \vbound
    \end{split}
  \]
  by the definition of $\rinf$.

  The proof for
  \begin{equation} \label{eq:negative_bound}
    V_{\mathbf{r}}(s) \ge \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1}
  \end{equation}
  follows the same argument until we get to
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(\log|\mathcal{A}| + r(s))}{\gamma - 1} \\
      &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(-\log|\mathcal{A}| -\rinf)}{\gamma - 1} \\
      &= \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1},
    \end{split}
  \]
  where we use the fact that $r(s) \ge -\rinf - 2\log|\mathcal{A}|$. Combining
  \eqref{eq:positive_bound} and \eqref{eq:negative_bound} gives
  \eqref{eq:bound}.
\end{proof}

\begin{theorem}[The Lebesgue Dominated Convergence Theorem
  \cite{royden2010real}] \label{thm:lebesgue}
  Let $(X, \mathcal{M}, \mu)$ be a measure space and $\{ f_n \}$ a sequence of
  measurable functions on $X$ for which $\{ f_n \} \to f$ pointwise a.e. on $X$
  and the function $f$ is measurable. Assume there is a non-negative function
  $g$ that is integrable over $X$ and dominates the sequence $\{ f_n \}$ on $X$
  in the sense that
  \[
    |f_n| \le g \text{ a.e. on $X$ for all $n$.}
  \]
  Then $f$ is integrable over $X$ and
  \[
    \lim_{n \to \infty} \int_X f_n\,d\mu = \int_X f\,d\mu.
  \]
\end{theorem}

\begin{lemma} \label{lemma:bound1}
  Let $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (a, b) \subset
  \mathbb{R}$ be an arbitrary bounded function. Then, for $i = 0,
  \dots, d$,
  \[
    \left. \frac{\partial q(\mathbf{r})}{\partial \lambda_i} \right|_{\lambda_i
      = c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{r})d(\mathbf{u})$, where
  $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Remember that
  \[
    \frac{\partial q(\mathbf{r})}{\partial \lambda_i} =
    q(\mathbf{r})\frac{1}{2}\tr
    \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} - \Kuu^{-1})
      \frac{\partial \Kuu}{\partial \lambda_i}
    \right)
  \]
  by Lemma \ref{lemma:derivatives}. We begin by producing constant upper and
  lower bounds for the elements of
  \[
    \left. \frac{\partial \Kuu}{\partial \lambda_i} \right|_{\lambda_i =
      c(\mathbf{r}, \mathbf{u})}.
  \]
  If $i = 0$, then each element of $\frac{\partial
    \Kuu}{\partial \lambda_0}$ is of the form
  \[
    \exp \left( -\frac{1}{2}(\mathbf{x}_j - \mathbf{x}_k)^\intercal
      \bm\Lambda (\mathbf{x}_j - \mathbf{x}_k) - \mathbbm{1}[j \ne
      k]\sigma^2\tr(\bm\Lambda) \right),
  \]
  i.e., without $\lambda_0$, so
  \[
    \left. \frac{\partial \Kuu}{\partial \lambda_0} \right|_{\lambda_0 =
      c(\mathbf{r}, \mathbf{u})} = \frac{\partial \Kuu}{\partial \lambda_0}
  \]
  is already independent of $\mathbf{r}$ and $\mathbf{u}$---there is no need
  for any bounds.

  If $i > 0$, then each element of $\frac{\partial \Kuu}{\partial
    \lambda_i}$ is a constant multiple of $k_{\bm\lambda}(\mathbf{x}_j,
  \mathbf{x}_k)$, for some $\mathbf{x}_j$ and $\mathbf{x}_k$. Since
  $k_{\bm\lambda}(\mathbf{x}_j, \mathbf{x}_k)$ is a decreasing function of
  $\lambda_i$, and $c(\mathbf{r}, \mathbf{u}) > a$,
  \[
    \begin{split}
      k_{\bm\lambda}(\mathbf{x}_j, \mathbf{x}_k)|_{\lambda_i = 
        c(\mathbf{r}, \mathbf{u})} &=
      \begin{multlined}[t]
        \lambda_0 \exp \left( \vphantom{\sum_{n \in \{ \} \setminus}}
        \right. -\frac{1}{2}c(\mathbf{r}, \mathbf{u})(x_{j,i} - x_{k,i})^2 -
        \mathbbm{1}[j \ne k]\sigma^2c(\mathbf{r}, \mathbf{u}) \\
        - \left. \sum_{n \in \{ 1, \dots, d \} \setminus \{ i \}}
          \frac{1}{2} \lambda_n(x_{j,n} - x_{k,n})^2 + \mathbbm{1}[j \ne
          k]\sigma^2 \lambda_n \right)
      \end{multlined} \\
      &<
      \begin{multlined}[t]
        \lambda_0 \exp \left( \vphantom{\sum_{n \in \{ \} \setminus}}
        \right. -\frac{1}{2}a(x_{j,i} - x_{k,i})^2 -
        \mathbbm{1}[j \ne k]\sigma^2a \\
        - \left. \sum_{n \in \{ 1, \dots, d \} \setminus \{ i \}}
          \frac{1}{2} \lambda_n(x_{j,n} - x_{k,n})^2 + \mathbbm{1}[j \ne
          k]\sigma^2 \lambda_n \right),
      \end{multlined}
    \end{split}
  \]
  which gives an upper bound on each element of
  \[
    \left. \frac{\partial \Kuu}{\partial \lambda_i} \right|_{\lambda_i =
      c(\mathbf{r}, \mathbf{u})}.
  \]
  A similar line of reasoning establishes lower bounds as well.

  Combining the bounds with the observation that
  every element of
  $\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal}$ is in
  $\mathbb{R}_2[\mathbf{u}]$ gives the required result.
\end{proof}

\begin{remark}
  In order to find $\frac{\partial q(\mathbf{u})}{\partial t}$,
  where $t$ is the $i$th element of the vector $\mathbf{m}$, we can
  find $\frac{\partial q(\mathbf{u})}{\partial \mathbf{m}}$ and simply take the
  $i$th element. A similar line of reasoning applies to matrices as well. Thus,
  we only need to consider derivatives with respect to $\mathbf{m}$ and
  $\mathbf{S}$.
\end{remark}

\begin{lemma} \label{lemma:bound2}
  Let $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (a, b) \subset
  \mathbb{R}$ be an arbitrary bounded function. Then, for $i = 1, \dots, m$,
  every element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \mathbf{m}} \right|_{m_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$,
  where $d(\mathbf{u}) \in \mathbb{R}_1[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma \ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \mathbf{m}} \right|_{m_i =
      c(\mathbf{r}, \mathbf{u})} = q(\mathbf{u})\frac{1}{2}(\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{c}(\mathbf{r}, \mathbf{u})),
  \]
  where $\mathbf{c}(\mathbf{r}, \mathbf{u}) = (m_1, \dots, m_{i - 1},
  c(\mathbf{r}, \mathbf{u}), m_{i + 1} \dots, m_m)^\intercal$. Since
  $c(\mathbf{r}, \mathbf{u})$ is bounded and $\mathbf{S}^{-1} +
  \mathbf{S}^{-\intercal}$ is a constant matrix, we can use the bounds on
  $c(\mathbf{r}, \mathbf{u})$ to manufacture both upper and lower bounds on
  \[
     \left. \frac{\partial q(\mathbf{u})}{\partial \mathbf{m}} \right|_{m_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  of the required form.
\end{proof}

\begin{lemma} \label{lemma:bound3}
  Let $i, j = 1, \dots, m$, and let $\epsilon > 0$ be arbitrary. Furthermore,
  let
  \[
    c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (S_{i,j} - \epsilon,
    S_{i,j} + \epsilon) \subset \mathbb{R}
  \]
  be a function with a codomain arbitrarily close to $S_{i,j}$. Then every
  element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} \right|_{S_{i,j} =
    c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$, where
  $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma \ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} \right|_{S_{i,j} =
    c(\mathbf{r}, \mathbf{u})} =
    -\frac{1}{2}\mathbf{C}(\mathbf{r}, \mathbf{u})^{-\intercal} +
    \frac{1}{2}\mathbf{C}(\mathbf{r}, \mathbf{u})^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} -
    \mathbf{m})^\intercal\mathbf{C}(\mathbf{r}, \mathbf{u})^{-\intercal},
  \]
  where
  \[
    [\mathbf{C}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) & \text{if } (k, l) = (i, j), \\
      S_{k,l} & \text{otherwise.}
    \end{cases}
  \]
  We can also express $\mathbf{C}(\mathbf{r},\mathbf{u})$ as
  $\mathbf{C}(\mathbf{r}, \mathbf{u}) = \mathbf{S} + \mathbf{E}(\mathbf{r},
  \mathbf{u})$, where
  \[
    [\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) - S_{i,j} & \text{if } (k, l) = (i, j), \\
      0 & \text{otherwise.}
    \end{cases}
  \]
  We begin by establishing upper and lower bounds on $\mathbf{C}(\mathbf{r},
  \mathbf{u})^{-1}$. For this we use the maximum norm $\lVert \cdot
  \rVert_\infty$ on both vectors and matrices. We can apply Lemma
  \ref{prop:condition_number} to $\mathbf{S}$ and $\mathbf{E}(\mathbf{r},
  \mathbf{u})$ since
  \[
    \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty = \max_k \sum_l
    |[\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l}| = |c(\mathbf{r}, \mathbf{u}) -
    S_{i,j}| < \epsilon
  \]
  can be made arbitrarily small so that $\lVert \mathbf{S}^{-1} \rVert_\infty
  \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty < 1$. Then
  $\mathbf{C}(\mathbf{r}, \mathbf{u})$ is invertible, and
  \[
    \lVert \mathbf{C}(\mathbf{r}, \mathbf{u})^{-1} \rVert_\infty \le
    \frac{\lVert \mathbf{S}^{-1} \rVert_\infty}{1 - \lVert \mathbf{S}^{-1}
      \rVert_\infty \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty} <
    \frac{\lVert \mathbf{S}^{-1} \rVert_\infty}{1 - \lVert \mathbf{S}^{-1}
      \rVert_\infty \epsilon},
  \]
  which means that
  \[
    \max_k \sum_l \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right|
    < \frac{\lVert \mathbf{S}^{-1} \rVert_\infty}{1 - \lVert \mathbf{S}^{-1}
      \rVert_\infty \epsilon},
  \]
  i.e., for any row $k$ and column $l$,
  \[
    \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right| <
    \frac{\lVert \mathbf{S}^{-1} \rVert_\infty}{1 - \lVert \mathbf{S}^{-1}
      \rVert_\infty \epsilon},
  \]
  which bounds all elements of $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ as
  required. Since every element of $(\mathbf{u} - \mathbf{m})(\mathbf{u} -
  \mathbf{m})^\intercal$ is in $\mathbb{R}_2[\mathbf{u}]$, and the elements of
  $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ are bounded, the desired result
  follows.
\end{proof}

\begin{lemma} \label{lemma:integral_of_r}
  \begin{equation} \label{eq:r-inequality}
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r})\,d\mathbf{r} \le a +
    \lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1,
  \end{equation}
  where $a$ is a constant independent of $\mathbf{u}$.
\end{lemma}
\begin{proof}
  Since $\rinf \le \lVert \mathbf{r} \rVert_1$,
  \[
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r})\,d\mathbf{r} \le \int
    \lVert \mathbf{r} \rVert_1 q(\mathbf{r})\,d\mathbf{r} =
    \sum_{i=1}^{|\mathcal{S}|} \mathbb{E}[|r_i|].
  \]
  As each $\mathbb{E}[|r_i|]$ is a mean of a folded Gaussian distribution,
  \[
    \mathbb{E}[|r_i|] = \sigma_i \sqrt{\frac{2}{\pi}} \exp
    \left(-\frac{\mu_i^2}{2\sigma_i^2} \right) + \mu_i \left( 1 - 2\Phi \left(
        -\frac{\mu_1}{\sigma_1} \right) \right),
  \]
  where $\mu_i = \left[\Kru^\intercal\Kuu^{-1}\mathbf{u}\right]_i$, $\sigma_i =
  \sqrt{[\Krr - \Kru^\intercal\Kuu^{-1}\Kru]_{i,i}}$\footnote{The expression
    under the square root sign is non-negative because $\Krr -
    \Kru^\intercal\Kuu^{-1}\Kru$ is a covariance matrix of a Gaussian
    distribution, hence also positive semi-definite, which means that its
    diagonal entries are non-negative.}, and $\Phi$ is the cumulative
  distribution function of the standard normal distribution. Furthermore,
  \[
    \mathbb{E}[|r_i|] \le \sigma_i\sqrt{\frac{2}{\pi}} + |\mu_i|,
  \]
  as $\sigma_i$ is non-negative, and $\Phi(x) \in [0, 1]$ for all $x$. Since
  \[ \sum_{i=1}^{|\mathcal{S}|} |\mu_i| = \lVert \Kru^\intercal \Kuu^{-1}
    \mathbf{u} \rVert_1, \]
  we can set
  \[ a = \sum_{i=1}^{|\mathcal{S}|} \sigma_i \sqrt{\frac{2}{\pi}} \]
  to get \eqref{eq:r-inequality}.
\end{proof}

Our main theorem is a specialised version of an integral differentiation result
by Chen \cite{lecture_notes}.
\begin{theorem}
  Whenever the derivative exists,
  \[
    \dt\iint
    V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})\dx
    = \iint
    \dt[V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})]\dx,
  \]
  where $t$ is any scalar part of $\mathbf{m}$, $\mathbf{S}$, or $\bm\lambda$.
\end{theorem}
\begin{proof}
  Let
  \begin{align*}
    \f &= V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u}), \\
    F(t) &= \iint \f\dx,
  \end{align*}
  and fix the value of $t$. Let $(t_n)_{n=1}^\infty$ be any sequence such that
  $\lim_{n \to \infty} t_n = t$, but $t_n \ne t$ for all $n$. We want to show
  that
  \begin{equation} \label{eq:to_prove}
    F'(t) = \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \iint \df\dx.
  \end{equation}
  We have
  \[
    \frac{F(t_n) - F(t)}{t_n - t} = \iint \frac{\ftn - \f}{t_n - t}\dx =
    \iint \fn\dx,
  \]
  where
  \[
    \fn = \frac{\ftn - \f}{t_n - t}.
  \]
  Since
  \[
    \lim_{n \to \infty} \fn = \df,
  \]
  \eqref{eq:to_prove} follows from Theorem \ref{thm:lebesgue} as soon as we show
  that both $f$ and $f_n$ are measurable and find a non-negative integrable
  function $g$ such that for all $n$, $\mathbf{r}$, $\mathbf{u}$,
  \[
    |\fn| \le \g.
  \]
  The MDP value function is measurable by Proposition \ref{thm:measurability}.
  The result of multiplying or adding measurable functions (e.g., probability
  density functions (PDFs)) to a measurable function is still measurable. Thus,
  both $f$ and $f_n$ are measurable.

  It remains to find $g$. For notational simplicity and without loss of
  generality, we will temporarily assume that $t$ is a parameter of
  $q(\mathbf{r})$. Then
  \[
    |\fn| = |V_{\mathbf{r}}(s)| \left| \frac{q(\mathbf{r})|_{t =
          t_n} - q(\mathbf{r})}{t_n - t} \right| q(\mathbf{u})
  \]
  since PDFs are non-negative. An upper bound for
  $|V_{\mathbf{r}}(s)|$ is given by Proposition \ref{thm:bound}, while
  \[
    \frac{q(\mathbf{r})|_{t = t_n} - q(\mathbf{r})}{t_n - t} = \left.
      \frac{\partial q(\mathbf{r})}{\partial t} \right|_{t = c(\mathbf{r},
      \mathbf{u})}
  \]
  for some function $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to
  (\min\{t, t_n\}, \max\{t, t_n\})$ due to the mean value theorem (since $q$ is
  a continuous and differentiable function of $t$, regardless of the specific
  choices of $q$ and $t$).

  We then have that
  \[
    |\fn| \le \vbound \left| \left. \frac{\partial
          q(\mathbf{r})}{\partial t} \right|_{t=c(\mathbf{r}, \mathbf{u})}
    \right| q(\mathbf{u}).
  \]
  The bound is clearly non-negative and measurable. It remains to show that it
  is also integrable. Depending on what $t$ represents, we can use one of the
  Lemmas \ref{lemma:bound1}, \ref{lemma:bound2}, and \ref{lemma:bound3}, which
  gives us two polynomials $p_1(\mathbf{u}), p_2(\mathbf{u}) \in
  \mathbb{R}_2[\mathbf{u}]$ such that
  \[
    p_1(\mathbf{u})q(\mathbf{r}) < \left. \frac{\partial q(\mathbf{r})}{\partial
        t} \right|_{t=c(\mathbf{r}, \mathbf{u})} < p_2(\mathbf{u})q(\mathbf{r}).
  \]
  Then
  \[
    \left| \left. \frac{\partial q(\mathbf{r})}{\partial t}
      \right|_{t=c(\mathbf{r}, \mathbf{u})} \right| < q(\mathbf{r}) \max \{
    |p_1(\mathbf{u})|, |p_2(\mathbf{u})| \}.
  \]
  We can now apply Lemma \ref{lemma:integral_of_r}, which allows us to integrate
  out $\mathbf{r}$, and we are left with showing the existence of
  \begin{equation} \label{eq:remaining_integral}
    \int \left( a + \lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1 \right) \max \{|p_1(\mathbf{u})|, |p_2(\mathbf{u})| \} q(\mathbf{u})\,d\mathbf{u},
  \end{equation}
  where $a$ is a constant. The integral
  \[
    \int \max \{|p_1(\mathbf{u})|, |p_2(\mathbf{u})| \}
    q(\mathbf{u})\,d\mathbf{u} = \int \max \{|p_1(\mathbf{u})q(\mathbf{u})|,
    |p_2(\mathbf{u})q(\mathbf{u})| \}\,d\mathbf{u}
  \]
  exists because $p_1(\mathbf{u})q(\mathbf{u})$ and
  $p_2(\mathbf{u})q(\mathbf{u})$ are both integrable, hence their absolute
  values are integrable, and the maximum of two integrable functions is also
  integrable. Since $\lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1 \in
  \mathbb{R}_1[\mathbf{u}]$, a similar argument can be applied to the rest of
  \eqref{eq:remaining_integral} as well.
\end{proof}

\section{Evidence Lower Bound}

% TODO: how does the derivation change? What's the new ELBO?

\subsection{$\partial/\partial\mathbf{m}$}

We begin by removing terms independent of $\mathbf{m}$:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{m}} =
    &- \frac{1}{2}\dm[\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}]
    + \dm[\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \dm\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
      s')\dm\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
Here
\begin{align*}
  \dm[\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}] &= (\mathbb{E}[\Kuu^{-1}] + \mathbb{E}[\Kuu^{-1}]^\intercal)\mathbf{m}, \\
  \dm[\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}] &= \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}],
\end{align*}
and
\begin{equation} \label{eq:1}
  \begin{split}
    \dm\mathbb{E}[V_{\mathbf{r}}(s)] &= \dm\iiint V_{\mathbf{r}}(s) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda \\
    &= \iiint V_{\mathbf{r}}(s) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \dm\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda,
  \end{split}
\end{equation}
where
% TODO: fill this gap with d/dm N(u; m, S)
Substituting it back into \eqref{eq:1} gives
\[
  \begin{split}
    \dm\mathbb{E}[V_{\mathbf{r}}(s)] &= \frac{1}{2}\iiint V_{\mathbf{r}}(s)
    (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})
    p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) \mathcal{N}(\mathbf{u};
    \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda \\
    &= \frac{1}{2}\mathbb{E}[V_{\mathbf{r}}(s) (\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})].
\end{split}
\]
Hence
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{m}} =
    &-\frac{1}{2}(\mathbb{E}[\Kuu^{-1}] +
    \mathbb{E}[\Kuu^{-1}]^\intercal)\mathbf{m} +
    \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}] \\
    &- \frac{1}{2}\sum_{i=1}^N\sum_{t=1}^T \mathbb{E}[V_{\mathbf{r}}(s_{i,t})
    (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})] \\
    &- \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')
    \mathbb{E}[V_{\mathbf{r}}(s') (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} -
    \mathbf{m})].
\end{split}
\]

\subsection{$\partial/\partial\mathbf{S}$}

Similarly to the previous section,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{S}} &=
    \frac{1}{2}\dS\log|\mathbf{S}|
    -\frac{1}{2}\dS\tr[\mathbb{E}[\Kuu^{-1}]\mathbf{S}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \dS\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
      s')\dS\mathbb{E}[V_{\mathbf{r}}(s')],
  \end{split}
\]
where
\[ \dS\log|\mathbf{S}| = \mathbf{S}^{-\intercal}, \]
and
\[ \dS\tr[\mathbb{E}[\Kuu^{-1}]\mathbf{S}] = \mathbb{E}[\Kuu^{-1}]^\intercal \]
by \emph{The Matrix Cookbook} \cite{petersen2008matrix}. Then
\[ \dS\mathbb{E}[V_{\mathbf{r}}(s)] = \iiint V_{\mathbf{r}}(s) q(\mathbf{r}) \dS\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
  q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda, \]
where
% TODO: insert d/dS N(u; m, S)
and
\[
  \begin{split}
    \dS\mathbb{E}[V_{\mathbf{r}}(s)] &= \frac{1}{2}\iiint V_{\mathbf{r}}(s)
    (\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
    \mathbf{m})^\intercal\mathbf{S}^{-\intercal} - \mathbf{S}^{-\intercal})
    q(\mathbf{r}) \mathcal{N}(\mathbf{u};
    \mathbf{m}, \mathbf{S}) q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda
    \\
    &= \frac{1}{2}\mathbb{E}[V_{\mathbf{r}}(s)(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})].
  \end{split}
\]
Therefore
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{S}} &=
    \frac{1}{2}\mathbf{S}^{-\intercal} -
    \frac{1}{2}\mathbb{E}[\Kuu^{-1}]^\intercal - \frac{1}{2}
    \sum_{i=1}^N\sum_{t=1}^T
    \mathbb{E}[V_{\mathbf{r}}(s_{i,t})(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})] \\
    &- \gamma\sum_{s' \in \mathcal{S}}\mathcal{T}(s_{i,t}, a_{i,t}, s')
    \mathbb{E}[V_{\mathbf{r}}(s')(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})].
  \end{split}
\]

\subsection{$\partial/\partial\alpha_j$}

We begin in the usual way:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\alpha_j} = &-
    \frac{1}{2}\da\mathbb{E}[\tr[\Kuu^{-2}]] - \frac{1}{2}\da\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]] - \frac{1}{2}\da\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] - \frac{1}{2}\da\mathbb{E}[\log|\Kuu|] \\
    &+ \da\mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] +
    \da [\alpha_j + \log\Gamma(\alpha_j) + (1 - \alpha_j)\psi(\alpha_j)] \\
      &- \sum_{i=1}^N \sum_{t=1}^T \da\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
        \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
        s')\da\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
First,
\[ \da [\alpha_j + \log\Gamma(\alpha_j) + (1 - \alpha_j)\psi(\alpha_j)] = 1 +
  \psi(\alpha_j) - \psi(\alpha_j) + (1 - \alpha_j)\psi'(\alpha_j) = 1 + (1 -
  \alpha_j)\psi'(\alpha_j) \]
by the definition of $\psi$. The remaining terms can all be treated in the same
way, as they all contain expectations of scalar functions that are independent
of $\alpha_j$, and $\alpha_j$ only occurs in $\Gamma(\lambda_j; \alpha_j,
\beta_j)$. Thus we can work with an abstract function as follows:
\[
  \begin{split}
    \da\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \da \iiint f(k_{\bm\lambda},
    \mathbf{r}) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1}) \da
      \left[ \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)} \lambda_j^{\alpha_j -
          1} \right] e^{-\beta_j\lambda_j} \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}.
    \end{multlined}
  \end{split}
\]
Then
\[
  \begin{split}
    \da \left[ \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)} \lambda_j^{\alpha_j - 1} \right] &=
    \frac{\da[\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}]\Gamma(\alpha_j) -
      \beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\da[\alpha_j\log\beta_j + (\alpha_j -
      1)\log\lambda_j]\Gamma(\alpha_j) - \beta_j^{\alpha_j}\lambda_j^{\alpha_j -
        1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}(\log\beta_j + \log\lambda_i)\Gamma(\alpha_j)
      - \beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}}{\Gamma(\alpha_j)}\left(\log\beta_j +
    \log\lambda_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)}\right),
  \end{split}
\]
which means that
\[
  \begin{split}
    \da\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)}\lambda_j^{\alpha_j -
        1}e^{-\beta_j\lambda_j} \left(\log\beta_j + \log\lambda_j -
        \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right) \\
      q(\lambda_{j+1}) \cdots q(\lambda_d)
      q(\mathbf{r})q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}
    \end{multlined} \\
    &= \iiint f(k_{\bm\lambda}, \mathbf{r}) \left(\log\beta_j + \log\lambda_j -
      \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right) q(\bm\lambda)
    q(\mathbf{r})q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \mathbb{E} \left[ f(k_{\bm\lambda}, \mathbf{r}) \left(\log\beta_j +
        \log\lambda_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right)
    \right] \\
    &= \left( \log\beta_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)}
    \right)\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] +
    \mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})\log\lambda_j].
  \end{split}
\]
With these results in mind, we can simplify the initial expression to
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\alpha_j} &= 1 + (1 -
    \alpha_j)\psi'(\alpha_j) + \left( \log\beta_j -
      \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right)
    \left(\vphantom{\sum_{i=1}^N} \right. - \frac{1}{2}
    \mathbb{E}[\tr[\Kuu^{-2}]] - \frac{1}{2}\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]]
    \\
    &- \frac{1}{2}\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] -
    \frac{1}{2}\mathbb{E}[\log|\Kuu|] +
    \mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \mathbb{E}[V_{\mathbf{r}}(s_{i,t})] - \gamma\sum_{s' \in
      \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}[V_{\mathbf{r}}(s')] \left.
      \vphantom{\sum_{i=1}^N} \right) \\
    & - \frac{1}{2}\mathbb{E}[\tr[\Kuu^{-2}]\log\lambda_j] -
    \frac{1}{2}\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]\log\lambda_j] -
    \frac{1}{2}\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}\log\lambda_j]
    \\
    &- \frac{1}{2}\mathbb{E}[\log|\Kuu|\log\lambda_j] +
    \mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}\log\lambda_j]
    \\
    &- \sum_{i=1}^N\sum_{t=1}^T\mathbb{E}[V_{\mathbf{r}}(s_{i,t})\log\lambda_j] -
    \gamma\sum_{s' \in \mathcal{S}}\mathcal{T}(s_{i,t}, a_{i,t},
    s')\mathbb{E}[V_{\mathbf{r}}(s')\log\lambda_j].
  \end{split}
\]

\subsection{$\partial/\partial\beta_j$}
Finally,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\beta_j} = &-
    \frac{1}{2}\db\mathbb{E}[\tr[\Kuu^{-2}]] -
    \frac{1}{2}\db\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]] -
    \frac{1}{2}\db\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] \\
    &- \frac{1}{2}\db\mathbb{E}[\log|\Kuu|] +
    \db\mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] -
    \db[\log\beta_j] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \db \mathbb{E}[V_{\mathbf{r}}(s_{i,t})] - \gamma\sum_{s'
      \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\db\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
Similarly to the previous section, we can handle all derivatives of expectations
in the same way:
\[
  \begin{split}
    \db\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \db \iiint f(k_{\bm\lambda},
    \mathbf{r}) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\lambda_j^{\alpha_j - 1}}{\Gamma(\alpha_j)} \db [\beta_j^{\alpha_j}
      e^{-\beta_j\lambda_j}] \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}.
    \end{multlined}
  \end{split}
\]
Since
\[ \db [\beta_j^{\alpha_j} e^{-\beta_j\lambda_j}] = \alpha_j\beta_j^{\alpha_j - 1}e^{-\beta_j\lambda_j}
  - \beta_j^{\alpha_j} e^{-\beta_j\lambda_j}\lambda_j = \beta_j^{\alpha_j}
  e^{-\beta_j\lambda_j} \left( \frac{\alpha_j}{\beta_j} - \lambda_j \right), \]
we have that
\[
  \begin{split}
    \db\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)}\lambda_j^{\alpha_j -
        1}e^{-\beta_j\lambda_j} \left(\frac{\alpha_j}{\beta_j} - \lambda_j
      \right) \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}
    \end{multlined}
    \\
    &= \iiint f(k_{\bm\lambda}, \mathbf{r}) \left(\frac{\alpha_j}{\beta_j} -
      \lambda_j \right) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \mathbb{E} \left[ f(k_{\bm\lambda}, \mathbf{r})
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] =
    \frac{\alpha_j}{\beta_j}\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] -
    \mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})\lambda_j].
  \end{split}
\]
This gives us the final expression of $\frac{\partial\mathcal{L}}{\partial\beta_j}$:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\beta_j} = &- \frac{1}{\beta_j} -
    \frac{1}{2}\mathbb{E} \left[ \tr[\Kuu^{-2}] \left( \frac{\alpha_j}{\beta_j}
        - \lambda_j \right) \right] - \frac{1}{2}\mathbb{E} \left[
      \tr[\Kuu^{-1}\mathbf{S}] \left(\frac{\alpha_j}{\beta_j} - \lambda_j
      \right) \right] \\
    &- \frac{1}{2}\mathbb{E} \left[ \mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] -
    \frac{1}{2}\mathbb{E} \left[ \log|\Kuu| \left(\frac{\alpha_j}{\beta_j} -
        \lambda_j \right) \right] \\
    &+ \mathbb{E} \left[ \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \mathbb{E} \left[ V_{\mathbf{r}}(s_{i,t})
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] -
    \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}
    \left[V_{\mathbf{r}}(s') \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right].
  \end{split}
\]

\bibliographystyle{abbrv}
\bibliography{paper.bib}

\end{document}