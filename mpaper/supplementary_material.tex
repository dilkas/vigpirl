\documentclass{article}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newenvironment{proofsketch}{%
  \renewcommand{\proofname}{Proof sketch}\proof}{\endproof}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\DeclareMathOperator{\tr}{tr}

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Luu}{\mathbf{L}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}
\newcommand{\pr}{\mathcal{N}(\mathbf{r}; \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr
  - \Kru^\intercal\Kuu^{-1}\Kru)}

\newcommand{\dm}{\frac{\partial}{\partial\mathbf{m}}}
\newcommand{\dS}{\frac{\partial}{\partial\mathbf{S}}}
\newcommand{\da}{\frac{\partial}{\partial\alpha_j}}
\newcommand{\db}{\frac{\partial}{\partial\beta_j}}
\newcommand{\dt}{\frac{\partial}{\partial t}}
\newcommand{\dlz}{\frac{\partial}{\partial \lambda_0}}
\newcommand{\dl}{\frac{\partial}{\partial \lambda_i}}

\newcommand{\f}{f(\mathbf{r}, \mathbf{u}, t)}
\newcommand{\ftn}{f(\mathbf{r}, \mathbf{u}, t_n)}
\newcommand{\fn}{f_n(\mathbf{r}, \mathbf{u})}
\newcommand{\dx}{\,d\mathbf{r}\,d\mathbf{u}}
\newcommand{\df}{\left.\frac{\partial f}{\partial t}\right|_{(\mathbf{r},
    \mathbf{u}, t)}}
\newcommand{\g}{g(\mathbf{r}, \mathbf{u})}
\newcommand{\rinf}{\lVert \mathbf{r} \rVert_\infty}
\newcommand{\vbound}{\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma}}

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian
  Processes: Supplementary Material}
\author{Paulius Dilkas (2146879)}

\begin{document}
\maketitle

\section{Preliminaries}

For any matrix $\mathbf{A}$, we will use either $A_{i,j}$ or
$[\mathbf{A}]_{i,j}$ to denote the element of $\mathbf{A}$ in row $i$ and column
$j$.

In this paper, all references to measurability are with respect to the Lebesgue
measure. Similarly, whenever we consider the existence of an integral, we use
the Lebesgue definition of integration.

\begin{lemma}[Derivatives of probability
  distributions] \label{lemma:derivatives}
  \begin{enumerate}
    \leavevmode
  \item $\frac{\partial q(\mathbf{u})}{\partial \mathbf{m}} =
    q(\mathbf{u})\frac{1}{2}(\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})$.
  \item $\frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} =
    -\frac{1}{2}\mathbf{S}^{-\intercal}q(\mathbf{u}) +
    \frac{1}{2}q(\mathbf{u})\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal}$.
  \item $\frac{\partial q(\mathbf{r})}{\partial \lambda_0} =
    q(\mathbf{r})\frac{1}{2}\tr
    \left((\Kuu^{-1}\mathbf{u}(\Kuu^{-1}\mathbf{u})^\intercal - \Kuu^{-1})
      \frac{1}{\lambda_0}\Kuu \right)$.
  \item For $i = 1, \dots, d$,
    \[
      \frac{\partial q(\mathbf{r})}{\partial \lambda_i} =
      q(\mathbf{r})\frac{1}{2}\tr
      \left((\Kuu^{-1}\mathbf{u}(\Kuu^{-1}\mathbf{u})^\intercal - \Kuu^{-1})
        \Luu \right),
    \]
    where
    \[
      [\Luu]_{j,k} = k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j},
      \mathbf{x}_{\mathbf{u},k}) \left( -\frac{1}{2}(x_{\mathbf{u},j,i} -
        x_{\mathbf{u},k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
    \]
  \end{enumerate}
\end{lemma}
\begin{proof}
  \leavevmode
  \begin{enumerate}
  \item
    \[
      \begin{split}
        \frac{\partial q(\mathbf{u})}{\partial m} &=
        q(\mathbf{u})\dm\left[-\frac{1}{2}(\mathbf{u} -
          \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})\right]
        \\
        &= q(\mathbf{u})\left(-\frac{1}{2}\right)(\mathbf{S}^{-1} +
        \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})\dm[\mathbf{u} -
        \mathbf{m}] \\
        &= q(\mathbf{u})\frac{1}{2}(\mathbf{S}^{-1} +
        \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m}).
      \end{split}
    \]
  \item
    \[
      \begin{split}
        \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} &=
        \dS\left[\frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\exp \left( -\frac{1}{2}
            (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
            \mathbf{m})\right)\right] \\
        &= \dS\left[\frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\right]\exp \left( -\frac{1}{2}
          (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
          \mathbf{m})\right) \\
        &+ \frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\dS\left[\exp\left( -\frac{1}{2}
            (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
            \mathbf{m})\right)\right] \\
        &=
        \frac{1}{(2\pi)^{m/2}}\dS\left[\frac{1}{|\mathbf{S}|^{1/2}}\right]\exp
        \left( -\frac{1}{2} (\mathbf{u} -
          \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})\right) \\
        &- \frac{1}{2}q(\mathbf{u})\dS[(\mathbf{u}
        - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})]. \\
      \end{split}
    \]
    The two remaining derivatives can be taken with the help of \emph{The Matrix
      Cookbook} \cite{petersen2008matrix}:
    \begin{gather*}
      \dS\left[\frac{1}{|\mathbf{S}|^{1/2}}\right] =
      -\frac{1}{2}|\mathbf{S}|^{-3/2}\frac{\partial |\mathbf{S}|}{\partial \mathbf{S}} =
      -\frac{1}{2}|\mathbf{S}|^{-3/2}|\mathbf{S}|\mathbf{S}^{-\intercal} = -\frac{1}{2|\mathbf{S}|^{1/2}}\mathbf{S}^{-\intercal}, \\
      \dS[(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
      \mathbf{m})] = -\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
                     \mathbf{m})^\intercal\mathbf{S}^{-\intercal}.
    \end{gather*}
    Plugging them back in gives
    \[
      \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} =
      -\frac{1}{2}\mathbf{S}^{-\intercal}q(\mathbf{u}) +
      \frac{1}{2}q(\mathbf{u})\mathbf{S}^{-\intercal}(\mathbf{u} -
      \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal}.
    \]
  \item
    \[
      \frac{\partial q(\mathbf{r})}{\partial \lambda_0} = q(\mathbf{r}) \dlz
      \left[-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} -
        \frac{1}{2}\log|\Kuu| \right] = q(\mathbf{r})\frac{1}{2}\tr
      \left((\Kuu^{-1}\mathbf{u}(\Kuu^{-1}\mathbf{u})^\intercal - \Kuu^{-1})
        \frac{\partial \Kuu}{\partial \lambda_0} \right)
    \]
    by Rasmussen and Williams \cite{DBLP:books/lib/RasmussenW06}, where
    \[
      \frac{\partial \Kuu}{\partial \lambda_0} = \frac{1}{\lambda_0}\Kuu.
    \]
  \item The derivation is the same as above, except
    \[
      \frac{\partial \Kuu}{\partial \lambda_i} = \Luu,
    \]
    where
    \[
      \begin{split}
        [\Luu]_{j,k} &= \dl k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j},
        \mathbf{x}_{\mathbf{u},k}) \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \dl \left[-\frac{1}{2}(\mathbf{x}_{\mathbf{u},j} -
          \mathbf{x}_{\mathbf{u},k})^\intercal \bm\Lambda
          (\mathbf{x}_{\mathbf{u},j} - \mathbf{x}_{\mathbf{u},k}) -
          \mathbbm{1}[j \ne k]\sigma^2\tr(\bm\Lambda) \right] \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \dl \left[-\frac{1}{2}\sum_{l=1}^d \lambda_l
          (x_{\mathbf{u},j,l} - x_{\mathbf{u},k,l})^2 -
          \mathbbm{1}[j \ne k]\sigma^2\sum_{l=1}^d \lambda_l \right] \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \left( -\frac{1}{2}(x_{\mathbf{u},j,i} -
        x_{\mathbf{u},k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
      \end{split}
    \]
  \end{enumerate}
\end{proof}

\subsection{Linear Algebra and Numerical Analysis}

% TODO: maybe source?
\begin{definition}[Norms]
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} is
  \[
    \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|
  \]
  whereas its \emph{taxicab} (or \emph{Manhattan}) \emph{norm} is
  \[
    \lVert \mathbf{x} \rVert_1 = \sum_{i = 1}^n |x_i|.
  \]
  Let $\mathbf{A}$ be an $m \times n$ matrix. For any vector norm $\lVert
  \cdot \rVert_p$, we can also define its \emph{induced norm} for matrices as
  \[
    \lVert \mathbf{A} \rVert_p = \sup_{\mathbf{x} \ne \mathbf{0}} \frac{\lVert
      \mathbf{Ax} \rVert_p}{\lVert \mathbf{x} \rVert_p}.
  \]
  In particular, for $p = \infty$, we have
  \[
    \lVert \mathbf{A} \rVert_\infty = \max_i \sum_{j} |A_{i,j}|.
  \]
\end{definition}

% TODO: source/fix this
\begin{definition}[Condition number]
  For any norm $\lVert \cdot \rVert$, the \emph{condition number} of a matrix
  $\mathbf{A}$ is
  \[
    \kappa(\mathbf{A}) = \lVert \mathbf{A} \rVert \lVert \mathbf{A}^{-1} \rVert
  \]
  if $\mathbf{A}$ is invertible, and $\kappa(\mathbf{A}) = \infty$ otherwise.
\end{definition}

% TODO: find source
% TODO: mention that this applies to any vector/matrix norm
% TODO: do I need to prove this for the infinity norm?
\begin{proposition} \label{prop:condition_number}
  \[
    \frac{\lVert (\mathbf{A} + \mathbf{E})^{-1} - \mathbf{A}^{-1} \rVert}{\lVert
    \mathbf{A}^{-1} \rVert} \le \kappa(\mathbf{A})\frac{\lVert \mathbf{E}
    \rVert}{\lVert \mathbf{A} \rVert}
  \]
\end{proposition}

\section{Proofs}

We primarily think of rewards as a vector $\mathbf{r} \in
\mathbb{R}^{|\mathcal{S}|}$, but sometimes we use a function notation $r(s)$ to
denote the reward of a particular state $s \in \mathcal{S}$. The functional
notation is purely a notational convenience.

MDP values are characterised by both a state and a reward function/vector. In
order to prove the next theorem, we think of the value function as $V :
\mathcal{S} \to \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$, i.e., $V$ takes a
state $s \in \mathcal{S}$ and returns a function $V(s) :
\mathbb{R}^{\mathcal{S}} \to \mathbb{R}$ that takes a reward vector $\mathbf{r}
\in \mathbb{R}^{|\mathcal{S}|}$ and returns a value of the state $s$,
$V_{\mathbf{r}}(s) \in \mathbb{R}$. The function $V(s)$ computes the values of
all states and returns the value of state $s$.

\begin{proposition} \label{thm:measurability}
  MDP value functions $V(s) : \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$ (for $s
  \in \mathcal{S}$) are Lebesgue measurable.
\end{proposition}
\begin{proofsketch}
  For any reward vector $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$, the
  collection of converged value functions $\{ V_{\mathbf{r}}(s) \mid s \in
  \mathcal{S} \}$ satisfy
  \[
    V_{\mathbf{r}}(s) = \log \sum_{a \in \mathcal{A}}
    \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
      s')V_{\mathbf{r}}(s') \right)
  \]
  for all $s \in \mathcal{S}$. Let $s_0 \in \mathcal{S}$ be an arbitrary state.
  In order to prove that $V(s_0)$ is measurable, it is enough to show that for
  any $\alpha \in \mathbb{R}$, the set
  \[
    \begin{split}
      \left\{ \vphantom{\sum_{a \in \mathcal{A}}} \right. \mathbf{r} \in
      \mathbb{R}^{|\mathcal{S}|} \left. \vphantom{\sum_{a \in \mathcal{A}}} \;
        \middle| \; \right. &V_{\mathbf{r}}(s_0) \in (-\infty, \alpha); \\
      &V_{\mathbf{r}}(s) \in
      \mathbb{R} \text{ for all } s \in \mathcal{S} \setminus \{ s_0 \}; \\
      &V_{\mathbf{r}}(s) = \left. \log \sum_{a \in \mathcal{A}} \exp\left( r(s)
          + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
          s')V_{\mathbf{r}}(s') \right) \text{ for all } s \in
        \mathcal{S}\right\}
    \end{split}
  \]
  is measurable. Since this set can be constructed in Zermelo-Fraenkel set
  theory \emph{without} the axiom of choice, it is measurable
  \cite{herrlich2006axiom}, which proves that $V(s)$ is a measurable function
  for any $s \in \mathcal{S}$.
\end{proofsketch}

\begin{proposition} \label{thm:bound}
  If the initial values of the MDP value function satisfy the following
  bound, then the bound remains satisfied throughout value iteration:
  \begin{equation} \label{eq:bound}
    |V_{\mathbf{r}}(s)| \le \vbound.
  \end{equation}
\end{proposition}
\begin{proof}
  We begin by considering \eqref{eq:bound} without taking the absolute value of
  $V_{\mathbf{r}}(s)$, i.e.,
  \begin{equation} \label{eq:positive_bound}
    V_{\mathbf{r}}(s) \le \vbound,
  \end{equation}
  and assuming that the initial values of $\{ V_{\mathbf{r}}(s) \mid s \in
  \mathcal{S} \}$ already satisfy \eqref{eq:positive_bound}. For each $s \in
  \mathcal{S}$, the value of $V_{\mathbf{r}}(s)$ is updated via this rule:
  \[
    V_{\mathbf{r}}(s) \coloneqq \log \sum_{a \in \mathcal{A}} \exp\left( r(s) +
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s')V_{\mathbf{r}}(s')
    \right).
  \]
  Note that both $\log$ and $\exp$ are increasing functions, $\gamma > 0$, and
  the $\mathcal{T}$ function gives a probability (a non-negative number).
  Thus
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\le \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s'
          \in \mathcal{S}} \mathcal{T}(s, a, s')\frac{\rinf +
          \log|\mathcal{A}|}{1 - \gamma} \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma}\sum_{s' \in \mathcal{S}}
        \mathcal{T}(s, a, s') \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma} \right)
    \end{split}
  \]
  by the definition of $\mathcal{T}$. Then
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\le \log \left( |\mathcal{A}| \exp\left( r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log \left( \exp\left( \log|\mathcal{A}| + r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \\
      &= \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + r(s))}{1 - \gamma} \\
      &\le \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + \rinf)}{1 - \gamma} \\
      &= \vbound
    \end{split}
  \]
  by the definition of $\rinf$.

  The proof for
  \begin{equation} \label{eq:negative_bound}
    V_{\mathbf{r}}(s) \ge \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1}
  \end{equation}
  follows the same argument until we get to
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(\log|\mathcal{A}| + r(s))}{\gamma - 1} \\
      &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(-\log|\mathcal{A}| -\rinf)}{\gamma - 1} \\
      &= \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1},
    \end{split}
  \]
  where we use the fact that $r(s) \ge -\rinf - 2\log|\mathcal{A}|$. Combining
  \eqref{eq:positive_bound} and \eqref{eq:negative_bound} gives
  \eqref{eq:bound}.
\end{proof}

\begin{theorem}[The Lebesgue Dominated Convergence Theorem
  \cite{royden2010real}] \label{thm:lebesgue}
  Let $(X, \mathcal{M}, \mu)$ be a measure space and $\{ f_n \}$ a sequence of
  measurable functions on $X$ for which $\{ f_n \} \to f$ pointwise a.e. on $X$
  and the function $f$ is measurable. Assume there is a non-negative function
  $g$ that is integrable over $X$ and dominates the sequence $\{ f_n \}$ on $X$
  in the sense that
  \[
    |f_n| \le g \text{ a.e. on $X$ for all $n$.}
  \]
  Then $f$ is integrable over $X$ and
  \[
    \lim_{n \to \infty} \int_X f_n\,d\mu = \int_X f\,d\mu.
  \]
\end{theorem}

% TODO: still need lambda in the list, since p(r) uses the covariance matrices
% TODO: do I need to show that the original integral exists?
Our main theorem is a specialised version of an integral differentiation result
by Chen \cite{lecture_notes}.
\begin{theorem}
  Using our usual notation,
  \[
    \dt\iint
    V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})\dx
    = \iint
    \dt[V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})]\dx,
  \]
  where $t$ is any scalar part of $\mathbf{m}$, $\mathbf{S}$, or $\bm\lambda$.
\end{theorem}
\begin{proof}
  Let
  \begin{align*}
    \f &= V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u}), \\
    F(t) &= \iint \f\dx,
  \end{align*}
  and fix the value of $t$. Let $(t_n)_{n=1}^\infty$ be any sequence such that
  $\lim_{n \to \infty} t_n = t$, but $t_n \ne t$ for all $n$. We want to show
  that
  \begin{equation} \label{eq:to_prove}
    F'(t) = \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \iint \df\dx.
  \end{equation}
  We have
  \[
    \frac{F(t_n) - F(t)}{t_n - t} = \iint \frac{\ftn - \f}{t_n - t}\dx =
    \iint \fn\dx,
  \]
  where
  \[
    \fn = \frac{\ftn - \f}{t_n - t}.
  \]
  Since
  \[
    \lim_{n \to \infty} \fn = \df,
  \]
  \eqref{eq:to_prove} follows from Theorem \ref{thm:lebesgue} as soon as we show
  that both $f$ and $f_n$ are measurable and find a non-negative integrable
  function $g$ such that for all $n$, $\mathbf{r}$, $\mathbf{u}$,
  \[
    |\fn| \le \g.
  \]
  The MDP value function is measurable by Proposition \ref{thm:measurability}.
  The result of multiplying or adding measurable functions (e.g., probability
  density functions (PDFs)) to a measurable function is still measurable. Thus,
  both $f$ and $f_n$ are measurable.

  % TODO: C is measurable since it is bounded
  It remains to find $g$. For the time being, we can assume that $t$ is a
  parameter of $q(\mathbf{u})$. Then
  \[
    |\fn| = |V_{\mathbf{r}}(s)|q(\mathbf{r}) \left| \frac{q(\mathbf{u})|_{t =
          t_n} - q(\mathbf{u})}{t_n - t} \right|
  \]
  since PDFs are non-negative. An upper bound for
  $|V_{\mathbf{r}}(s)|$ is given by Proposition \ref{thm:bound}, while
  \[
    \frac{q(\mathbf{u})|_{t = t_n} - q(\mathbf{u})}{t_n - t} = \left.
      \frac{\partial q(\mathbf{u})}{\partial t} \right|_{t = c(\mathbf{u})}
  \]
  for some function $c : \mathbb{R}^m \to (\min\{t, t_n\}, \max\{t, t_n\})$
  due to the mean value theorem (since $q$ is a continuous and differentiable
  function of $t$, regardless of the specific choices of $q$ and $t$).

  Let $\epsilon > 0$ be arbitrary. Then, for sufficiently large $n$, $|t_n - t|
  < \epsilon$, and thus
  \begin{equation} \label{eq:epsilon_bound}
    |c(\mathbf{u}) - t| < \epsilon.
  \end{equation}
  We can rearrange the inequality to produce bounds on $c(\mathbf{u})$ and
  $|c(\mathbf{u})|$ that will be useful later:
  \begin{align}
    t - \epsilon < c(\mathbf{u}) < t + \epsilon, \nonumber \\
    |c(\mathbf{u})| < \max \{ |t - \epsilon|, |t + \epsilon| \}. \label{eq:c_bound}
  \end{align}
  
  We then have that
  \[
    |\fn| \le \vbound q(\mathbf{r}) \left| \left. \frac{\partial
          q(\mathbf{u})}{\partial t} \right|_{t=c(\mathbf{u})} \right|.
  \]
  The bound is clearly non-negative and measurable. It remains to show that it
  is also integrable. Since $\rinf \le \lVert \mathbf{r} \rVert_1$, and
  \[
    \int \frac{\lVert \mathbf{r} \rVert_1 + \log|\mathcal{A}|}{1 -
      \gamma}q(\mathbf{r}) \,d\mathbf{r} = \frac{\log|\mathcal{A}|}{1 - \gamma}
    + \frac{1}{1 - \gamma} \sum_{i=1}^k \mathbb{E}[|r_i|],
  \]
  which clearly exists and is finite, so
  \[
    \int \vbound q(\mathbf{r}) \,d\mathbf{r} < \infty.
  \]
  Now we just need to show that
  \begin{equation} \label{eq:last_goal}
    \iint \vbound q(\mathbf{r}) \left| \left. \frac{\partial
          q(\mathbf{u})}{\partial t} \right|_{t=c(\mathbf{u})} \right|\dx
  \end{equation}
  exists. If $t$ is the $i$th element of the vector $\mathbf{m}$, we can get
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial t} \right|_{t = c(\mathbf{u})}
  \]
  by first finding the derivative $\frac{\partial q(\mathbf{u})}{\partial
    \mathbf{m}}$, then replacing $t$ with $c(\mathbf{u})$, and finally taking
  the $i$th element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \mathbf{m}} \right|_{t =
      c(\mathbf{u})}.
  \]
  A similar line of reasoning applies to matrices as well. Thus, we only need to
  consider derivatives with respect to $\mathbf{m}$, $\mathbf{S}$, and
  $\lambda_i$ (for $i = 0, \dots, d$). Regardless of what $t$ represents, we can
  use results in Lemma \ref{lemma:derivatives} to show that
  \[
    \iint \vbound q(\mathbf{r}) \left| \left. \frac{\partial
          q(\mathbf{u})}{\partial t} \right|_{t=c(\mathbf{u})} \right|\dx =
    \iint \vbound q(\mathbf{r}) q(\mathbf{u}) |h(\mathbf{u})| \dx,
  \]
  where $h(\mathbf{u})$ is one of:
  \begin{enumerate}
  \item The $i$th element of the vector
    \[
      \frac{1}{2}(\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} -
      \mathbf{c}(\mathbf{u})) \in \mathbb{R}^m,
    \]
    for $i = 1, \dots, m$, where $\mathbf{c}(\mathbf{u}) = (c_1, \dots, c_{i -
      1}, c(\mathbf{u}), c_{i + 1} \dots, c_m)$ (constant everywhere except the
    $i$th element).
  \item The $(i,j)$-th element of the matrix
    \[
      -\frac{1}{2}\mathbf{C}(\mathbf{u})^{-\intercal} +
      \frac{1}{2}\mathbf{C}(\mathbf{u})^{-\intercal}(\mathbf{u} -
      \mathbf{m})(\mathbf{u} -
      \mathbf{m})^\intercal\mathbf{C}(\mathbf{u})^{-\intercal} \in \mathbb{R}^{m
        \times m},
    \]
    for $i, j = 1, \dots, m$, where $[\mathbf{C}(\mathbf{u})]_{i,j} =
    c(\mathbf{u})$, and all other elements are constant.
  \item $\frac{1}{2}\tr
    \left((\Kuu^{-1}\mathbf{u}(\Kuu^{-1}\mathbf{u})^\intercal - \Kuu^{-1})
      \left. \frac{\partial \Kuu}{\partial \lambda_i} \right|_{\lambda_i = c(\mathbf{u})}
    \right)$, where $i = 0, \dots, d$.
  \end{enumerate} % TODO: prove continuity for MVT
  % TODO: looking for an upper bound only applies to positive functions.
  % Otherwise bound the absolute value.
  % TODO: mention that i,j are fixed and will appear later
  % TODO: the 'As' part should be higher up, I think (with the talk about
  % integrating out r from c(u, r))
  As
  \[
    \int \vbound q(\mathbf{r})\,d\mathbf{r}
  \]
  gives us a constant, it remains to show that
  \[
    \mathbb{E}_{\mathbf{u} \sim q(\mathbf{u})}[|h(\mathbf{u})|] = \int
    |h(\mathbf{u})| q(\mathbf{u}) \,d\mathbf{u}
  \]
  exists. Hereinafter we fix $i$ and $j$ to refer respectively to the row and
  column of $t$ in case of derivative by a matrix, and we fix $i$ to be the
  index of $t$ in $\mathbf{m}$ if the derivative is taken with respect to a
  vector. We will analyse each of the three cases separately.
  \begin{enumerate}
  \item Since
    \[
      \mathbb{E} \left[ \frac{1}{2}(\mathbf{S}^{-1} +
        \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{c}(\mathbf{u})) \right] =
      \frac{1}{2}(\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})
      \mathbb{E}[\mathbf{u} - \mathbf{c}(\mathbf{u})],
    \]
    we just need to bound $\mathbb{E}[\mathbf{u} - \mathbf{c}(\mathbf{u})]$. For
    any $k \in \{ 1, \dots, m \} \setminus \{ i \}$,
    \[
      \mathbb{E}[\mathbf{u} - \mathbf{c}(\mathbf{u})]_k = \mathbb{E}[u_k - c_k]
      = m_k - c_k,
    \]
    while
    \[
      \mathbb{E}[\mathbf{u} - \mathbf{c}(\mathbf{u})]_i = \mathbb{E}[u_i
      - c(\mathbf{u})] = m_i - \mathbb{E}[c(\mathbf{u})],
    \]
    where $\mathbb{E}[c(\mathbf{u})]$ exists because $|c(\mathbf{u})|$ has an
    integrable upper bound in \eqref{eq:c_bound}. % TODO: double check this
  \item First, we can express $\mathbf{C}(\mathbf{u})$ as
    $\mathbf{C}(\mathbf{u}) = \mathbf{S} + \mathbf{E}(\mathbf{u})$, where
    $\mathbf{S}$ is a constant invertible positive semi-definite matrix, and
    $\mathbf{E}(\mathbf{u})$ is a matrix such that
    $[\mathbf{E}(\mathbf{u})]_{i,j} = c(\mathbf{u}) - t$, while all other
    elements of $\mathbf{E}(\mathbf{u})$ are zero.

    Next, we can divide the problem into two parts; namely, proving the
    existence of
    \[
      \mathbb{E}[\mathbf{C}(\mathbf{u})^{-\intercal}] \quad \text{and} \quad
      \mathbb{E}[\mathbf{C}(\mathbf{u})^{-\intercal}(\mathbf{u} -
      \mathbf{m})(\mathbf{u} -
      \mathbf{m})^\intercal\mathbf{C}(\mathbf{u})^{-\intercal}].
    \]
    \begin{enumerate}
    \item \label{part_2a} Applying Proposition \ref{prop:condition_number} to
      $\mathbf{S}$ and $\mathbf{E}(\mathbf{u})$ gives
      \[
        \frac{\lVert \mathbf{C}(\mathbf{u})^{-1} - \mathbf{S}^{-1}
          \rVert}{\lVert \mathbf{S}^{-1} \rVert} \le
        \kappa(\mathbf{S})\frac{\lVert \mathbf{E}(\mathbf{u}) \rVert}{\lVert
          \mathbf{S} \rVert},
      \] % TODO: what about norms equal to zero or infinity?
      which can be reformulated to
      \[
        \lVert \mathbf{C}(\mathbf{u})^{-1} - \mathbf{S}^{-1} \rVert \le \lVert
        \mathbf{S}^{-1} \rVert^2 \lVert \mathbf{E}(\mathbf{u}) \rVert.
      \]
      Choosing to use the maximum norm we get % TODO: actually use the sign for
                                % the norm
      % TODO: larger absolute value signs?
      \[
        \max_k \sum_l |[\mathbf{C}(\mathbf{u})^{-1}]_{k,l} -
        [\mathbf{S}^{-1}]_{k,l}| \le \lVert \mathbf{S}^{-1} \rVert^2
        |c(\mathbf{u}) - t|.
      \]
      Using \eqref{eq:epsilon_bound} we get that for any row $k$,
      \[
        \sum_l \left| [\mathbf{C}(\mathbf{u})^{-1}]_{k,l} -
          [\mathbf{S}^{-1}]_{k,l} \right| <
        \lVert \mathbf{S}^{-1} \rVert^2\epsilon,
      \]
      and for any row $k$ and column $l$,
      \[
        \left| [\mathbf{C}(\mathbf{u})^{-1}]_{k,l} - [\mathbf{S}^{-1}]_{k,l}
        \right| < \lVert \mathbf{S}^{-1} \rVert^2\epsilon,
      \]
      which bounds all elements of $\mathbf{C}(\mathbf{u})^{-1}$ and proves that
      $\mathbb{E}[\mathbf{C}(\mathbf{u})^{-\intercal}]$ exists.
    \item Because of the result in \ref{part_2a}, we only need to prove the
      existence of
      \[
        \mathbb{E}[(\mathbf{u} - \mathbf{m})(\mathbf{u} -
        \mathbf{m})^\intercal].
      \]
      The desired result follows from the existence of $\mathbb{E}[\mathbf{u}]$
      and $\mathbb{E}[\mathbf{u}\mathbf{u}^\intercal]$.
    \end{enumerate}
  \item Again, we can split the proof into two parts: showing the existence of
    \[
      \mathbb{E} \left[ \left. \frac{\partial \Kuu}{\partial \lambda_k}
        \right|_{\lambda_k = c(\mathbf{u})} \right] \quad \text{and} \quad
      \mathbb{E} \left[ \Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal}
        \left. \frac{\partial \Kuu}{\partial \lambda_k} \right|_{\lambda_k =
          c(\mathbf{u})} \right].
    \]
    \begin{enumerate}
    \item \label{part_a} If $k = 0$, then each element of $\frac{\partial
        \Kuu}{\partial \lambda_0}$ is of the form
      \[
        \exp \left( -\frac{1}{2}(\mathbf{x}_l - \mathbf{x}_m)^\intercal
          \bm\Lambda (\mathbf{x}_l - \mathbf{x}_m) - \mathbbm{1}[l \ne
          m]\sigma^2\tr(\bm\Lambda) \right),
      \]
      i.e., without $\lambda_0$, so
      \[
        \left. \frac{\partial \Kuu}{\partial \lambda_0} \right|_{\lambda_0 =
          c(\mathbf{u})} = \frac{\partial \Kuu}{\partial \lambda_0}
      \]
      has no $\mathbf{u}$, which means that
      \[
        \mathbb{E} \left[ \left. \frac{\partial \Kuu}{\partial \lambda_0}
          \right|_{\lambda_0 = c(\mathbf{u})} \right] = \frac{\partial
          \Kuu}{\partial \lambda_0}.
      \]

      If $k > 0$, then each element of $\frac{\partial \Kuu}{\partial
        \lambda_k}$ is a constant multiple of $k_{\bm\lambda}(\mathbf{x}_l,
      \mathbf{x}_m)$ for some $\mathbf{x}_l$ and $\mathbf{x}_m$. Since
      $k_{\bm\lambda}(\mathbf{x}_l, \mathbf{x}_m)$ is a decreasing function of
      $\lambda_k$, and $c(\mathbf{u}) > \lambda_k - \epsilon$,
      \[
        \begin{split}
          k_{\bm\lambda}(\mathbf{x}_l, \mathbf{x}_m)|_{\lambda_k = 
            c(\mathbf{u})} &=
          \begin{multlined}[t]
            \lambda_0 \exp \left( \vphantom{\sum_{n \in \{ \} \setminus}}
            \right. -\frac{1}{2}c(\mathbf{u})(x_{l,k} - x_{m,k})^2 -
            \mathbbm{1}[l \ne m]\sigma^2c(\mathbf{u}) \\
            - \left. \sum_{n \in \{ 1, \dots, d \} \setminus \{ k \}}
              \frac{1}{2} \lambda_n(x_{l,n} - x_{m,n})^2 + \mathbbm{1}[l \ne
              m]\sigma^2 \lambda_n \right)
          \end{multlined} \\
          &<
          \begin{multlined}[t]
            \lambda_0 \exp \left( \vphantom{\sum_{n \in \{ \} \setminus}}
            \right. -\frac{1}{2}(\lambda_k - \epsilon)(x_{l,k} - x_{m,k})^2 -
            \mathbbm{1}[l \ne m]\sigma^2(\lambda_k - \epsilon) \\
            - \left. \sum_{n \in \{ 1, \dots, d \} \setminus \{ k \}}
              \frac{1}{2} \lambda_n(x_{l,n} - x_{m,n})^2 + \mathbbm{1}[l \ne
              m]\sigma^2 \lambda_n \right),
          \end{multlined}
        \end{split}
      \]
      which gives an upper bound on each element of
      \[
        \left. \frac{\partial \Kuu}{\partial \lambda_k} \right|_{\lambda_k =
          c(\mathbf{u})}.
      \]
      and shows the existence of
      \[
        \mathbb{E} \left[ \left. \frac{\partial \Kuu}{\partial \lambda_k}
          \right|_{\lambda_k = c(\mathbf{u})} \right].
      \]
    \item Since we already found an upper bound for
      \[
        \left. \frac{\partial \Kuu}{\partial \lambda_k} \right|_{\lambda_k =
          c(\mathbf{u})}
      \]
      in \ref{part_a}, $\mathbb{E}[\Kuu^{-1}] = \Kuu^{-1}$, and
      $\mathbb{E}[\mathbf{u}\mathbf{u}^\intercal]$ clearly exists,
      \[
        \mathbb{E} \left[
          \Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} \left.
            \frac{\partial \Kuu}{\partial \lambda_k} \right|_{\lambda_k =
            c(\mathbf{u})} \right]
      \]
      also exists.
    \end{enumerate}
  \end{enumerate}
\end{proof}
% TODO: replace c, bold c, bold capital C with a function of u, r. After
% integrating out r, leave it as a function of u.
% TODO: can c be a function of r? I think so. We can probably just integrate the
% r part by adding an upper bound, similarly to the situation with u.
% TODO: why can S be invertible? I probably need something different here
% TODO: the variable i has multiple meanings
% TODO: are the inequalities with epsilon actually useful?
% TODO: high-level introduction and conclusion of the proof

\section{Derivatives of the Evidence Lower Bound}

\subsection{$\partial/\partial\mathbf{m}$}

We begin by removing terms independent of $\mathbf{m}$:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{m}} =
    &- \frac{1}{2}\dm[\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}]
    + \dm[\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \dm\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
      s')\dm\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
Here
\begin{align*}
  \dm[\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}] &= (\mathbb{E}[\Kuu^{-1}] + \mathbb{E}[\Kuu^{-1}]^\intercal)\mathbf{m}, \\
  \dm[\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}] &= \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}],
\end{align*}
and
\begin{equation} \label{eq:1}
  \begin{split}
    \dm\mathbb{E}[V_{\mathbf{r}}(s)] &= \dm\iiint V_{\mathbf{r}}(s) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda \\
    &= \iiint V_{\mathbf{r}}(s) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \dm\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda,
  \end{split}
\end{equation}
where
% TODO: fill this gap with d/dm N(u; m, S)
Substituting it back into \eqref{eq:1} gives
\[
  \begin{split}
    \dm\mathbb{E}[V_{\mathbf{r}}(s)] &= \frac{1}{2}\iiint V_{\mathbf{r}}(s)
    (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})
    p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) \mathcal{N}(\mathbf{u};
    \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda \\
    &= \frac{1}{2}\mathbb{E}[V_{\mathbf{r}}(s) (\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})].
\end{split}
\]
Hence
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{m}} =
    &-\frac{1}{2}(\mathbb{E}[\Kuu^{-1}] +
    \mathbb{E}[\Kuu^{-1}]^\intercal)\mathbf{m} +
    \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}] \\
    &- \frac{1}{2}\sum_{i=1}^N\sum_{t=1}^T \mathbb{E}[V_{\mathbf{r}}(s_{i,t})
    (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})] \\
    &- \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')
    \mathbb{E}[V_{\mathbf{r}}(s') (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} -
    \mathbf{m})].
\end{split}
\]

\subsection{$\partial/\partial\mathbf{S}$}

Similarly to the previous section,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{S}} &=
    \frac{1}{2}\dS\log|\mathbf{S}|
    -\frac{1}{2}\dS\tr[\mathbb{E}[\Kuu^{-1}]\mathbf{S}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \dS\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
      s')\dS\mathbb{E}[V_{\mathbf{r}}(s')],
  \end{split}
\]
where
\[ \dS\log|\mathbf{S}| = \mathbf{S}^{-\intercal}, \]
and
\[ \dS\tr[\mathbb{E}[\Kuu^{-1}]\mathbf{S}] = \mathbb{E}[\Kuu^{-1}]^\intercal \]
by \emph{The Matrix Cookbook} \cite{petersen2008matrix}. Then
\[ \dS\mathbb{E}[V_{\mathbf{r}}(s)] = \iiint V_{\mathbf{r}}(s) q(\mathbf{r}) \dS\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
  q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda, \]
where
% TODO: insert d/dS N(u; m, S)
and
\[
  \begin{split}
    \dS\mathbb{E}[V_{\mathbf{r}}(s)] &= \frac{1}{2}\iiint V_{\mathbf{r}}(s)
    (\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
    \mathbf{m})^\intercal\mathbf{S}^{-\intercal} - \mathbf{S}^{-\intercal})
    q(\mathbf{r}) \mathcal{N}(\mathbf{u};
    \mathbf{m}, \mathbf{S}) q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda
    \\
    &= \frac{1}{2}\mathbb{E}[V_{\mathbf{r}}(s)(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})].
  \end{split}
\]
Therefore
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{S}} &=
    \frac{1}{2}\mathbf{S}^{-\intercal} -
    \frac{1}{2}\mathbb{E}[\Kuu^{-1}]^\intercal - \frac{1}{2}
    \sum_{i=1}^N\sum_{t=1}^T
    \mathbb{E}[V_{\mathbf{r}}(s_{i,t})(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})] \\
    &- \gamma\sum_{s' \in \mathcal{S}}\mathcal{T}(s_{i,t}, a_{i,t}, s')
    \mathbb{E}[V_{\mathbf{r}}(s')(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})].
  \end{split}
\]

\subsection{$\partial/\partial\alpha_j$}

We begin in the usual way:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\alpha_j} = &-
    \frac{1}{2}\da\mathbb{E}[\tr[\Kuu^{-2}]] - \frac{1}{2}\da\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]] - \frac{1}{2}\da\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] - \frac{1}{2}\da\mathbb{E}[\log|\Kuu|] \\
    &+ \da\mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] +
    \da [\alpha_j + \log\Gamma(\alpha_j) + (1 - \alpha_j)\psi(\alpha_j)] \\
      &- \sum_{i=1}^N \sum_{t=1}^T \da\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
        \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
        s')\da\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
First,
\[ \da [\alpha_j + \log\Gamma(\alpha_j) + (1 - \alpha_j)\psi(\alpha_j)] = 1 +
  \psi(\alpha_j) - \psi(\alpha_j) + (1 - \alpha_j)\psi'(\alpha_j) = 1 + (1 -
  \alpha_j)\psi'(\alpha_j) \]
by the definition of $\psi$. The remaining terms can all be treated in the same
way, as they all contain expectations of scalar functions that are independent
of $\alpha_j$, and $\alpha_j$ only occurs in $\Gamma(\lambda_j; \alpha_j,
\beta_j)$. Thus we can work with an abstract function as follows:
\[
  \begin{split}
    \da\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \da \iiint f(k_{\bm\lambda},
    \mathbf{r}) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1}) \da
      \left[ \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)} \lambda_j^{\alpha_j -
          1} \right] e^{-\beta_j\lambda_j} \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}.
    \end{multlined}
  \end{split}
\]
Then
\[
  \begin{split}
    \da \left[ \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)} \lambda_j^{\alpha_j - 1} \right] &=
    \frac{\da[\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}]\Gamma(\alpha_j) -
      \beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\da[\alpha_j\log\beta_j + (\alpha_j -
      1)\log\lambda_j]\Gamma(\alpha_j) - \beta_j^{\alpha_j}\lambda_j^{\alpha_j -
        1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}(\log\beta_j + \log\lambda_i)\Gamma(\alpha_j)
      - \beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}}{\Gamma(\alpha_j)}\left(\log\beta_j +
    \log\lambda_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)}\right),
  \end{split}
\]
which means that
\[
  \begin{split}
    \da\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)}\lambda_j^{\alpha_j -
        1}e^{-\beta_j\lambda_j} \left(\log\beta_j + \log\lambda_j -
        \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right) \\
      q(\lambda_{j+1}) \cdots q(\lambda_d)
      q(\mathbf{r})q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}
    \end{multlined} \\
    &= \iiint f(k_{\bm\lambda}, \mathbf{r}) \left(\log\beta_j + \log\lambda_j -
      \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right) q(\bm\lambda)
    q(\mathbf{r})q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \mathbb{E} \left[ f(k_{\bm\lambda}, \mathbf{r}) \left(\log\beta_j +
        \log\lambda_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right)
    \right] \\
    &= \left( \log\beta_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)}
    \right)\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] +
    \mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})\log\lambda_j].
  \end{split}
\]
With these results in mind, we can simplify the initial expression to
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\alpha_j} &= 1 + (1 -
    \alpha_j)\psi'(\alpha_j) + \left( \log\beta_j -
      \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right)
    \left(\vphantom{\sum_{i=1}^N} \right. - \frac{1}{2}
    \mathbb{E}[\tr[\Kuu^{-2}]] - \frac{1}{2}\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]]
    \\
    &- \frac{1}{2}\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] -
    \frac{1}{2}\mathbb{E}[\log|\Kuu|] +
    \mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \mathbb{E}[V_{\mathbf{r}}(s_{i,t})] - \gamma\sum_{s' \in
      \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}[V_{\mathbf{r}}(s')] \left.
      \vphantom{\sum_{i=1}^N} \right) \\
    & - \frac{1}{2}\mathbb{E}[\tr[\Kuu^{-2}]\log\lambda_j] -
    \frac{1}{2}\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]\log\lambda_j] -
    \frac{1}{2}\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}\log\lambda_j]
    \\
    &- \frac{1}{2}\mathbb{E}[\log|\Kuu|\log\lambda_j] +
    \mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}\log\lambda_j]
    \\
    &- \sum_{i=1}^N\sum_{t=1}^T\mathbb{E}[V_{\mathbf{r}}(s_{i,t})\log\lambda_j] -
    \gamma\sum_{s' \in \mathcal{S}}\mathcal{T}(s_{i,t}, a_{i,t},
    s')\mathbb{E}[V_{\mathbf{r}}(s')\log\lambda_j].
  \end{split}
\]

\subsection{$\partial/\partial\beta_j$}
Finally,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\beta_j} = &-
    \frac{1}{2}\db\mathbb{E}[\tr[\Kuu^{-2}]] -
    \frac{1}{2}\db\mathbb{E}[\tr[\Kuu^{-1}\mathbf{S}]] -
    \frac{1}{2}\db\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] \\
    &- \frac{1}{2}\db\mathbb{E}[\log|\Kuu|] +
    \db\mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] -
    \db[\log\beta_j] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \db \mathbb{E}[V_{\mathbf{r}}(s_{i,t})] - \gamma\sum_{s'
      \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\db\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
Similarly to the previous section, we can handle all derivatives of expectations
in the same way:
\[
  \begin{split}
    \db\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \db \iiint f(k_{\bm\lambda},
    \mathbf{r}) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\lambda_j^{\alpha_j - 1}}{\Gamma(\alpha_j)} \db [\beta_j^{\alpha_j}
      e^{-\beta_j\lambda_j}] \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}.
    \end{multlined}
  \end{split}
\]
Since
\[ \db [\beta_j^{\alpha_j} e^{-\beta_j\lambda_j}] = \alpha_j\beta_j^{\alpha_j - 1}e^{-\beta_j\lambda_j}
  - \beta_j^{\alpha_j} e^{-\beta_j\lambda_j}\lambda_j = \beta_j^{\alpha_j}
  e^{-\beta_j\lambda_j} \left( \frac{\alpha_j}{\beta_j} - \lambda_j \right), \]
we have that
\[
  \begin{split}
    \db\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)}\lambda_j^{\alpha_j -
        1}e^{-\beta_j\lambda_j} \left(\frac{\alpha_j}{\beta_j} - \lambda_j
      \right) \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}
    \end{multlined}
    \\
    &= \iiint f(k_{\bm\lambda}, \mathbf{r}) \left(\frac{\alpha_j}{\beta_j} -
      \lambda_j \right) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \mathbb{E} \left[ f(k_{\bm\lambda}, \mathbf{r})
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] =
    \frac{\alpha_j}{\beta_j}\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] -
    \mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})\lambda_j].
  \end{split}
\]
This gives us the final expression of $\frac{\partial\mathcal{L}}{\partial\beta_j}$:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\beta_j} = &- \frac{1}{\beta_j} -
    \frac{1}{2}\mathbb{E} \left[ \tr[\Kuu^{-2}] \left( \frac{\alpha_j}{\beta_j}
        - \lambda_j \right) \right] - \frac{1}{2}\mathbb{E} \left[
      \tr[\Kuu^{-1}\mathbf{S}] \left(\frac{\alpha_j}{\beta_j} - \lambda_j
      \right) \right] \\
    &- \frac{1}{2}\mathbb{E} \left[ \mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] -
    \frac{1}{2}\mathbb{E} \left[ \log|\Kuu| \left(\frac{\alpha_j}{\beta_j} -
        \lambda_j \right) \right] \\
    &+ \mathbb{E} \left[ \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \mathbb{E} \left[ V_{\mathbf{r}}(s_{i,t})
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] -
    \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}
    \left[V_{\mathbf{r}}(s') \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right].
  \end{split}
\]

\bibliographystyle{abbrv}
\bibliography{paper.bib}

\end{document}