\documentclass{article}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newenvironment{proofsketch}{%
  \renewcommand{\proofname}{Proof sketch}\proof}{\endproof}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\DeclareMathOperator{\Tr}{Tr}

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}

\newcommand{\dm}{\frac{\partial}{\partial\mathbf{m}}}
\newcommand{\dS}{\frac{\partial}{\partial\mathbf{S}}}
\newcommand{\da}{\frac{\partial}{\partial\alpha_j}}
\newcommand{\db}{\frac{\partial}{\partial\beta_j}}
\newcommand{\dt}{\frac{\partial}{\partial t}}

\newcommand{\f}{f(\mathbf{r}, \mathbf{u}, \bm\lambda, t)}
\newcommand{\ftn}{f(\mathbf{r}, \mathbf{u}, \bm\lambda, t_n)}
\newcommand{\fn}{f_n(\mathbf{r}, \mathbf{u}, \bm\lambda)}
\newcommand{\dx}{\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda}
\newcommand{\df}{\left.\frac{\partial f}{\partial t}\right|_{(\mathbf{r},
    \mathbf{u}, \bm\lambda, t)}}
\newcommand{\g}{g(\mathbf{r}, \mathbf{u}, \bm\lambda)}
\newcommand{\rinf}{\lVert \mathbf{r} \rVert_\infty}
\newcommand{\vbound}{\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma}}

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian
  Processes: Supplementary Material}
\author{Paulius Dilkas (2146879)}

\begin{document}
\maketitle

% TODO: by measurable I mean Lebesgue measurable, and integrable means Lebesgue
% integrable

\section{Proofs}

We primarily think of rewards as a vector $\mathbf{r} \in
\mathbb{R}^{|\mathcal{S}|}$, but sometimes we use a function notation $r(s)$ to
denote the reward of a particular state $s \in \mathcal{S}$. The functional
notation is purely a notational convenience.

MDP values are characterised by both a state and a reward function/vector. In
order to prove the next theorem, we think of the value function as $V :
\mathcal{S} \to \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$, i.e., $V$ takes a
state $s \in \mathcal{S}$ and returns a function $V(s) :
\mathbb{R}^{\mathcal{S}} \to \mathbb{R}$ that takes a reward vector $\mathbf{r}
\in \mathbb{R}^{|\mathcal{S}|}$ and returns a value of the state $s$,
$V_{\mathbf{r}}(s) \in \mathbb{R}$. The function $V(s)$ computes the values of
all states and returns the value of state $s$.

\begin{theorem} \label{thm:measurability}
  MDP value functions $V(s) : \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$ (for $s
  \in \mathcal{S}$) are Lebesgue measurable.
\end{theorem}
\begin{proofsketch}
  For any reward vector $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$, the
  collection of converged value functions $\{ V_{\mathbf{r}}(s) \mid s \in
  \mathcal{S} \}$ satisfy
  \[ \forall s \in \mathcal{S},\;V_{\mathbf{r}}(s) = \log \sum_{a \in \mathcal{A}}
    \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
      s')V_{\mathbf{r}}(s') \right). \]
  Let $s_i \in \mathcal{S}$ be an arbitrary state. In order to prove that $V(s_i)$
  is measurable, it is enough to show that for any $\alpha \in \mathbb{R}$, the
  set
  \[
    \begin{split}
      \left\{ \vphantom{\sum_{a \in \mathcal{A}}} \right. \mathbf{r} \in
      \mathbb{R}^{|\mathcal{S}|} &\mid V_{\mathbf{r}}(s_i) \in (-\infty,
      \alpha); \\
      &\forall s \in \mathcal{S} \setminus \{ s_i \}, \; V_{\mathbf{r}}(s) \in
      \mathbb{R}; \\
      &\left. \forall s \in \mathcal{S},\;V_{\mathbf{r}}(s) = \log \sum_{a \in
          \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}}
          \mathcal{T}(s, a, s')V_{\mathbf{r}}(s') \right) \right\}
    \end{split}
  \]
  is measurable. Since this set can be constructed in Zermelo-Fraenkel set
  theory \emph{without} the axiom of choice, it is measurable
  \cite{herrlich2006axiom}, which proves that $V(s)$ is a measurable function
  for any $s \in \mathcal{S}$.
\end{proofsketch}

\begin{definition}
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} is
  \[ \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|. \]
\end{definition}

\begin{theorem} \label{thm:bound}
  If the initial values of the MDP value function satisfy the following
  bound, then the bound remains satisfied throughout value iteration:
  \begin{equation} \label{eq:bound}
    |V_{\mathbf{r}}(s)| \le \vbound.
  \end{equation}
\end{theorem}
\begin{proof}
  We begin by considering \eqref{eq:bound} without taking the absolute value of
  $V_{\mathbf{r}}(s)$, i.e.,
  \begin{equation} \label{eq:positive_bound}
    V_{\mathbf{r}}(s) \le \vbound,
  \end{equation}
  and assuming that the initial values of $\{ V_{\mathbf{r}}(s) \mid s \in
  \mathcal{S} \}$ already satisfy \eqref{eq:positive_bound}. For each $s \in
  \mathcal{S}$, the value of $V_{\mathbf{r}}(s)$ is updated via this rule:
  \[ V_{\mathbf{r}}(s) \coloneqq \log \sum_{a \in \mathcal{A}} \exp\left( r(s) +
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s')V_{\mathbf{r}}(s') \right). \]
  Note that both $\log$ and $\exp$ are increasing functions, $\gamma > 0$, and
  the $\mathcal{T}$ function gives a probability (a non-negative number).
  Thus
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\le \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s'
          \in \mathcal{S}} \mathcal{T}(s, a, s')\frac{\rinf +
          \log|\mathcal{A}|}{1 - \gamma} \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma}\sum_{s' \in \mathcal{S}}
        \mathcal{T}(s, a, s') \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma} \right)
    \end{split}
  \]
  by the definition of $\mathcal{T}$. Then
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\le \log \left( |\mathcal{A}| \exp\left( r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log \left( \exp\left( \log|\mathcal{A}| + r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \\
      &= \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + r(s))}{1 - \gamma} \\
      &\le \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + \rinf)}{1 - \gamma} \\
      &= \vbound
    \end{split}
  \]
  by the definition of $\rinf$.

  The proof for
  \begin{equation} \label{eq:negative_bound}
    V_{\mathbf{r}}(s) \ge \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1}
  \end{equation}
  follows the same argument until we get to
  \[
    \begin{split}
      V_{\mathbf{r}}(s) &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(\log|\mathcal{A}| + r(s))}{\gamma - 1} \\
      &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(-\log|\mathcal{A}| -\rinf)}{\gamma - 1} \\
      &= \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1},
    \end{split}
  \]
  where we use the fact that $r(s) \ge -\rinf - 2\log|\mathcal{A}|$. Combining
  \eqref{eq:positive_bound} and \eqref{eq:negative_bound} gives
  \eqref{eq:bound}.
\end{proof}

\begin{theorem}[The Lebesgue Dominated Convergence Theorem
  \cite{royden2010real}] \label{thm:lebesgue}
  Let $(X, \mathcal{M}, \mu)$ be a measure space and $\{ f_n \}$ a sequence of
  measurable functions on $X$ for which $\{ f_n \} \to f$ pointwise a.e. on $X$
  and the function $f$ is measurable. Assume there is a non-negative function
  $g$ that is integrable over $X$ and dominates the sequence $\{ f_n \}$ on $X$
  in the sense that
  \[ |f_n| \le g \text{ a.e. on $X$ for all $n$.} \]
  Then $f$ is integrable over $X$ and
  \[ \lim_{n \to \infty} \int_X f_n\,d\mu = \int_X f\,d\mu. \]
\end{theorem}

%\begin{definition}[\cite{royden2010real}]
%  A non-negative measurable function $f$ on a measurable set $E$ is said to be
%  \emph{integrable} over $E$ provided
%  \[ \int_E f < \infty. \]
%\end{definition}

\begin{proposition}[\cite{royden2010real}]
  Let $f$ be a measurable function on $E$. Suppose there is a non-negative
  function $g$ that is integrable over $E$ and dominates $f$ in the sense that
  \[ |f| \le g \text{ on } E. \]
  Then $f$ is integrable over $E$.
\end{proposition}

\begin{theorem}
  Using our usual notation,
  \[ \dt\iiint
    V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})q(\bm\lambda)\dx
    = \iiint
    \dt[V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})q(\bm\lambda)]\dx, \]
  where $t \in \{ \mathbf{m}, \mathbf{S}, \alpha_0, \dots, \alpha_d, \beta_0,
  \dots, \beta_d \}$.
\end{theorem}
\begin{proof}
  Let
  \begin{align*}
    \f &= V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})q(\bm\lambda), \\
    F(t) &= \iiint \f\dx,
  \end{align*}
  and, for any $t$, let $(t_n)_{n=1}^\infty$ be any sequence such that
  $\lim_{n \to \infty} t_n = t$, but $t_n \ne t$ for all $n$. We want to show
  that
  \begin{equation} \label{eq:to_prove}
    F'(t) = \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \iiint \df\dx.
  \end{equation}
  We have
  \[ \frac{F(t_n) - F(t)}{t_n - t} = \iiint \frac{\ftn - \f}{t_n - t}\dx =
    \iiint \fn\dx, \]
  where
  \[ \fn = \frac{\ftn - \f}{t_n - t}. \]
  Since
  \[ \lim_{n \to \infty} \fn = \df, \]
  \eqref{eq:to_prove} follows from Theorem \ref{thm:lebesgue} as soon as we show
  that both $f$ and $f_n$ are measurable and find a non-negative integrable
  function $g$ such that for all $n$, $\mathbf{r}$, $\mathbf{u}$, $\bm\lambda$,
  \[ |\fn| \le \g. \]
  The MDP value function is measurable by Theorem \ref{thm:measurability}. The
  result of multiplying or adding measurable functions (e.g., probability
  density functions (PDFs)) to a measurable function is still measurable. Thus,
  both $f$ and $f_n$ are measurable.

  It remains to find $g$. Without loss of generality, assume that $t$ is a
  parameter of $q(\bm\lambda)$. Then
  \[ |\fn| = |V_{\mathbf{r}}(s)q(\mathbf{r})q(\mathbf{u})q(\bm\lambda)| =
    |V_{\mathbf{r}}(s)|q(\mathbf{r})q(\mathbf{u}) \left| \frac{q(\bm\lambda)|_{t = t_n} -
        q(\bm\lambda)}{t_n - t} \right| \]
  since PDFs are non-negative. An upper bound for
  $|V_{\mathbf{r}}(s)|$ is given by Theorem \ref{thm:bound}, while
  \[ \frac{q(\bm\lambda)|_{t = t_n} - q(\bm\lambda)}{t_n - t} = \left.
      \frac{\partial q(\bm\lambda)}{\partial t} \right|_{t = c} \]
  for some $c$ between $t$ and $t_n$ due to the mean value theorem (since $q$ is
  a continuous and differentiable function of $t$, regardless of the specific
  choices of $q$ and $t$). % TODO: is it?
  Therefore,
  \[ |\fn| \le \vbound q(\mathbf{r})q(\mathbf{u}) \left| \left. \frac{\partial
        q(\bm\lambda)}{\partial t} \right|_{t=c} \right|. \]
The bound is clearly non-negative and measurable. It remains to show that it is
also integrable. Let $\mathbf{r} = (r_1, \dots, r_k)^\intercal$. Then
\[ \vbound q(\mathbf{r}) \le \frac{q(\mathbf{r})}{1 - \gamma} \left(
    \log|\mathcal{A}| + \sum_{i=1}^k |r_i| \right), \]
and
\[ \int \frac{q(\mathbf{r})}{1 - \gamma} \left(\log|\mathcal{A}| + \sum_{i=1}^k
    |r_i| \right)\,d\mathbf{r} = \frac{\log|\mathcal{A}|}{1 - \gamma} +
  \frac{1}{1 - \gamma} \sum_{i=1}^k \mathbb{E}[|r_i|], \]
which clearly exists and is finite, so
\[ \int \vbound q(\mathbf{r}) \,d\mathbf{r} < \infty \]
for all $\gamma$ and $\mathbf{u}$. The existence of
\[ \iiint \vbound q(\mathbf{r})q(\mathbf{u}) \left| \left. \frac{\partial
        q(\bm\lambda)}{\partial t} \right|_{t=c} \right|\dx \]
then comes from a boundedness argument, as...
% not quite, E[|r|] can be any function of u.
% TODO: finish this
\end{proof}

\begin{theorem}[Differentiating under the integral sign]
  Assume $f : R \times R \to R$ is such that $x \mapsto f(x, t)$ is measurable
  for each $t \in R$, that $f(x, t_0)$ is integrable for some $t_0 \in R$ and
  $\frac{\partial f(x, t)}{\partial t}$ exists for each $(x, t)$. Assume also
  that there is an integrable $g : R \to R$ with $\left| \frac{\partial f(x,
      t)}{\partial t} \right| \le g(x)$ for each $x, t \in R$. Then the function
  $x \mapsto f(x, t)$ is integrable for each $t$ and the function $F : R \to R$
  defined by
  \[ F(t) = \int_R f_t\,d\mu = \int_R f(x, t)\,d\mu(x) \]
  is differentiable with derivative
  \[ F'(t) = \frac{d}{dt} \int_R f(x, t)\,d\mu(x) = \int_R
    \frac{\partial}{\partial t} f(x, t)\,d\mu(x). \]
\end{theorem}
\begin{proof}
%  Applying the mean value theorem to the function $t \mapsto f(x, t)$, for each
%  $t = t_0$ we have to have some $c$ between $t_0$ and $t$ so that
%  \[ f(x, t) - f(x, t_0) = \left.\frac{\partial f}{\partial t}\right|_{(x, c)}(t
%    - t_0). \]
%  It follows that
%  \[ |f(x, t) - f(x, t_0)| \le g(x)|t - t_0| \]
%  and so
%  \[ |f(x, t)| \le |f(x, t_0)| + g(x)|t - t_0|. \]
%  Thus
%  \[ \int_R |f(x, t)|\,d\mu(x) \le \int (|f(x, t_0)| + g(x)|t - t_0|)\,d\mu(x)
%    = \int_R |f(x, t_0)|\,d\mu(x) + |t - t_0|\int_R g\,d\mu < \infty, \]
%  which establishes that the function $x \mapsto f(x, t)$ is integrable for each
%  $t$.
  To prove the formula for $F(t)$ consider any sequence $(t_n)_{n=1}^\infty$ so
  that $\lim_{n \to \infty} t_n = t$ but $t_n \neq t$ for each $t$. We claim
  that
  \begin{equation} \label{tempEq}
    \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \int_R
    \frac{\partial f(x, t)}{\partial t}\,d\mu(x).
  \end{equation}
  We have
  \[ \frac{F(t_n) - F(t)}{t_n - t} = \int_R \frac{f(x, t_n) - f(x, t)}{t_n -
      t}\,d\mu(x) = \int_R f_n(x)\,d\mu(x) \]
  where
  \[ f_n(x) = \frac{f(x, t_n) - f(x, t)}{t_n - t}. \]
  Notice that, for each $x$ we know
  \[ \lim_{n \to \infty} f_n(x) = \left.\frac{\partial f}{\partial
        t}\right|_{(x, t)} \]
  and so \eqref{tempEq} will follow from the dominated convergence theorem once we show
  that $|f_n(x)| \le g(x)$ for each $x$.

  That follows from the mean value theorem again because there is $c$ between $t$
  and $t_0$ (with $c$ depending on $x$) so that
  \[ f_n(x) = \frac{f(x, t_n) - f(x, t)}{t_n - t} = \left.\frac{\partial
        f}{\partial t}\right|_{(x, c)}. \]
  So $|f_n(x)| \le g(x)$ for each $x$.
\end{proof}

\section{Derivatives of the Evidence Lower Bound}

\subsection{$\partial/\partial\mathbf{m}$}

We begin by removing terms independent of $\mathbf{m}$:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{m}} =
    &- \frac{1}{2}\dm[\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}]
    + \dm[\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \dm\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
      s')\dm\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
Here
\begin{align*}
  \dm[\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}] &= (\mathbb{E}[\Kuu^{-1}] + \mathbb{E}[\Kuu^{-1}]^\intercal)\mathbf{m}, \\
  \dm[\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}] &= \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}],
\end{align*}
and
\begin{equation} \label{eq:1}
  \begin{split}
    \dm\mathbb{E}[V_{\mathbf{r}}(s)] &= \dm\iiint V_{\mathbf{r}}(s) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda \\
    &= \iiint V_{\mathbf{r}}(s) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \dm\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda,
  \end{split}
\end{equation}
where
\[
  \begin{split}
    \dm\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S}) &=
    \mathcal{N}(\mathbf{u}; \mathbf{m},
    \mathbf{S})\dm\left[-\frac{1}{2}(\mathbf{u} -
      \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})\right] \\
    &= \mathcal{N}(\mathbf{u}; \mathbf{m},
    \mathbf{S})\left(-\frac{1}{2}\right)(\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})\dm[\mathbf{u} -
    \mathbf{m}] \\
    &= \mathcal{N}(\mathbf{u}; \mathbf{m},
    \mathbf{S})\frac{1}{2}(\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u}
    - \mathbf{m}).
  \end{split}
\]
Substituting it back into \eqref{eq:1} gives
\[
  \begin{split}
    \dm\mathbb{E}[V_{\mathbf{r}}(s)] &= \frac{1}{2}\iiint V_{\mathbf{r}}(s)
    (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})
    p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) \mathcal{N}(\mathbf{u};
    \mathbf{m}, \mathbf{S})
    q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda \\
    &= \frac{1}{2}\mathbb{E}[V_{\mathbf{r}}(s) (\mathbf{S}^{-1} +
    \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})].
\end{split}
\]
Hence
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{m}} =
    &-\frac{1}{2}(\mathbb{E}[\Kuu^{-1}] +
    \mathbb{E}[\Kuu^{-1}]^\intercal)\mathbf{m} +
    \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}] \\
    &- \frac{1}{2}\sum_{i=1}^N\sum_{t=1}^T \mathbb{E}[V_{\mathbf{r}}(s_{i,t})
    (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} - \mathbf{m})] \\
    &- \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')
    \mathbb{E}[V_{\mathbf{r}}(s') (\mathbf{S}^{-1} + \mathbf{S}^{-\intercal})(\mathbf{u} -
    \mathbf{m})].
\end{split}
\]

\subsection{$\partial/\partial\mathbf{S}$}

Similarly to the previous section,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{S}} &=
    \frac{1}{2}\dS\log|\mathbf{S}|
    -\frac{1}{2}\dS\Tr[\mathbb{E}[\Kuu^{-1}]\mathbf{S}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \dS\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
      s')\dS\mathbb{E}[V_{\mathbf{r}}(s')],
  \end{split}
\]
where
\[ \dS\log|\mathbf{S}| = \mathbf{S}^{-\intercal}, \]
and
\[ \dS\Tr[\mathbb{E}[\Kuu^{-1}]\mathbf{S}] = \mathbb{E}[\Kuu^{-1}]^\intercal \]
by \emph{The Matrix Cookbook} \cite{petersen2008matrix}. Then
\[ \dS\mathbb{E}[V_{\mathbf{r}}(s)] = \iiint V_{\mathbf{r}}(s) q(\mathbf{r}) \dS\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
  q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda, \]
where
\[
  \begin{split}
    \dS\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S}) &=
    \dS\left[\frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\exp \left( -\frac{1}{2}
        (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
        \mathbf{m})\right)\right] \\
    &= \dS\left[\frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\right]\exp \left( -\frac{1}{2}
        (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
        \mathbf{m})\right) \\
      &+ \frac{1}{(2\pi)^{m/2}|\mathbf{S}|^{1/2}}\dS\left[\exp\left( -\frac{1}{2}
        (\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
        \mathbf{m})\right)\right] \\
    &=
    \frac{1}{(2\pi)^{m/2}}\dS\left[\frac{1}{|\mathbf{S}|^{1/2}}\right]\exp
    \left( -\frac{1}{2} (\mathbf{u} -
      \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})\right) \\
    &- \frac{1}{2}\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})\dS[(\mathbf{u}
    - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} - \mathbf{m})]. \\
  \end{split}
\]
The two remaining derivatives can be taken with the help of \emph{The Matrix
  Cookbook} \cite{petersen2008matrix}:
\begin{align*}
  \dS\left[\frac{1}{|\mathbf{S}|^{1/2}}\right] =
  -\frac{1}{2}|\mathbf{S}|^{-3/2}\frac{\partial |\mathbf{S}|}{\partial \mathbf{S}} =
  -\frac{1}{2}|\mathbf{S}|^{-3/2}|\mathbf{S}|\mathbf{S}^{-\intercal} &=
  -\frac{1}{2|\mathbf{S}|^{1/2}}\mathbf{S}^{-\intercal}, \\
  \dS[(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-1}(\mathbf{u} -
  \mathbf{m})] &= -\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
  \mathbf{m})^\intercal\mathbf{S}^{-\intercal}.
\end{align*}
Plugging them back in gives
\[ \dS\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S}) =
  -\frac{1}{2}\mathbf{S}^{-\intercal}\mathcal{N}(\mathbf{u};
  \mathbf{m}, \mathbf{S}) + \frac{1}{2}\mathcal{N}(\mathbf{u}; \mathbf{m},
  \mathbf{S})\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
  \mathbf{m})^\intercal\mathbf{S}^{-\intercal}, \]
and
\[
  \begin{split}
    \dS\mathbb{E}[V_{\mathbf{r}}(s)] &= \frac{1}{2}\iiint V_{\mathbf{r}}(s)
    (\mathbf{S}^{-\intercal}(\mathbf{u} - \mathbf{m})(\mathbf{u} -
    \mathbf{m})^\intercal\mathbf{S}^{-\intercal} - \mathbf{S}^{-\intercal})
    q(\mathbf{r}) \mathcal{N}(\mathbf{u};
    \mathbf{m}, \mathbf{S}) q(\bm\lambda)\,d\mathbf{r}\,d\mathbf{u}\,d\bm\lambda
    \\
    &= \frac{1}{2}\mathbb{E}[V_{\mathbf{r}}(s)(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})].
  \end{split}
\]
Therefore
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\mathbf{S}} &=
    \frac{1}{2}\mathbf{S}^{-\intercal} -
    \frac{1}{2}\mathbb{E}[\Kuu^{-1}]^\intercal - \frac{1}{2}
    \sum_{i=1}^N\sum_{t=1}^T
    \mathbb{E}[V_{\mathbf{r}}(s_{i,t})(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})] \\
    &- \gamma\sum_{s' \in \mathcal{S}}\mathcal{T}(s_{i,t}, a_{i,t}, s')
    \mathbb{E}[V_{\mathbf{r}}(s')(\mathbf{S}^{-\intercal}(\mathbf{u} -
    \mathbf{m})(\mathbf{u} - \mathbf{m})^\intercal\mathbf{S}^{-\intercal} -
    \mathbf{S}^{-\intercal})].
  \end{split}
\]

\subsection{$\partial/\partial\alpha_j$}

We begin in the usual way:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\alpha_j} = &-
    \frac{1}{2}\da\mathbb{E}[\Tr[\Kuu^{-2}]] - \frac{1}{2}\da\mathbb{E}[\Tr[\Kuu^{-1}\mathbf{S}]] - \frac{1}{2}\da\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] - \frac{1}{2}\da\mathbb{E}[\log|\Kuu|] \\
    &+ \da\mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] +
    \da [\alpha_j + \log\Gamma(\alpha_j) + (1 - \alpha_j)\psi(\alpha_j)] \\
      &- \sum_{i=1}^N \sum_{t=1}^T \da\mathbb{E}[V_{\mathbf{r}}(s_{i,t})] -
        \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t},
        s')\da\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
First,
\[ \da [\alpha_j + \log\Gamma(\alpha_j) + (1 - \alpha_j)\psi(\alpha_j)] = 1 +
  \psi(\alpha_j) - \psi(\alpha_j) + (1 - \alpha_j)\psi'(\alpha_j) = 1 + (1 -
  \alpha_j)\psi'(\alpha_j) \]
by the definition of $\psi$. The remaining terms can all be treated in the same
way, as they all contain expectations of scalar functions that are independent
of $\alpha_j$, and $\alpha_j$ only occurs in $\Gamma(\lambda_j; \alpha_j,
\beta_j)$. Thus we can work with an abstract function as follows:
\[
  \begin{split}
    \da\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \da \iiint f(k_{\bm\lambda},
    \mathbf{r}) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \!\begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1}) \da
      \left[ \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)} \lambda_j^{\alpha_j -
          1} \right] e^{-\beta_j\lambda_j} \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}.
    \end{multlined}
  \end{split}
\]
Then
\[
  \begin{split}
    \da \left[ \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)} \lambda_j^{\alpha_j - 1} \right] &=
    \frac{\da[\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}]\Gamma(\alpha_j) -
      \beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\da[\alpha_j\log\beta_j + (\alpha_j -
      1)\log\lambda_j]\Gamma(\alpha_j) - \beta_j^{\alpha_j}\lambda_j^{\alpha_j -
        1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}(\log\beta_j + \log\lambda_i)\Gamma(\alpha_j)
      - \beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}\Gamma'(\alpha_j)}{(\Gamma(\alpha_j))^2} \\
    &= \frac{\beta_j^{\alpha_j}\lambda_j^{\alpha_j - 1}}{\Gamma(\alpha_j)}\left(\log\beta_j +
    \log\lambda_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)}\right),
  \end{split}
\]
which means that
\[
  \begin{split}
    \da\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \!\begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)}\lambda_j^{\alpha_j -
        1}e^{-\beta_j\lambda_j} \left(\log\beta_j + \log\lambda_j -
        \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right) \\
      q(\lambda_{j+1}) \cdots q(\lambda_d)
      q(\mathbf{r})q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}
    \end{multlined} \\
    &= \iiint f(k_{\bm\lambda}, \mathbf{r}) \left(\log\beta_j + \log\lambda_j -
      \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right) q(\bm\lambda)
    q(\mathbf{r})q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \mathbb{E} \left[ f(k_{\bm\lambda}, \mathbf{r}) \left(\log\beta_j +
        \log\lambda_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right)
    \right] \\
    &= \left( \log\beta_j - \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)}
    \right)\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] +
    \mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})\log\lambda_j].
  \end{split}
\]
With these results in mind, we can simplify the initial expression to
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\alpha_j} &= 1 + (1 -
    \alpha_j)\psi'(\alpha_j) + \left( \log\beta_j -
      \frac{\Gamma'(\alpha_j)}{\Gamma(\alpha_j)} \right)
    \left(\vphantom{\sum_{i=1}^N} \right. - \frac{1}{2}
    \mathbb{E}[\Tr[\Kuu^{-2}]] - \frac{1}{2}\mathbb{E}[\Tr[\Kuu^{-1}\mathbf{S}]]
    \\
    &- \frac{1}{2}\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] -
    \frac{1}{2}\mathbb{E}[\log|\Kuu|] +
    \mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \mathbb{E}[V_{\mathbf{r}}(s_{i,t})] - \gamma\sum_{s' \in
      \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}[V_{\mathbf{r}}(s')] \left.
      \vphantom{\sum_{i=1}^N} \right) \\
    & - \frac{1}{2}\mathbb{E}[\Tr[\Kuu^{-2}]\log\lambda_j] -
    \frac{1}{2}\mathbb{E}[\Tr[\Kuu^{-1}\mathbf{S}]\log\lambda_j] -
    \frac{1}{2}\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}\log\lambda_j]
    \\
    &- \frac{1}{2}\mathbb{E}[\log|\Kuu|\log\lambda_j] +
    \mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}\log\lambda_j]
    \\
    &- \sum_{i=1}^N\sum_{t=1}^T\mathbb{E}[V_{\mathbf{r}}(s_{i,t})\log\lambda_j] -
    \gamma\sum_{s' \in \mathcal{S}}\mathcal{T}(s_{i,t}, a_{i,t},
    s')\mathbb{E}[V_{\mathbf{r}}(s')\log\lambda_j].
  \end{split}
\]

\subsection{$\partial/\partial\beta_j$}
Finally,
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\beta_j} = &-
    \frac{1}{2}\db\mathbb{E}[\Tr[\Kuu^{-2}]] -
    \frac{1}{2}\db\mathbb{E}[\Tr[\Kuu^{-1}\mathbf{S}]] -
    \frac{1}{2}\db\mathbb{E}[\mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] \\
    &- \frac{1}{2}\db\mathbb{E}[\log|\Kuu|] +
    \db\mathbb{E}[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}] -
    \db[\log\beta_j] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \db \mathbb{E}[V_{\mathbf{r}}(s_{i,t})] - \gamma\sum_{s'
      \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\db\mathbb{E}[V_{\mathbf{r}}(s')].
  \end{split}
\]
Similarly to the previous section, we can handle all derivatives of expectations
in the same way:
\[
  \begin{split}
    \db\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \db \iiint f(k_{\bm\lambda},
    \mathbf{r}) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \!\begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\lambda_j^{\alpha_j - 1}}{\Gamma(\alpha_j)} \db [\beta_j^{\alpha_j}
      e^{-\beta_j\lambda_j}] \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}.
    \end{multlined}
  \end{split}
\]
Since
\[ \db [\beta_j^{\alpha_j} e^{-\beta_j\lambda_j}] = \alpha_j\beta_j^{\alpha_j - 1}e^{-\beta_j\lambda_j}
  - \beta_j^{\alpha_j} e^{-\beta_j\lambda_j}\lambda_j = \beta_j^{\alpha_j}
  e^{-\beta_j\lambda_j} \left( \frac{\alpha_j}{\beta_j} - \lambda_j \right), \]
we have that
\[
  \begin{split}
    \db\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] &= \!\begin{multlined}[t]
      \iiint f(k_{\bm\lambda}, \mathbf{r}) q(\lambda_0) \cdots q(\lambda_{j-1})
      \frac{\beta_j^{\alpha_j}}{\Gamma(\alpha_j)}\lambda_j^{\alpha_j -
        1}e^{-\beta_j\lambda_j} \left(\frac{\alpha_j}{\beta_j} - \lambda_j
      \right) \\
      q(\lambda_{j+1}) \cdots q(\lambda_d) q(\mathbf{r})
      q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u}
    \end{multlined}
    \\
    &= \iiint f(k_{\bm\lambda}, \mathbf{r}) \left(\frac{\alpha_j}{\beta_j} -
      \lambda_j \right) q(\bm\lambda) q(\mathbf{r})
    q(\mathbf{u})\,d\bm\lambda\,d\mathbf{r}\,d\mathbf{u} \\
    &= \mathbb{E} \left[ f(k_{\bm\lambda}, \mathbf{r})
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] =
    \frac{\alpha_j}{\beta_j}\mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})] -
    \mathbb{E}[f(k_{\bm\lambda}, \mathbf{r})\lambda_j].
  \end{split}
\]
This gives us the final expression of $\frac{\partial\mathcal{L}}{\partial\beta_j}$:
\[
  \begin{split}
    \frac{\partial\mathcal{L}}{\partial\beta_j} = &- \frac{1}{\beta_j} -
    \frac{1}{2}\mathbb{E} \left[ \Tr[\Kuu^{-2}] \left( \frac{\alpha_j}{\beta_j}
        - \lambda_j \right) \right] - \frac{1}{2}\mathbb{E} \left[
      \Tr[\Kuu^{-1}\mathbf{S}] \left(\frac{\alpha_j}{\beta_j} - \lambda_j
      \right) \right] \\
    &- \frac{1}{2}\mathbb{E} \left[ \mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] -
    \frac{1}{2}\mathbb{E} \left[ \log|\Kuu| \left(\frac{\alpha_j}{\beta_j} -
        \lambda_j \right) \right] \\
    &+ \mathbb{E} \left[ \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\mathbf{m}
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] \\
    &- \sum_{i=1}^N \sum_{t=1}^T \mathbb{E} \left[ V_{\mathbf{r}}(s_{i,t})
      \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right] -
    \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}
    \left[V_{\mathbf{r}}(s') \left(\frac{\alpha_j}{\beta_j} - \lambda_j \right) \right].
  \end{split}
\]

\bibliographystyle{abbrv}
\bibliography{paper.bib}

\end{document}