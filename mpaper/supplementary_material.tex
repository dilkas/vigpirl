\documentclass{article}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{url}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newenvironment{proofsketch}{%
  \renewcommand{\proofname}{Proof sketch}\proof}{\endproof}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\adj}{adj}

\newcommand{\Eq}{\mathbb{E}_{(\mathbf{u}, \mathbf{r}) \sim \approximation}}
\newcommand{\pfull}{p(\mathcal{D}, \mathbf{X_u}, \mathbf{u}, \mathbf{r})}
\newcommand{\approximation}{q_{\bm\nu}(\mathbf{u}, \mathbf{r})}
\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Luu}{\mathbf{L}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}
\newcommand{\V}{V_{\mathbf{r}}}

\newcommand{\dm}{\frac{\partial}{\partial\bm\mu}}
\newcommand{\dS}{\frac{\partial}{\partial\mathbf{S}}}
\newcommand{\dD}{\frac{\partial}{\partial\mathbf{D}}}
\newcommand{\dB}{\frac{\partial}{\partial\mathbf{B}}}
\newcommand{\dt}{\frac{\partial}{\partial t}}
\newcommand{\dl}{\frac{\partial}{\partial \lambda_i}}
\newcommand{\dlj}{\frac{\partial}{\partial \lambda_j}}

\newcommand{\f}{f(\mathbf{r}, \mathbf{u}, t)}
\newcommand{\ftn}{f(\mathbf{r}, \mathbf{u}, t_n)}
\newcommand{\fn}{f_n(\mathbf{r}, \mathbf{u})}
\newcommand{\dx}{\,d\mathbf{r}\,d\mathbf{u}}
\newcommand{\df}{\left.\frac{\partial f}{\partial t}\right|_{(\mathbf{r},
    \mathbf{u}, t)}}
\newcommand{\g}{g(\mathbf{r}, \mathbf{u})}
\newcommand{\rinf}{\lVert \mathbf{r} \rVert_\infty}
\newcommand{\vbound}{\frac{\rinf + \log|\mathcal{A}|}{1 - \gamma}}

\title{Variational Inference for Inverse Reinforcement Learning with Gaussian
  Processes: Supplementary Material}
\author{Paulius Dilkas (2146879)}

\begin{document}
\maketitle

% TODO: express all lambdas as exponentials (update the derivatives or sth)
\section{Preliminaries}

For any matrix $\mathbf{A}$, we will use either $A_{i,j}$ or
$[\mathbf{A}]_{i,j}$ to denote the element of $\mathbf{A}$ in row $i$ and column
$j$.

For any vector $\mathbf{x}$, we write $\mathbb{R}_d[\mathbf{x}]$ to denote a
vector space of polynomials with degree at most $d$, where variables are
elements of $\mathbf{x}$, and coefficients are in $\mathbb{R}$.

In this paper, all references to measurability are with respect to the Lebesgue
measure. Similarly, whenever we consider the existence of an integral, we use
the Lebesgue definition of integration.

As recently suggested by Ong et al. \cite{ong2018gaussian}, we use a
decomposition $\bm\Sigma = \mathbf{B}\mathbf{B}^\intercal + \mathbf{D}^2$, where
$\mathbf{B}$ is a lower triangular $m \times p$ matrix with positive diagonal
entries, and $\mathbf{D} = \diag(d_1, \dots, d_m)$. Typically, we would set $p$
so that $p \ll m$ to get an efficient approximation, but it is also worth
pointing out that we can retain full accuracy by setting $p = m$ and $\mathbf{D}
= \mathbf{O}$. Moreover, we define a few matrices that will simplify expressions
for the derivatives: $\mathbf{T} = (\mathbf{B}\mathbf{B}^\intercal +
(\mathbf{D}^2)^\intercal)^{-1}$ and $\mathbf{U} = (\mathbf{u} -
\bm\mu)(\mathbf{u} - \bm\mu)^\intercal$.

\begin{lemma}[Derivatives of probability
  distributions] \label{lemma:derivatives}
  \begin{enumerate}
    \leavevmode
  \item $\frac{\partial q(\mathbf{u})}{\partial \bm\mu} =
    \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u}
    - \bm\mu)$.
  \item
    \begin{enumerate}
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} =
      \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-\intercal}\mathbf{U}\bm\Sigma^{-\intercal}
      - \bm\Sigma^{-\intercal})$.
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
      \frac{1}{2}q(\mathbf{u}) \{ |\bm\Sigma|^{-1}(\adj(\mathbf{T}) +
      \adj(\bm\Sigma)) + \mathbf{TUT} +
      \bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} \} \mathbf{B}$.
    \item
      $\frac{\partial q(\mathbf{u})}{\partial \mathbf{D}} =
      \frac{1}{2}q(\mathbf{u}) \{ \mathbf{T}\mathbf{UT}\mathbf{D}^\intercal +
      \mathbf{D}^\intercal\mathbf{T}\mathbf{UT} -
      |\bm\Sigma|^{-1}(\adj(\mathbf{T})\mathbf{D}^\intercal +
      \mathbf{D}^\intercal \adj(\mathbf{T})) \}$.
    \end{enumerate}
  \item For $i = 0, \dots, d$,
    \[
      \frac{\partial q(\mathbf{r})}{\partial \lambda_i} =
      \frac{1}{2}q(\mathbf{r})\tr
      \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} -
        \Kuu^{-1}) \frac{\partial \Kuu}{\partial \lambda_i} \right),
    \]
    where
    \[
      \frac{\partial \Kuu}{\partial \lambda_i} = \frac{1}{\lambda_i}\Kuu
    \]
    if $i = 0$, and
    \[
      \left[ \frac{\partial \Kuu}{\partial \lambda_i} \right]_{j,k} =
      k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
      \left( -\frac{1}{2}(x_{\mathbf{u},j,i} - x_{\mathbf{u},k,i})^2 -
        \mathbbm{1}[j \ne k]\sigma^2 \right)
    \]
    otherwise.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \leavevmode
  \begin{enumerate}
  \item
    \[
      \begin{split}
        \frac{\partial q(\mathbf{u})}{\partial m} &=
        q(\mathbf{u})\dm\left[-\frac{1}{2}(\mathbf{u} -
          \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)\right]
        \\
        &= -\frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
        \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu)\dm[\mathbf{u} -
        \bm\mu] \\
        &= \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
        \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu).
      \end{split}
    \]
  \item For $\mathbf{S} \in \{ \bm\Sigma, \mathbf{B}, \mathbf{D} \}$,
    \begin{align}
      \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}} &= \dS\left[\frac{1}{(2\pi)^{m/2}|\bm\Sigma|^{1/2}}\exp \left( -\frac{1}{2} (\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)\right)\right] \nonumber \\
                                                         &= \dS\left[\frac{1}{(2\pi)^{m/2}|\bm\Sigma|^{1/2}}\right]\exp \left( -\frac{1}{2} (\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)\right) \nonumber \\
                                                         &+ \frac{1}{(2\pi)^{m/2}|\bm\Sigma|^{1/2}}\dS\left[\exp\left( -\frac{1}{2} (\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)\right)\right] \nonumber \\
                                                         &= -\frac{1}{2} q(\mathbf{u}) \left( |\bm\Sigma|^{-3/2} \frac{\partial |\bm\Sigma|}{\partial \mathbf{S}} + \dS[(\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)] \right). \label{eq:2_before}
    \end{align}
    For derivatives with respect to $\bm\Sigma$, we can refer to Petersen and
    Pedersen \cite{petersen2008matrix}:
    \begin{equation} \label{eq:partial_derivatives1}
      \begin{gathered}
        \frac{\partial |\bm\Sigma|}{\partial \bm\Sigma} =
        |\bm\Sigma|\bm\Sigma^{-\intercal}, \\
        \dS[(\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} -
        \bm\mu)] = -\bm\Sigma^{-\intercal}\mathbf{U}\bm\Sigma^{-\intercal},
      \end{gathered}
    \end{equation}
    while we can use an online tool by Laue et
    al.\footnote{\url{http://www.matrixcalculus.org/}}
    \cite{DBLP:conf/nips/LaueMG18} for the remaining ones:
    \begin{equation} \label{eq:partial_derivatives2}
      \begin{gathered}
        \frac{\partial |\bm\Sigma|}{\partial \mathbf{B}} =
        (\adj(\mathbf{T}) + \adj(\bm\Sigma))\mathbf{B}, \\
        \frac{\partial |\bm\Sigma|}{\partial \mathbf{D}} =
        \adj(\mathbf{T})\mathbf{D}^\intercal + \mathbf{D}^\intercal
        \adj(\mathbf{T}), \\
        \dB[(\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} -
        \bm\mu)] = -(\mathbf{TUT} +
        \bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1})\mathbf{B}, \\
        \dD[(\mathbf{u} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{u} - \bm\mu)] =
        -\mathbf{T}\mathbf{UT}\mathbf{D}^\intercal -
        \mathbf{D}^\intercal\mathbf{T}\mathbf{UT}.
      \end{gathered}
    \end{equation}
    Substituting results from \eqref{eq:partial_derivatives1} and
    \eqref{eq:partial_derivatives2} back into \eqref{eq:2_before} gives:
    \begin{gather*}
      \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} =
      \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-\intercal}\mathbf{U}\bm\Sigma^{-\intercal}
      - \bm\Sigma^{-\intercal}), \\
      \frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} =
      \frac{1}{2}q(\mathbf{u}) \{ |\bm\Sigma|^{-3/2}(\adj(\mathbf{T}) +
      \adj(\bm\Sigma)) + \mathbf{TUT} +
      \bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} \} \mathbf{B}, \\
      \frac{\partial q(\mathbf{u})}{\partial \mathbf{D}} =
      \frac{1}{2}q(\mathbf{u}) \{ \mathbf{T}\mathbf{UT}\mathbf{D}^\intercal +
      \mathbf{D}^\intercal\mathbf{T}\mathbf{UT} -
      |\bm\Sigma|^{-3/2}(\adj(\mathbf{T})\mathbf{D}^\intercal +
      \mathbf{D}^\intercal \adj(\mathbf{T})) \}.
    \end{gather*}
  \item Using a result by Rasmussen and Williams
    \cite{DBLP:books/lib/RasmussenW06},
    \[
      \frac{\partial q(\mathbf{r})}{\partial \lambda_i} = q(\mathbf{r}) \dl
      \left[-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} -
        \frac{1}{2}\log|\Kuu| \right] = \frac{1}{2}q(\mathbf{r})\tr
      \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} - \Kuu^{-1})
        \frac{\partial \Kuu}{\partial \lambda_i} \right).
    \]
    The remaining derivative is
    \[
      \frac{\partial \Kuu}{\partial \lambda_i} =
      \begin{cases}
        \frac{1}{\lambda_i}\Kuu & \text{if } i = 0, \\
        \Luu & \text{otherwise,}
      \end{cases}
    \]
    where
    \[
      \begin{split}
        [\Luu]_{j,k} &= \dl k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j},
        \mathbf{x}_{\mathbf{u},k}) \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \dl \left[-\frac{1}{2}(\mathbf{x}_{\mathbf{u},j} -
          \mathbf{x}_{\mathbf{u},k})^\intercal \bm\Lambda
          (\mathbf{x}_{\mathbf{u},j} - \mathbf{x}_{\mathbf{u},k}) -
          \mathbbm{1}[j \ne k]\sigma^2\tr(\bm\Lambda) \right] \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \dl \left[-\frac{1}{2}\sum_{l=1}^d \lambda_l
          (x_{\mathbf{u},j,l} - x_{\mathbf{u},k,l})^2 -
          \mathbbm{1}[j \ne k]\sigma^2\sum_{l=1}^d \lambda_l \right] \\
        &= k_{\bm\lambda}(\mathbf{x}_{\mathbf{u},j}, \mathbf{x}_{\mathbf{u},k})
        \left( -\frac{1}{2}(x_{\mathbf{u},j,i} -
        x_{\mathbf{u},k,i})^2 - \mathbbm{1}[j \ne k]\sigma^2 \right).
      \end{split}
    \]
  \end{enumerate}
\end{proof}

\subsection{Linear Algebra and Numerical Analysis}

\begin{definition}[Norms]
  For any finite-dimensional vector $\mathbf{x} = (x_1, \dots, x_n)^\intercal$,
  its \emph{maximum norm} is
  \[
    \lVert \mathbf{x} \rVert_\infty = \max_i |x_i|
  \]
  whereas its \emph{taxicab} (or \emph{Manhattan}) \emph{norm} is
  \[
    \lVert \mathbf{x} \rVert_1 = \sum_{i = 1}^n |x_i|.
  \]
  Let $\mathbf{A}$ be a matrix. For any vector norm $\lVert
  \cdot \rVert_p$, we can also define its \emph{induced norm} for matrices as
  \[
    \lVert \mathbf{A} \rVert_p = \sup_{\mathbf{x} \ne \mathbf{0}} \frac{\lVert
      \mathbf{Ax} \rVert_p}{\lVert \mathbf{x} \rVert_p}.
  \]
  In particular, for $p = \infty$, we have
  \[
    \lVert \mathbf{A} \rVert_\infty = \max_i \sum_{j} |A_{i,j}|.
  \]
\end{definition}

\begin{lemma}[Perturbation Lemma
  \cite{layton2014numerical}] \label{prop:condition_number}
  Let $\lVert \cdot \rVert$ be any matrix norm, and let $\mathbf{A}$ and
  $\mathbf{E}$ be matrices such that $\mathbf{A}$ is invertible and $\lVert
  \mathbf{A}^{-1} \rVert \lVert \mathbf{E} \rVert < 1$, then $\mathbf{A} +
  \mathbf{E}$ is invertible, and
  \[
    \lVert (\mathbf{A} + \mathbf{E})^{-1} \rVert \le \frac{\lVert
      \mathbf{A}^{-1} \rVert}{1 - \lVert \mathbf{A}^{-1} \rVert \lVert
      \mathbf{E} \rVert}.
  \]
\end{lemma}

\section{Proofs}

We primarily think of rewards as a vector $\mathbf{r} \in
\mathbb{R}^{|\mathcal{S}|}$, but sometimes we use a function notation $r(s)$ to
denote the reward of a particular state $s \in \mathcal{S}$. The functional
notation is purely a notational convenience.

MDP values are characterised by both a state and a reward function/vector. In
order to prove the next theorem, we think of the value function as $V :
\mathcal{S} \to \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$, i.e., $V$ takes a
state $s \in \mathcal{S}$ and returns a function $V(s) :
\mathbb{R}^{\mathcal{S}} \to \mathbb{R}$ that takes a reward vector $\mathbf{r}
\in \mathbb{R}^{|\mathcal{S}|}$ and returns a value of the state $s$,
$\V(s) \in \mathbb{R}$. The function $V(s)$ computes the values of
all states and returns the value of state $s$.

\begin{proposition} \label{thm:measurability}
  MDP value functions $V(s) : \mathbb{R}^{|\mathcal{S}|} \to \mathbb{R}$ (for $s
  \in \mathcal{S}$) are Lebesgue measurable.
\end{proposition}
\begin{proofsketch}
  For any reward vector $\mathbf{r} \in \mathbb{R}^{|\mathcal{S}|}$, the
  collection of converged value functions $\{ \V(s) \mid s \in
  \mathcal{S} \}$ satisfy
  \[
    \V(s) = \log \sum_{a \in \mathcal{A}}
    \exp\left( r(s) + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
      s')\V(s') \right)
  \]
  for all $s \in \mathcal{S}$. Let $s_0 \in \mathcal{S}$ be an arbitrary state.
  In order to prove that $V(s_0)$ is measurable, it is enough to show that for
  any $\alpha \in \mathbb{R}$, the set
  \[
    \begin{split}
      \left\{ \vphantom{\sum_{a \in \mathcal{A}}} \right. \mathbf{r} \in
      \mathbb{R}^{|\mathcal{S}|} \left. \vphantom{\sum_{a \in \mathcal{A}}} \;
        \middle| \; \right. &\V(s_0) \in (-\infty, \alpha); \\
      &\V(s) \in
      \mathbb{R} \text{ for all } s \in \mathcal{S} \setminus \{ s_0 \}; \\
      &\V(s) = \left. \log \sum_{a \in \mathcal{A}} \exp\left( r(s)
          + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a,
          s')\V(s') \right) \text{ for all } s \in
        \mathcal{S}\right\}
    \end{split}
  \]
  is measurable. Since this set can be constructed in Zermelo-Fraenkel set
  theory \emph{without} the axiom of choice, it is measurable
  \cite{herrlich2006axiom}, which proves that $V(s)$ is a measurable function
  for any $s \in \mathcal{S}$.
\end{proofsketch}

\begin{proposition} \label{thm:bound}
  If the initial values of the MDP value function satisfy the following
  bound, then the bound remains satisfied throughout value iteration:
  \begin{equation} \label{eq:bound}
    |\V(s)| \le \vbound.
  \end{equation}
\end{proposition}
\begin{proof}
  We begin by considering \eqref{eq:bound} without taking the absolute value of
  $\V(s)$, i.e.,
  \begin{equation} \label{eq:positive_bound}
    \V(s) \le \vbound,
  \end{equation}
  and assuming that the initial values of $\{ \V(s) \mid s \in
  \mathcal{S} \}$ already satisfy \eqref{eq:positive_bound}. For each $s \in
  \mathcal{S}$, the value of $\V(s)$ is updated via this rule:
  \[
    \V(s) \coloneqq \log \sum_{a \in \mathcal{A}} \exp\left( r(s) +
      \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s, a, s')\V(s')
    \right).
  \]
  Note that both $\log$ and $\exp$ are increasing functions, $\gamma > 0$, and
  the $\mathcal{T}$ function gives a probability (a non-negative number).
  Thus
  \[
    \begin{split}
      \V(s) &\le \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s'
          \in \mathcal{S}} \mathcal{T}(s, a, s')\frac{\rinf +
          \log|\mathcal{A}|}{1 - \gamma} \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma}\sum_{s' \in \mathcal{S}}
        \mathcal{T}(s, a, s') \right) \\
      &= \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \frac{\gamma (\rinf +
          \log|\mathcal{A}|)}{1 - \gamma} \right)
    \end{split}
  \]
  by the definition of $\mathcal{T}$. Then
  \[
    \begin{split}
      \V(s) &\le \log \left( |\mathcal{A}| \exp\left( r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log \left( \exp\left( \log|\mathcal{A}| + r(s) + \frac{\gamma
            (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \right) \right) \\
      &= \log|\mathcal{A}| + r(s) + \frac{\gamma (\rinf + \log|\mathcal{A}|)}{1 - \gamma} \\
      &= \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + r(s))}{1 - \gamma} \\
      &\le \frac{\gamma (\rinf + \log|\mathcal{A}|) + (1 -
        \gamma)(\log|\mathcal{A}| + \rinf)}{1 - \gamma} \\
      &= \vbound
    \end{split}
  \]
  by the definition of $\rinf$.

  The proof for
  \begin{equation} \label{eq:negative_bound}
    \V(s) \ge \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1}
  \end{equation}
  follows the same argument until we get to
  \[
    \begin{split}
      \V(s) &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(\log|\mathcal{A}| + r(s))}{\gamma - 1} \\
      &\ge \frac{\gamma(\rinf + \log|\mathcal{A}|) + (\gamma -
        1)(-\log|\mathcal{A}| -\rinf)}{\gamma - 1} \\
      &= \frac{\rinf + \log|\mathcal{A}|}{\gamma - 1},
    \end{split}
  \]
  where we use the fact that $r(s) \ge -\rinf - 2\log|\mathcal{A}|$. Combining
  \eqref{eq:positive_bound} and \eqref{eq:negative_bound} gives
  \eqref{eq:bound}.
\end{proof}

\begin{theorem}[The Lebesgue Dominated Convergence Theorem
  \cite{royden2010real}] \label{thm:lebesgue}
  Let $(X, \mathcal{M}, \mu)$ be a measure space and $\{ f_n \}$ a sequence of
  measurable functions on $X$ for which $\{ f_n \} \to f$ pointwise a.e. on $X$
  and the function $f$ is measurable. Assume there is a non-negative function
  $g$ that is integrable over $X$ and dominates the sequence $\{ f_n \}$ on $X$
  in the sense that
  \[
    |f_n| \le g \text{ a.e. on $X$ for all $n$.}
  \]
  Then $f$ is integrable over $X$ and
  \[
    \lim_{n \to \infty} \int_X f_n\,d\mu = \int_X f\,d\mu.
  \]
\end{theorem}

\begin{lemma} \label{lemma:bound1}
  Let $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (a, b) \subset
  \mathbb{R}$ be an arbitrary bounded function. Then, for $i = 0,
  \dots, d$,
  \[
    \left. \frac{\partial q(\mathbf{r})}{\partial \lambda_i} \right|_{\lambda_i
      = c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{r})d(\mathbf{u})$, where
  $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Remember that
  \[
    \frac{\partial q(\mathbf{r})}{\partial \lambda_i} =
    \frac{1}{2}q(\mathbf{r})\tr
    \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} - \Kuu^{-1})
      \frac{\partial \Kuu}{\partial \lambda_i}
    \right)
  \]
  by Lemma \ref{lemma:derivatives}. We begin by producing constant upper and
  lower bounds for the elements of
  \[
    \left. \frac{\partial \Kuu}{\partial \lambda_i} \right|_{\lambda_i =
      c(\mathbf{r}, \mathbf{u})}.
  \]
  If $i = 0$, then each element of $\frac{\partial
    \Kuu}{\partial \lambda_0}$ is of the form
  \[
    \exp \left( -\frac{1}{2}(\mathbf{x}_j - \mathbf{x}_k)^\intercal
      \bm\Lambda (\mathbf{x}_j - \mathbf{x}_k) - \mathbbm{1}[j \ne
      k]\sigma^2\tr(\bm\Lambda) \right),
  \]
  i.e., without $\lambda_0$, so
  \[
    \left. \frac{\partial \Kuu}{\partial \lambda_0} \right|_{\lambda_0 =
      c(\mathbf{r}, \mathbf{u})} = \frac{\partial \Kuu}{\partial \lambda_0}
  \]
  is already independent of $\mathbf{r}$ and $\mathbf{u}$---there is no need
  for any bounds.

  If $i > 0$, then each element of $\frac{\partial \Kuu}{\partial
    \lambda_i}$ is a constant multiple of $k_{\bm\lambda}(\mathbf{x}_j,
  \mathbf{x}_k)$, for some $\mathbf{x}_j$ and $\mathbf{x}_k$. Since
  $k_{\bm\lambda}(\mathbf{x}_j, \mathbf{x}_k)$ is a decreasing function of
  $\lambda_i$, and $c(\mathbf{r}, \mathbf{u}) > a$,
  \[
    \begin{split}
      k_{\bm\lambda}(\mathbf{x}_j, \mathbf{x}_k)|_{\lambda_i = 
        c(\mathbf{r}, \mathbf{u})} &=
      \begin{multlined}[t]
        \lambda_0 \exp \left( \vphantom{\sum_{n \in \{ \} \setminus}}
        \right. -\frac{1}{2}c(\mathbf{r}, \mathbf{u})(x_{j,i} - x_{k,i})^2 -
        \mathbbm{1}[j \ne k]\sigma^2c(\mathbf{r}, \mathbf{u}) \\
        - \left. \sum_{n \in \{ 1, \dots, d \} \setminus \{ i \}}
          \frac{1}{2} \lambda_n(x_{j,n} - x_{k,n})^2 + \mathbbm{1}[j \ne
          k]\sigma^2 \lambda_n \right)
      \end{multlined} \\
      &<
      \begin{multlined}[t]
        \lambda_0 \exp \left( \vphantom{\sum_{n \in \{ \} \setminus}}
        \right. -\frac{1}{2}a(x_{j,i} - x_{k,i})^2 -
        \mathbbm{1}[j \ne k]\sigma^2a \\
        - \left. \sum_{n \in \{ 1, \dots, d \} \setminus \{ i \}}
          \frac{1}{2} \lambda_n(x_{j,n} - x_{k,n})^2 + \mathbbm{1}[j \ne
          k]\sigma^2 \lambda_n \right),
      \end{multlined}
    \end{split}
  \]
  which gives an upper bound on each element of
  \[
    \left. \frac{\partial \Kuu}{\partial \lambda_i} \right|_{\lambda_i =
      c(\mathbf{r}, \mathbf{u})}.
  \]
  A similar line of reasoning establishes lower bounds as well.

  Combining the bounds with the observation that
  every element of
  $\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal}$ is in
  $\mathbb{R}_2[\mathbf{u}]$ gives the required result.
\end{proof}

\begin{remark}
  In order to find $\frac{\partial q(\mathbf{u})}{\partial t}$,
  where $t$ is the $i$th element of the vector $\bm\mu$, we can
  find $\frac{\partial q(\mathbf{u})}{\partial \bm\mu}$ and simply take the
  $i$th element. A similar line of reasoning applies to matrices as well. Thus,
  we only need to consider derivatives with respect to $\bm\mu$ and
  $\bm\Sigma$.
\end{remark}

\begin{lemma} \label{lemma:bound2}
  Let $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (a, b) \subset
  \mathbb{R}$ be an arbitrary bounded function. Then, for $i = 1, \dots, m$,
  every element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$,
  where $d(\mathbf{u}) \in \mathbb{R}_1[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma \ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})} = \frac{1}{2}q(\mathbf{u})(\bm\Sigma^{-1} +
    \bm\Sigma^{-\intercal})(\mathbf{u} - \mathbf{c}(\mathbf{r}, \mathbf{u})),
  \]
  where $\mathbf{c}(\mathbf{r}, \mathbf{u}) = (\mu_1, \dots, \mu_{i - 1},
  c(\mathbf{r}, \mathbf{u}), \mu_{i + 1} \dots, \mu_m)^\intercal$. Since
  $c(\mathbf{r}, \mathbf{u})$ is bounded and $\bm\Sigma^{-1} +
  \bm\Sigma^{-\intercal}$ is a constant matrix, we can use the bounds on
  $c(\mathbf{r}, \mathbf{u})$ to manufacture both upper and lower bounds on
  \[
     \left. \frac{\partial q(\mathbf{u})}{\partial \bm\mu} \right|_{\mu_i =
      c(\mathbf{r}, \mathbf{u})}
  \]
  of the required form.
\end{proof}

\begin{lemma} \label{lemma:bound3}
  Let $i, j = 1, \dots, m$, and let $\epsilon > 0$ be arbitrary. Furthermore,
  let
  \[
    c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to (\Sigma_{i,j} - \epsilon,
    \Sigma_{i,j} + \epsilon) \subset \mathbb{R}
  \]
  be a function with a codomain arbitrarily close to $\Sigma_{i,j}$. Then every
  element of
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} \right|_{\Sigma_{i,j} =
    c(\mathbf{r}, \mathbf{u})}
  \]
  has upper and lower bounds of the form $q(\mathbf{u})d(\mathbf{u})$, where
  $d(\mathbf{u}) \in \mathbb{R}_2[\mathbf{u}]$.
\end{lemma}
\begin{proof}
  Using Lemma \ref{lemma:derivatives},
  \[
    \left. \frac{\partial q(\mathbf{u})}{\partial \bm\Sigma} \right|_{\Sigma_{i,j} =
    c(\mathbf{r}, \mathbf{u})} =
    \frac{1}{2}q(\mathbf{u})(\mathbf{C}(\mathbf{r},
    \mathbf{u})^{-\intercal}\mathbf{UC}(\mathbf{r}, \mathbf{u})^{-\intercal} -
    \mathbf{C}(\mathbf{r}, \mathbf{u})^{-\intercal}),
  \]
  where
  \[
    [\mathbf{C}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) & \text{if } (k, l) = (i, j), \\
      \Sigma_{k,l} & \text{otherwise.}
    \end{cases}
  \]
  We can also express $\mathbf{C}(\mathbf{r},\mathbf{u})$ as
  $\mathbf{C}(\mathbf{r}, \mathbf{u}) = \bm\Sigma + \mathbf{E}(\mathbf{r},
  \mathbf{u})$, where
  \[
    [\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l} =
    \begin{cases}
      c(\mathbf{r}, \mathbf{u}) - \Sigma_{i,j} & \text{if } (k, l) = (i, j), \\
      0 & \text{otherwise.}
    \end{cases}
  \]
  We begin by establishing upper and lower bounds on $\mathbf{C}(\mathbf{r},
  \mathbf{u})^{-1}$. For this, we use the maximum norm $\lVert \cdot
  \rVert_\infty$ on both vectors and matrices. We can apply Lemma
  \ref{prop:condition_number} to $\bm\Sigma$ and $\mathbf{E}(\mathbf{r},
  \mathbf{u})$ since
  \[
    \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty = \max_k \sum_l
    |[\mathbf{E}(\mathbf{r}, \mathbf{u})]_{k,l}| = |c(\mathbf{r}, \mathbf{u}) -
    \Sigma_{i,j}| < \epsilon
  \]
  can be made arbitrarily small so that $\lVert \bm\Sigma^{-1} \rVert_\infty
  \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty < 1$. Then
  $\mathbf{C}(\mathbf{r}, \mathbf{u})$ is invertible, and
  \[
    \lVert \mathbf{C}(\mathbf{r}, \mathbf{u})^{-1} \rVert_\infty \le
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \lVert \bm\Sigma^{-1}
      \rVert_\infty \lVert \mathbf{E}(\mathbf{r}, \mathbf{u}) \rVert_\infty} <
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \lVert \bm\Sigma^{-1}
      \rVert_\infty \epsilon},
  \]
  which means that
  \[
    \max_k \sum_l \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right|
    < \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \lVert \bm\Sigma^{-1}
      \rVert_\infty \epsilon},
  \]
  i.e., for any row $k$ and column $l$,
  \[
    \left| [\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}]_{k,l} \right| <
    \frac{\lVert \bm\Sigma^{-1} \rVert_\infty}{1 - \lVert \bm\Sigma^{-1}
      \rVert_\infty \epsilon},
  \]
  which bounds all elements of $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ as
  required. Since every element of $\mathbf{U} = (\mathbf{u} - \bm\mu)(\mathbf{u} -
  \bm\mu)^\intercal$ is in $\mathbb{R}_2[\mathbf{u}]$, and the elements of
  $\mathbf{C}(\mathbf{r}, \mathbf{u})^{-1}$ are bounded, the desired result
  follows.
\end{proof}

\begin{lemma} \label{lemma:integral_of_r}
  \begin{equation} \label{eq:r-inequality}
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r})\,d\mathbf{r} \le a +
    \lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1,
  \end{equation}
  where $a$ is a constant independent of $\mathbf{u}$.
\end{lemma}
\begin{proof}
  Since $\rinf \le \lVert \mathbf{r} \rVert_1$,
  \[
    \int \lVert \mathbf{r} \rVert_\infty q(\mathbf{r})\,d\mathbf{r} \le \int
    \lVert \mathbf{r} \rVert_1 q(\mathbf{r})\,d\mathbf{r} =
    \sum_{i=1}^{|\mathcal{S}|} \mathbb{E}[|r_i|].
  \]
  As each $\mathbb{E}[|r_i|]$ is a mean of a folded Gaussian distribution,
  \[
    \mathbb{E}[|r_i|] = \sigma_i \sqrt{\frac{2}{\pi}} \exp
    \left(-\frac{\xi_i^2}{2\sigma_i^2} \right) + \xi_i \left( 1 - 2\Phi \left(
        -\frac{\xi_1}{\sigma_1} \right) \right),
  \]
  where $\xi_i = \left[\Kru^\intercal\Kuu^{-1}\mathbf{u}\right]_i$, $\sigma_i =
  \sqrt{[\Krr - \Kru^\intercal\Kuu^{-1}\Kru]_{i,i}}$\footnote{The expression
    under the square root sign is non-negative because $\Krr -
    \Kru^\intercal\Kuu^{-1}\Kru$ is a covariance matrix of a Gaussian
    distribution, hence also positive semi-definite, which means that its
    diagonal entries are non-negative.}, and $\Phi$ is the cumulative
  distribution function of the standard normal distribution. Furthermore,
  \[
    \mathbb{E}[|r_i|] \le \sigma_i\sqrt{\frac{2}{\pi}} + |\xi_i|,
  \]
  as $\sigma_i$ is non-negative, and $\Phi(x) \in [0, 1]$ for all $x$. Since
  \[ \sum_{i=1}^{|\mathcal{S}|} |\xi_i| = \lVert \Kru^\intercal \Kuu^{-1}
    \mathbf{u} \rVert_1, \]
  we can set
  \[ a = \sum_{i=1}^{|\mathcal{S}|} \sigma_i \sqrt{\frac{2}{\pi}} \]
  to get \eqref{eq:r-inequality}.
\end{proof}

Our main theorem is a specialised version of an integral differentiation result
by Chen \cite{lecture_notes}.
\begin{theorem} \label{thm:main}
  Whenever the derivative exists,
  \[
    \dt\iint
    \V(s)q(\mathbf{r})q(\mathbf{u})\dx
    = \iint
    \dt[\V(s)q(\mathbf{r})q(\mathbf{u})]\dx,
  \]
  where $t$ is any scalar part of $\bm\mu$, $\bm\Sigma$, or $\bm\lambda$.
\end{theorem}
\begin{proof}
  Let
  \begin{align*}
    \f &= \V(s)q(\mathbf{r})q(\mathbf{u}), \\
    F(t) &= \iint \f\dx,
  \end{align*}
  and fix the value of $t$. Let $(t_n)_{n=1}^\infty$ be any sequence such that
  $\lim_{n \to \infty} t_n = t$, but $t_n \ne t$ for all $n$. We want to show
  that
  \begin{equation} \label{eq:to_prove}
    F'(t) = \lim_{n \to \infty} \frac{F(t_n) - F(t)}{t_n - t} = \iint \df\dx.
  \end{equation}
  We have
  \[
    \frac{F(t_n) - F(t)}{t_n - t} = \iint \frac{\ftn - \f}{t_n - t}\dx =
    \iint \fn\dx,
  \]
  where
  \[
    \fn = \frac{\ftn - \f}{t_n - t}.
  \]
  Since
  \[
    \lim_{n \to \infty} \fn = \df,
  \]
  \eqref{eq:to_prove} follows from Theorem \ref{thm:lebesgue} as soon as we show
  that both $f$ and $f_n$ are measurable and find a non-negative integrable
  function $g$ such that for all $n$, $\mathbf{r}$, $\mathbf{u}$,
  \[
    |\fn| \le \g.
  \]
  The MDP value function is measurable by Proposition \ref{thm:measurability}.
  The result of multiplying or adding measurable functions (e.g., probability
  density functions (PDFs)) to a measurable function is still measurable. Thus,
  both $f$ and $f_n$ are measurable.

  It remains to find $g$. For notational simplicity and without loss of
  generality, we will temporarily assume that $t$ is a parameter of
  $q(\mathbf{r})$. Then
  \[
    |\fn| = |\V(s)| \left| \frac{q(\mathbf{r})|_{t =
          t_n} - q(\mathbf{r})}{t_n - t} \right| q(\mathbf{u})
  \]
  since PDFs are non-negative. An upper bound for
  $|\V(s)|$ is given by Proposition \ref{thm:bound}, while
  \[
    \frac{q(\mathbf{r})|_{t = t_n} - q(\mathbf{r})}{t_n - t} = \left.
      \frac{\partial q(\mathbf{r})}{\partial t} \right|_{t = c(\mathbf{r},
      \mathbf{u})}
  \]
  for some function $c : \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m \to
  (\min\{t, t_n\}, \max\{t, t_n\})$ due to the mean value theorem (since $q$ is
  a continuous and differentiable function of $t$, regardless of the specific
  choices of $q$ and $t$).

  We then have that
  \[
    |\fn| \le \vbound \left| \left. \frac{\partial
          q(\mathbf{r})}{\partial t} \right|_{t=c(\mathbf{r}, \mathbf{u})}
    \right| q(\mathbf{u}).
  \]
  The bound is clearly non-negative and measurable. It remains to show that it
  is also integrable. Depending on what $t$ represents, we can use one of the
  Lemmas \ref{lemma:bound1}, \ref{lemma:bound2}, and \ref{lemma:bound3}, which
  gives us two polynomials $p_1(\mathbf{u}), p_2(\mathbf{u}) \in
  \mathbb{R}_2[\mathbf{u}]$ such that
  \[
    p_1(\mathbf{u})q(\mathbf{r}) < \left. \frac{\partial q(\mathbf{r})}{\partial
        t} \right|_{t=c(\mathbf{r}, \mathbf{u})} < p_2(\mathbf{u})q(\mathbf{r}).
  \]
  Then
  \[
    \left| \left. \frac{\partial q(\mathbf{r})}{\partial t}
      \right|_{t=c(\mathbf{r}, \mathbf{u})} \right| < q(\mathbf{r}) \max \{
    |p_1(\mathbf{u})|, |p_2(\mathbf{u})| \}.
  \]
  We can now apply Lemma \ref{lemma:integral_of_r}, which allows us to integrate
  out $\mathbf{r}$, and we are left with showing the existence of
  \begin{equation} \label{eq:remaining_integral}
    \int \left( a + \lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1 \right) \max \{|p_1(\mathbf{u})|, |p_2(\mathbf{u})| \} q(\mathbf{u})\,d\mathbf{u},
  \end{equation}
  where $a$ is a constant. The integral
  \[
    \int \max \{|p_1(\mathbf{u})|, |p_2(\mathbf{u})| \}
    q(\mathbf{u})\,d\mathbf{u} = \int \max \{|p_1(\mathbf{u})q(\mathbf{u})|,
    |p_2(\mathbf{u})q(\mathbf{u})| \}\,d\mathbf{u}
  \]
  exists because $p_1(\mathbf{u})q(\mathbf{u})$ and
  $p_2(\mathbf{u})q(\mathbf{u})$ are both integrable, hence their absolute
  values are integrable, and the maximum of two integrable functions is also
  integrable. Since $\lVert \Kru^\intercal \Kuu^{-1} \mathbf{u} \rVert_1 \in
  \mathbb{R}_1[\mathbf{u}]$, a similar argument can be applied to the rest of
  \eqref{eq:remaining_integral} as well.
\end{proof}

\section{Evidence Lower Bound}

\begin{equation} \label{eq:elbo}
  \begin{split}
    \mathcal{L} &= \Eq \left[ \log \frac{\pfull}{\approximation}
    \right] \\
    &= \iint \approximation \log
    \frac{\pfull}{\approximation}\dx.
  \end{split}
\end{equation}

\begin{equation} \label{eq:full}
  \pfull = p(\mathbf{X_u}) \times p(\mathbf{u} | \mathbf{X_u}) \times p(\mathbf{r} | \mathbf{X_u}, \mathbf{u}) \times p(\mathcal{D} | \mathbf{r}).
\end{equation}

\begin{equation} \label{eq:approximation}
  \approximation = q(\mathbf{u}) \times q(\mathbf{r} | \mathbf{u}).
\end{equation}

In this section we derive and simplify the ELBO for this (now fully specified)
model. In order to derive the ELBO, let us go back to \eqref{eq:elbo} and
write\footnote{At this point, we will drop the subscript denoting which
  variables the expectation is taken over. Also note that throughout the
  derivation equality is taken to mean `equality up to an additive constant'.}
\[
  \mathcal{L} = \mathbb{E}[\log\pfull] - \mathbb{E}[\log\approximation].
\]
By substituting in \eqref{eq:full} and \eqref{eq:approximation}, we get
\[
  \mathcal{L} = \mathbb{E}[\log p(\mathbf{X_u}) + \log p(\mathbf{u} |
  \mathbf{X_u}) + \log p(\mathbf{r} | \mathbf{X_u}, \mathbf{u}) + \log
  p(\mathcal{D} | \mathbf{r})] - \mathbb{E}[\log q(\mathbf{u}) + \log
  q(\mathbf{r} | \mathbf{u})].
\]
Note that $\mathbb{E}[\log p(\mathbf{X_u})]$ is just a constant, so we can
simply drop it from the expression. Furthermore, since $q(\mathbf{r} |
\mathbf{u}) = p(\mathbf{r} | \mathbf{X_u}, \mathbf{u})$, they cancel each other
out. Then, we can substitute various terms with their definitions to get
\[
  \mathcal{L} = \mathbb{E}[\log \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu)] +
  \mathbb{E}\left[ \sum_{i=1}^N \sum_{t=1}^T Q_{\mathbf{r}}(s_{i,t}, a_{i,t}) -
    \V(s_{i,t}) \right] - \mathbb{E}[\log\mathcal{N}(\mathbf{u}; \bm\mu,
  \bm\Sigma)].
\]
Using the expressions for $Q_{\mathbf{r}}$ and the entropy of a normal
distribution \cite{DBLP:journals/tit/AhmedG89},
\[
  \mathcal{L} = \frac{1}{2}\log|\bm\Sigma| - \frac{1}{2}\log|\Kuu| +
  \mathbb{E}\left[\sum_{i=1}^N \sum_{t=1}^T r(s_{i,t}) - \V(s_{i,t}) +
    \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\V(s')
  \right].
\]
We can simplify $\sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t})$ by defining a new vector
$\mathbf{t} = (t_1, \dots, t_{|\mathcal{S}|})^\intercal$, where $t_i$ is the
number of times the state associated with the reward $r_i$ has been visited
across all demonstrations. Then
\[
  \mathbb{E} \left[ \sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t}) \right] =
  \mathbb{E}[\mathbf{t}^\intercal\mathbf{r}] =
  \mathbf{t}^\intercal\mathbb{E}[\mathbf{r}] =
  \mathbf{t}^\intercal\mathbb{E}\left[\Kru^\intercal\Kuu^{-1}\mathbf{u}\right] =
  \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu.
\]
This allows us to simplify $\mathcal{L}$ to
\[
  \mathcal{L} = \frac{1}{2}\log|\bm\Sigma| - \frac{1}{2}\log|\Kuu| +
  \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu - \mathbb{E}[v],
\]
where
\[
v = \sum_{i=1}^N \sum_{t=1}^T \V(s_{i,t}) - \gamma\sum_{s' \in
  \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\V(s').
\]

\subsection{$\partial/\partial\bm\mu$}

We begin by removing terms independent of $\bm\mu$:
\[
  \frac{\partial\mathcal{L}}{\partial\bm\mu} =
  \dm[\mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1}\bm\mu] - \dm\mathbb{E}[v].
\]
Here
\[
  \dm\mathbb{E}[\V(s)] = \dm\iint \V(s) q(\mathbf{r})
  q(\mathbf{u})\dx = \iint \V(s) q(\mathbf{r}) \frac{\partial
    q(\mathbf{u})}{\partial \bm\mu}\dx = \frac{1}{2}\mathbb{E}[\V(s)
  (\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu)]
\]
by Theorem \ref{thm:main} and Lemma \ref{lemma:derivatives}.
Hence,
\[
  \frac{\partial\mathcal{L}}{\partial\bm\mu} =
  \mathbf{t}^\intercal\Kru^\intercal\Kuu^{-1} - \frac{1}{2}\mathbb{E} \left[
    (\bm\Sigma^{-1} + \bm\Sigma^{-\intercal})(\mathbf{u} - \bm\mu) v \right].
\]

\subsection{$\partial/\partial\mathbf{B}$, $\partial/\partial\mathbf{D}$}

For $\mathbf{S} \in \{ \mathbf{B}, \mathbf{D} \}$,
\[
  \frac{\partial\mathcal{L}}{\partial\mathbf{S}} = \frac{1}{2}\dS\log|\bm\Sigma|
  - \dS\mathbb{E}[v].
\]
By Theorem \ref{thm:main},
\[
  \dS\mathbb{E}[\V(s)] = \iint \V(s) q(\mathbf{r})
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{S}}\dx.
\]
Then, using the aforementioned tool by Laue et al.
\cite{DBLP:conf/nips/LaueMG18}, we get
\begin{gather*}
  \dB\log|\bm\Sigma| = (\mathbf{T} + \bm\Sigma^{-1})\mathbf{B}, \\
  \dD\log|\bm\Sigma| = \mathbf{T}\mathbf{D}^\intercal +
  \mathbf{D}^\intercal\mathbf{T},
\end{gather*}
and Lemma \ref{lemma:derivatives} gives
\begin{gather*}
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{B}} = \frac{1}{2}q(\mathbf{u})
  \{ |\bm\Sigma|^{-1}(\adj(\mathbf{T}) + \adj(\bm\Sigma)) + \mathbf{TUT} +
  \bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} \} \mathbf{B}, \\
  \frac{\partial q(\mathbf{u})}{\partial \mathbf{D}} = \frac{1}{2}q(\mathbf{u})
  \{ \mathbf{T}\mathbf{UT}\mathbf{D}^\intercal +
  \mathbf{D}^\intercal\mathbf{T}\mathbf{UT} -
  |\bm\Sigma|^{-1}(\adj(\mathbf{T})\mathbf{D}^\intercal + \mathbf{D}^\intercal
  \adj(\mathbf{T})) \}.
\end{gather*}
Therefore,
\begin{gather*}
  \frac{\partial \mathcal{L}}{\partial \mathbf{B}} = \frac{1}{2}(\mathbf{T} +
  \bm\Sigma^{-1})\mathbf{B} - \frac{1}{2} \mathbb{E} [\{
  |\bm\Sigma|^{-1}(\adj(\mathbf{T}) + \adj(\bm\Sigma)) + \mathbf{TUT} +
  \bm\Sigma^{-1}\mathbf{U}\bm\Sigma^{-1} \} \mathbf{B}v], \\
  \frac{\partial \mathcal{L}}{\partial \mathbf{D}} =
  \frac{1}{2}(\mathbf{TD}^\intercal + \mathbf{D}^\intercal\mathbf{T}) -
  \frac{1}{2}\mathbb{E}[\{ \mathbf{T}\mathbf{UT}\mathbf{D}^\intercal +
  \mathbf{D}^\intercal\mathbf{T}\mathbf{UT} -
  |\bm\Sigma|^{-1}(\adj(\mathbf{T})\mathbf{D}^\intercal + \mathbf{D}^\intercal
  \adj(\mathbf{T})) \}v].
\end{gather*}

\subsection{$\partial/\partial \lambda_j$}

For $j = 0, \dots, d$,
\[
  \frac{\partial \mathcal{L}}{\partial \lambda_j} = - \frac{1}{2}\dlj\log|\Kuu|
  + \mathbf{t}^\intercal\dlj \left[ \Kru^\intercal\Kuu^{-1} \right] \bm\mu -
  \dlj\mathbb{E}[v],
\]
where
\begin{align*}
  \dlj\log|\Kuu| &= \tr \left( \Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j}
  \right), \\
  \dlj \left[ \Kru^\intercal\Kuu^{-1} \right] &= \frac{\partial
    \Kru^\intercal}{\partial \lambda_j} \Kuu^{-1} + \Kru^\intercal
  \frac{\partial \Kuu^{-1}}{\partial \lambda_j} = \frac{\partial
    \Kru^\intercal}{\partial \lambda_j} \Kuu^{-1} -
  \Kru^\intercal\Kuu^{-1}\frac{\partial \Kuu}{\partial \lambda_j}\Kuu^{-1}
\end{align*}
by Petersen and Pedersen \cite{petersen2008matrix}, and
\[
  \dlj \mathbb{E}[\V(s)] = \iint\V(s)\frac{\partial q(\mathbf{r})}{\partial
    \lambda_j}q(\mathbf{u})\dx = \frac{1}{2}\mathbb{E} \left[ \V(s) \tr
    \left((\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} - \Kuu^{-1})
      \frac{\partial \Kuu}{\partial \lambda_j} \right) \right]
\]
by Theorem \ref{thm:main} and Lemma \ref{lemma:derivatives}. Thus,
\[
  \begin{split}
    \frac{\partial \mathcal{L}}{\partial \lambda_j} = &- \frac{1}{2} \tr
    \left(\Kuu^{-1} \frac{\partial \Kuu}{\partial \lambda_j} \right) +
    \mathbf{t}^\intercal \left( \frac{\partial \Kru^\intercal}{\partial
        \lambda_j} - \Kru^\intercal\Kuu^{-1}\frac{\partial
        \Kuu}{\partial \lambda_j} \right) \Kuu^{-1} \bm\mu \\
    &- \frac{1}{2} \mathbb{E} \left[ \tr \left(
        (\Kuu^{-1}\mathbf{u}\mathbf{u}^\intercal\Kuu^{-\intercal} - \Kuu^{-1})
        \frac{\partial \Kuu}{\partial \lambda_j} \right) v \right],
  \end{split}
\]
where the remaining derivatives can be found in Lemma \ref{lemma:derivatives}.
% TODO: define adj() (adjugate/adjoint)

\bibliographystyle{abbrv}
\bibliography{paper.bib}
\end{document}