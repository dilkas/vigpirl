\documentclass{mprop}
\usepackage{graphicx}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{url}
\usepackage{bm}
\usepackage{bbm}
\usepackage{booktabs}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}

\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\diag}{diag}

\begin{document}
\title{Variational Inference for Inverse Reinforcement Learning with Gaussian
  Processes}
\author{Paulius Dilkas}
\date{Date of submission placed here} % TODO
\maketitle
\tableofcontents
\newpage

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}
\newcommand{\DKL}{D_{\mathrm{KL}}}
\newcommand{\pfull}{p(\mathcal{D}, \Theta, \mathbf{X_u}, \mathbf{u}, \mathbf{r})}
\newcommand{\pfullS}{p(\mathcal{D}, \Theta_s, \mathbf{X_u}, \mathbf{u}_s, \mathbf{r}_s)}
\newcommand{\posterior}{p(\Theta, \mathbf{u}, \mathbf{r} | \mathcal{D}, \mathbf{X_u})}
\newcommand{\approximation}{q_{\bm\nu}(\Theta, \mathbf{u}, \mathbf{r})}
\newcommand{\approximationS}{q_{\bm\nu}(\Theta_s, \mathbf{u}_s, \mathbf{r}_s)}
\newcommand{\Eq}{\mathbb{E}_{(\Theta, \mathbf{u}, \mathbf{r}) \sim \approximation}}

\section{Introduction}
%briefly explain the context of the project problem

Inverse reinforcement learning (IRL)---a problem proposed by Russell in 1998
\cite{DBLP:conf/colt/Russell98}---asks us to find a reward function for a Markov
decision process that best explains a set of given demonstrations. IRL is
important because reward functions can be hard to define manually
\cite{DBLP:conf/icml/PieterN04,DBLP:journals/corr/abs-1806-06877}, and rewards
are not entirely specific to a given environment, allowing one to reuse the same
reward structure in previously unseen environments
\cite{DBLP:journals/corr/abs-1806-06877,DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}.
Moreover, IRL has seen a wide array of applications in autonomous vehicle
control
\cite{DBLP:journals/ijsr/KimP16,DBLP:journals/ijrr/KretzschmarSSB16}
and learning to predict another agent's behaviour
\cite{DBLP:journals/ai/BogertD18,DBLP:conf/aaai/VogelRGR12,ziebart2008maximum,DBLP:conf/huc/ZiebartMDB08,DBLP:conf/iros/ZiebartRGMPBHDS09}.
Most approaches in the literature (see Section \ref{literature}) make a
convenient yet unjustified assumption that the reward function can be expressed
as a linear combination of features. One proven way to abandon this assumption
is by representing the reward function as a Gaussian process
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11,DBLP:journals/corr/abs-1208-2112}.

\section{Statement of the Problem}
%clearly state the problem to be addressed in your forthcoming project. Explain
%why it would be worthwhile to solve this problem.

\begin{definition}
  A \emph{Markov decision process} (MDP) is a set $\mathcal{M} = \{ \mathcal{S},
  \mathcal{A}, \mathcal{T}, \gamma, r \}$, where $\mathcal{S}$ and
  $\mathcal{A}$ are sets of states and actions, respectively; $\mathcal{T} :
  \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is a function
  defined so that $\mathcal{T}(s, a, s')$ is the probability of moving to state $s'$
  after taking action $a$ in state $s$; $\gamma \in [0, 1)$ is the discount
  factor (with higher $\gamma$ values, it makes little difference whether a
  reward is received now or later, while with lower $\gamma$ values the future
  becomes gradually less and less important); and $r : \mathcal{S} \to
  \mathbb{R}$ is the reward function.
\end{definition}

In \emph{inverse reinforcement learning}, one is presented with an MDP without a
reward function $\mathcal{M} \setminus \{ r \}$ and a set of expert
demonstrations $\mathcal{D} = \{ \zeta_i \}_{i=1}^N$, where each demonstration
$\zeta_i = \{ (s_{i,0}, a_{i,0}), \dots, (s_{i,T}, a_{i,T}) \}$ is a multiset of
state-action pairs representing the actions taken by the expert during a
particular recorded session. Each state is also characterised by a number of
features. The goal of IRL is then to find $r$ such that the optimal policy under
$r$
\[ \pi^* = \argmax_\pi \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t r(s_t) | \pi
  \right] \]
matches the actions in $\mathcal{D}$.

The likelihood of the data can be written down as
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}
\begin{equation} \label{pDr}
  p(\mathcal{D} | r) = \prod_{i=1}^N \prod_{t=1}^T p(a_{i,t} | s_{i,t}) = \exp\left( \sum_{i=1}^N \sum_{t=1}^T Q_r(s_{i,t}, a_{i,t}) - V_r(s_{i,t}) \right),
\end{equation}
where
\begin{equation}
  Q_r(s_{i,t}, a_{i,t}) = r(s_{i,t}) + \gamma\sum_{s' \in \mathcal{S}}
  \mathcal{T}(s_{i,t}, a_{i,t}, s')V_r(s'),
\end{equation}
and $V_r(s)$ can be obtained by repeatedly applying the equation
\cite{supplementary_material}
\[ V_r(s) = \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s' \in
      \mathcal{S}} \mathcal{T}(s, a, s')V_r(s') \right). \]
However, a reward function learned by maximising this likelihood is not
transferable to new situations
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}. One needs to model the
reward structure in a way that would allow reward predictions for previously
unseen states.

One way to model rewards without assumptions of linearity is with a
\emph{Gaussian process} (GP). A GP is a collection of random variables, any
finite combination of which has a joint Gaussian distribution
\cite{DBLP:books/lib/RasmussenW06}. We write $r \sim \mathcal{GP}(0,
k_{\Theta})$ to say that $r$ is a GP with mean $0$ and covariance function
$k_{\Theta}$, which uses a set of hyperparameters $\Theta$. Covariance functions
take two state feature vectors as input and quantify how similar the two states
are, in a sense that we would expect them to have similar rewards.

% TODO: positive definite
As training a GP with $n$ data points has a time complexity of
$\mathcal{O}(n^3)$ \cite{DBLP:books/lib/RasmussenW06}, numerous approximation
methods have been suggested, many of which select a subset of data called
\emph{inducing points} and focus most of the training effort on them
\cite{DBLP:journals/corr/abs-1807-01065}. Let $\mathbf{X_u}$ be the matrix of
features at inducing states, $\mathbf{u}$ the rewards at those states, and
$\mathbf{r}$ a vector with $r(\mathcal{S})$ as elements. Then the full joint
probability distribution can be factorised as
\begin{equation} \label{full}
  \pfull = p(\mathbf{X_u}) \times p(\Theta | \mathbf{X_u}) \times p(\mathbf{u}
  | \Theta, \mathbf{X_u}) \times p(\mathbf{r} | \Theta, \mathbf{X_u},
  \mathbf{u}) \times p(\mathcal{D} | r).
\end{equation}
Here $p(\mathbf{X_u})$ and $p(\Theta | \mathbf{X_u})$ are freely chosen priors,
\begin{align}
  p(\mathbf{u} | \Theta, \mathbf{X_u}) &= \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu) \nonumber \\
                                       &= \frac{1}{(2\pi)^{m/2}|\Kuu|^{1/2}}\exp \left( -\frac{1}{2} \mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} \right) \nonumber \\
                                       &= \exp\left(-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} - \frac{1}{2}\log|\Kuu| - \frac{m}{2}\log 2\pi\right) \label{eq:normal}
\end{align}
is the GP prior \cite{DBLP:books/lib/RasmussenW06}, where $m \in \mathbb{N}$ is
the number of inducing points. The GP posterior is a
multivariate Gaussian \cite{DBLP:conf/nips/LevinePK11}
\begin{equation} \label{eq:r}
  p(\mathbf{r} | \Theta, \mathbf{X_u}, \mathbf{u}) =
  \mathcal{N}(\mathbf{r}; \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr - \Kru^\intercal\Kuu^{-1}\Kru),
\end{equation}
and $p(\mathcal{D} | r)$ is as in \eqref{pDr}. The matrices such as
$\Kru$ are called \emph{covariance matrices} and are defined as
$[\Kru]_{i,j} = k_\Theta(\mathbf{x}_{\mathbf{r},i}, \mathbf{x}_{\mathbf{u},j})$,
where $\mathbf{x}_{\mathbf{r},i}$ and $\mathbf{x}_{\mathbf{u},j}$ denote feature
vectors for the $i$th state in $\mathcal{S}$ and the $j$th state in
$\mathbf{X_u}$, respectively \cite{DBLP:conf/uai/JinDAS17}.

Given this model, data $\mathcal{D}$, and inducing feature matrix
$\mathbf{X_u}$, our goal is then to find optimal values of hyperparameters
$\Theta$, inducing rewards $\mathbf{u}$, and the reward function $r$. While the
previous paper that considered this IRL model computed maximum likelihood estimates
for $\Theta$ and $\mathbf{u}$, and made an assumption that $\mathbf{r}$ in
\eqref{eq:r} has zero variance \cite{DBLP:conf/nips/LevinePK11}, we aim to
avoid this assumption and use variational inference to approximate the full
posterior distribution $\posterior$. \emph{Variational inference} (VI) is an
approximation technique for probability densities \cite{blei2017variational}.
Let $\approximation$ be our approximating family of probability distributions
for $\posterior$ with its own hyperparameter vector $\bm\nu$. Then it is up
to VI algorithms to optimise $\bm\nu$ in order to minimise the
\emph{Kullback-Leibler} (KL) divergence between the original probability
distribution and our approximation.  KL divergence (asymmetrically) measures how
different the two distributions are and in this case can be defined as follows
\cite{blei2017variational}:
\[ \DKL(\approximation || \posterior) = \Eq \left[ \log
  \frac{\approximation}{\posterior} \right]. \]
Since KL divergence is typically hard to compute, instead of minimising it, VI
typically tries to maximise the \emph{evidence lower bound} (ELBO) defined as
\cite{DBLP:books/lib/Bishop07,blei2017variational}
\begin{equation} \label{eq:elbo}
  \begin{split}
    \mathcal{L}(\bm\nu) &= \Eq \left[ \log \frac{\pfull}{\approximation}
    \right] \\
    &= \iiint \approximation \log
    \frac{\pfull}{\approximation}\,d\Theta\,d\mathbf{u}\,d\mathbf{r}.
  \end{split}
\end{equation}

By considering full probability distributions instead of point estimates,---as
long as the approximations are able to capture important features of the
posterior---our predictions are likely to be more accurate and rely on fewer
assumptions. Moreover, we hope to make use of various recent advancements in VI
for both time complexity and approximation distribution fit (see Section
\ref{literature}), making the resulting algorithm competitive both in terms of
speed and model fit.

\section{Literature Survey} \label{literature}
%present an overview of relevant previous work including articles, books, and
%existing software products. Critically evaluate the strengths and weaknesses of
%the previous work.

Variational inference has seen a recent increase in interest among academics,
with different approaches focusing on different goals: better time complexity,
handling a wider variety of models, making approximations more accurate, and
using more complex function approximation techniques (such as neural networks)
to infer local latent variable values without having to calculate them
individually for each data point \cite{DBLP:journals/corr/abs-1711-05597}. As
our IRL model is based on a GP, we will begin by reviewing some of the VI
approaches applied specifically to GP regression. Based on a recent review of
scalable GPs \cite{DBLP:journals/corr/abs-1711-05597}, we will concentrate on
\emph{stochastic variational sparse approximations}, as they have achieved
modelling accuracy close to that of the full GP with no approximations, while
providing a time complexity of $\mathcal{O}(m^3)$. Below we provide a short
overview of various assumptions that have been used in approximating
\emph{sparse} GPs (i.e., GPs that use inducing points), following on a paper by
Qui\~{n}onero-Candela and Rasmussen \cite{DBLP:journals/jmlr/CandelaR05}.

\begin{description}
  \item [Subset of data ($\mathcal{O}(m^3)$)] is a baseline method of simply
    using a subset of data points.
  \item [Subset of regressors ($\mathcal{O}(nm^2)$)
    \cite{silverman1985some,DBLP:conf/nips/SmolaB00,DBLP:conf/nips/WahbaLGXKK98}]
    is a degenerate approximation that uses a weight for each inducing point. A
    GP is called \emph{degenerate} if the covariance function has a finite
    number of non-zero eigenvalues, restricting the prior distribution to only a
    finite number of linearly independent functions
    \cite{DBLP:journals/jmlr/CandelaR05}.
  \item [Deterministic training conditional ($\mathcal{O}(nm^2)$)
    \cite{DBLP:conf/aistats/SeegerWL03}] approximation imposes a zero-variance
    normal distribution for $\mathbf{r} | \mathbf{u}$, resulting in the same mean
    but different variance predictions compared to the subset of regressors.
  \item [Fully independent training conditional ($\mathcal{O}(nm^2)$)
    \cite{DBLP:conf/nips/SnelsonG05}] has the assumption that the GP values are
    independent of each other when conditioned on the inducing values:
    \[ q(\mathbf{r} | \mathbf{u}) = \prod_{i=1}^n p(r_i | \mathbf{u}). \]
  \item [Partially independent training conditional ($\mathcal{O}(nm^2)$)
    \cite{DBLP:journals/neco/Tresp00,DBLP:conf/nips/SchwaighoferT02}]
    approximates the same distribution as the fully independent training
    conditional, but considers a block diagonal rather than a diagonal
    covariance matrix.
  \item [Transduction] tailors the predictive distribution to
    specific test inputs \cite{DBLP:journals/jmlr/CandelaR05}. As we are not too
    concerned about a specific set of test inputs in the IRL setting,
    transduction is of limited interest to us.
  \item [Augmentation \cite{rasmussen2002reduced}] aims to improve predictive
    accuracy by adding each test input to the inducing points.
  \item [Nystr\"{o}m approximation ($\mathcal{O}(nm^2)$)
    \cite{DBLP:conf/nips/WilliamsS00}] approximates the prior covariance of
    $\mathbf{r}$, but can lead to negative predictive variances.
  \item [Relevance vector machine ($\mathcal{O}(m^3)$)
    \cite{DBLP:journals/jmlr/Tipping01}] is a degenerative approximation
    supporting a limited range of covariance functions.
\end{description}

\begin{table}
  \centering % TODO: recheck
  \begin{tabular}{llllll}
    \toprule
    Authors, year & Inducing points & Hyperparameters & Complexity \\
    \midrule
    Titsias, 2009 \cite{DBLP:journals/jmlr/Titsias09} & variational & variational & $\mathcal{O}(nm^2)$ \\
    Hensman et al., 2013 \cite{DBLP:conf/uai/HensmanFL13} & fixed & variational & $\mathcal{O}(m^3)$ \\
    Gal et al., 2014 \cite{DBLP:conf/nips/GalWR14} & variational & variational & $\mathcal{O}(nm^2)$ \\
    Hoang et al., 2015 \cite{DBLP:conf/icml/HoangHL15} & fixed & fixed & $\mathcal{O}(m^3)$ \\
    Cheng and Boots, 2017 \cite{DBLP:conf/nips/ChengB17} & variational & variational & $\mathcal{O}(nm_\alpha + nm_\beta^2)$ \\
    Hensman et al., 2017 \cite{DBLP:journals/jmlr/HensmanDS17} & fixed & variational & $\mathcal{O}(nm)$ \\
    Peng et al., 2017 \cite{DBLP:conf/icml/PengZZQ17} & variational & variational & $\mathcal{O}(m^3)$ \\
    \bottomrule
  \end{tabular}
  \caption{Summary of relevant VI approximations to GPs. For both inducing
    points and hyperparameters, 'fixed' means 'chosen before the algorithm
    starts' and 'variational' means 'included amongst the variational
    parameters'. Hyperparameters $m_\alpha$ and $m_\beta$ refer to the number of
    bases used to represent the GP's mean and covariance, respectively.}
  \label{table:approximations}
\end{table}

% TODO: expand on this
Table \ref{table:approximations} further summarises some of the recent and/or
influential GP approximation approaches that might be suitable for our GP IRL
model. In order to derive a reliable ELBO, the inducing points should be either
fixed or modelled by a probability distribution (none of the approaches do
that), while hyperparameters cannot be variational (the reason will be explained
in Section \ref{sec:proposed_approach}).

\begin{itemize}
\item We would also like to avoid having the hyperparameters fixed, as learning
  them efficiently would introduce a separate problem. As the approach by Hoang
  et al. does not seem to be easily extendable to modelled hyperparameters, it
  is unsuitable to our needs.
\item (TODO: this will have comments about other papers as well and will be
  restructured into a paragraph) The variational Fourier features (VFF)
  algorithm by Hensman et al. \cite{DBLP:journals/jmlr/HensmanDS17} is only
  defined for Mat\'ern kernels, which is likely to be too restrictive for our
  situation (the original paper on using GPs for IRL
  \cite{DBLP:conf/nips/LevinePK11} used the automatic relevance detection kernel
  that has weights controlling how important each feature is). While extending
  VFF to support a flexible class of kernels defined by Wilson and Adams
  \cite{pmlr-v28-wilson13} is an interesting and promising avenue of work, it is
  likely to be beyond the scope of this project.
\item The derivation of the ELBO in the work of Peng et al.
  \cite{DBLP:conf/icml/PengZZQ17} relies primarily on the fact that the evidence
  for a GP is a Gaussian, while in our case the evidence can be defined in
  several ways depending on the model, but must always involve $\mathcal{D}$,
  making the main idea of the paper inapplicable to IRL.
\end{itemize}

Since we expect $\posterior$ to be highly irregular, we would like our
approximation to be capable of representing a wide range of possible probability
distributions. The primary way to represent complex posteriors in VI is by using
\emph{normalising flows}, i.e., a collection of invertible
functions---parametrised by additional variational parameters---that are applied
to latent variables \cite{DBLP:conf/icml/RezendeM15}. Unfortunately, this
parametrisation also means that the gradient of the joint probability
distribution w.r.t. variational parameters $\nabla_{\bm\nu}\pfull$ is no
longer zero, making an analytic expression for ELBO impossible using the usual
methods. While it may be possible to have an approximating distribution for the
flow parameters, it is uncertain how such an algorithm would behave, as it
would have to perform optimisation in a space with significantly more dimensions.

\section{Proposed Approach} \label{sec:proposed_approach}

%state how you propose to solve the software development problem. Show that your
%proposed approach is feasible, but identify any risks.

In order to properly investigate the difference between variational inference
and maximum likelihood estimation for the model, we keep other parts of the
model the same. Namely, we set the covariance function to a version of the
automatic relevance detection kernel
\cite{DBLP:conf/nips/LevinePK11,neal2012bayesian}
\[ k_\Theta(\mathbf{x}_i, \mathbf{x}_j) = \zeta\exp\left(
    -\frac{1}{2}(\mathbf{x}_i - \mathbf{x}_j)^\intercal\bm\Lambda(\mathbf{x}_i -
    \mathbf{x}_j) - \mathbbm{1}[i \ne j]\sigma^2\Tr[\bm\Lambda] \right), \]
where $\zeta$ is the overall ``scale'' factor for how similar or distant the
states are,
\[ \bm\Lambda = \diag(\lambda_1, \dots, \lambda_d) = \diag(\bm\lambda) \]
is a diagonal matrix that determines how relevant each feature is (where $d$
denotes the number of features), $\mathbbm{1}$ is defined as
\[ \mathbbm{1}[b] = \begin{cases}
    1 & \mbox{if $b$ is true} \\
    0 & \mbox{otherwise},
  \end{cases} \]
and $\sigma^2$ is set to $10^{-2}/2$ (as the original paper noted that
the value makes little difference to the performance of the algorithm
\cite{DBLP:conf/nips/LevinePK11}). Thus we can set $\Theta = \{ \zeta,
\bm\lambda \}$. Similarly, we keep the expression for the prior of $\Theta$:
\begin{equation} \label{eq:theta_prior}
  p(\Theta | \mathbf{X_u}) = \exp\left( -\frac{1}{2}\Tr[\Kuu^{-2}] -
    \sum_{i=1}^d \log(\lambda_i + 1) \right).
\end{equation}

Next, we can rewrite the posterior by using the chain rule and Bayes' theorem in
order to get a better sense of what we are trying to approximate:
\[
  \begin{split}
    \posterior &= p(\Theta | \mathbf{X_u}, \mathcal{D}) \times p(\mathbf{u} |
    \Theta, \mathbf{X_u}, \mathcal{D}) \times p(\mathbf{r} | \Theta,
    \mathbf{X_u}, \mathbf{u}, \mathcal{D}) \\
    &\propto p(\Theta | \mathbf{X_u}, \mathcal{D}) \times p(\mathbf{u} | \Theta,
    \mathbf{X_u}, \mathcal{D}) \times p(\mathcal{D} | r) \times p(\mathbf{r} |
    \Theta, \mathbf{X_u}, \mathbf{u}) \\
    &\propto p(\Theta | \mathbf{X_u}, \mathcal{D}) \times p(\mathcal{D} |
    \Theta, \mathbf{X_u}, \mathbf{u}) \times p(\mathbf{u} | \Theta,
    \mathbf{X_u}) \times p(\mathcal{D} | r) \times p(\mathbf{r} | \Theta,
    \mathbf{X_u}, \mathbf{u}) \\
    &\propto p(\mathcal{D} | \Theta, \mathbf{X_u}) \times p(\Theta |
    \mathbf{X_u}) \times p(\mathcal{D} | \Theta, \mathbf{X_u}, \mathbf{u})
    \times p(\mathbf{u} | \Theta, \mathbf{X_u}) \times p(\mathcal{D} | r) \times
    p(\mathbf{r} | \Theta, \mathbf{X_u}, \mathbf{u})
  \end{split}
\]
Note that now there are only two unknown probability distributions:
$p(\mathcal{D} | \Theta, \mathbf{X_u})$ and $p(\mathcal{D} | \Theta,
\mathbf{X_u}, \mathbf{u})$, which can be computed as follows:
\begin{align*}
  p(\mathcal{D} | \Theta, \mathbf{X_u}, \mathbf{u}) = \int &p(\mathcal{D} | r) \times p(\mathbf{r} | \Theta, \mathbf{X_u}, \mathbf{u}) \, d\mathbf{r}, \\
  p(\mathcal{D} | \Theta, \mathbf{X_u}) = \iint &p(\mathcal{D} | r) \times p(\mathbf{r} | \Theta, \mathbf{X_u}, \mathbf{u}) \times p(\mathbf{u} | \Theta, \mathbf{X_u}) \, d\mathbf{u} \, d\mathbf{r}.
\end{align*}
This suggests the following form for the approximation:
\begin{equation} \label{eq:approximation}
  \approximation = q(\Theta) \times q(\mathbf{u} | \Theta) \times q(\mathbf{r}
  | \Theta, \mathbf{u}).
\end{equation}

%where $q(\Theta)$ can be of the same form as $p(\Theta | \mathbf{X_u})$,
%$q(\mathbf{u} | \Theta) = \mathcal{N}(\mathbf{0}, \bm\Sigma_{\mathbf{u}})$, and
%$q(\mathbf{r} | \Theta, \mathbf{u}) = \mathcal{N}(\bm\mu_{\mathbf{r}},
%\bm\Sigma_{\mathbf{r}})$.

\subsection{The Structure of the Approximating Distribution}

At this point we are forced to make assumptions about the approximate posterior
in order to arrive at an implementable solution. Ways to relax the assumptions
may be investigated towards the end of the project, if time permits.

First, as is common in the literature for applying VI to GPs
\cite{DBLP:conf/nips/ChengB17,DBLP:conf/uai/HensmanFL13,DBLP:conf/icml/HoangHL15,DBLP:journals/jmlr/Titsias09},
we simply set
\begin{equation}
  q(\mathbf{r} | \Theta, \mathbf{u}) = p(\mathbf{r} | \Theta, \mathbf{X_u},
  \mathbf{u}).
\end{equation}
We can make a similarly justified choice for $q(\mathbf{u} | \Theta)$:
\begin{equation}
  q(\mathbf{u} | \Theta) = q(\mathbf{u}) = \mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})
\end{equation}
for some variational parameters $\mathbf{m} \in \mathbb{R}^{m}$ and $\mathbf{S}
\in \mathbb{R}^{m \times m}$
\cite{DBLP:conf/nips/ChengB17,DBLP:journals/jmlr/HensmanDS17,DBLP:conf/aaai/HoangHL17}.

In order to have a reasonable way of calculating/approximating the ELBO (see
Section \ref{vi_algs}), we need to have an approximating distribution for
$\Theta$, but unfortunately all the papers in Table \ref{table:approximations}
either fix it or treat it as a variational parameter. Hence we make our first
assumption without justification from previous literature:
\begin{equation} \label{eq:theta}
  q(\Theta) = q(\zeta) \times q(\bm\lambda).
\end{equation}
As it is reasonable for $\bm\lambda$ to take any value in $\mathbb{R}^d$, we can
approximate it simply as
\begin{equation} \label{eq:lambda}
  q(\bm\lambda) = \mathcal{N}(\bm\lambda; \bm\mu, \bm\Sigma)
\end{equation}
for some variational parameters $\bm\mu \in \mathbb{R}^d$ and $\bm\Sigma \in
\mathbb{R}^{d \times d}$. In contrast, as the covariance function must produce
non-negative values, we want to restrict $\zeta$ to positive real numbers. At
the same time, the distribution for $\zeta$ should have a flexible mean (i.e.,
not be tied to zero, like in the exponential distribution) and tails that
converge to zero as the value of the random variable moves away from the mean.
We might want to support some right skew, but the distribution should be close
to symmetric with at least some parameter values. This limits our choice of
distributions quite significantly, and we decide to go with the gamma
distribution as it is fairly flexible and commonly used
\cite{hogg2018introduction}. We then define the probability density function of
the gamma distribution as
\begin{equation} \label{eq:zeta}
  q(\zeta) = \Gamma(\zeta; \alpha, \beta) =
  \frac{\beta^\alpha}{\Gamma(\alpha)}\zeta^{\alpha - 1}e^{-\beta\zeta},
\end{equation}
where $\alpha > 0$ and $\beta > 0$ are two parameters of the distribution, and
$\Gamma(\cdot)$ is the gamma function. This gives us our vector of variational
parameters $\bm\nu = (\mathbf{m}, \mathbf{S}, \bm\mu, \bm\Sigma, \alpha,
\beta)^\intercal$.

\subsection{Evidence Lower Bound}

In this section we derive and simplify the ELBO for this (now fully specified)
model. We begin with a lemma that will be useful during the derivation.

% TODO: this is actually the entropy of the multivariate Gaussian
\begin{lemma} \label{expectation_lemma}
  Let $\mathbf{x} \sim \mathcal{N}(\bm\mu, \bm\Sigma)$ be a random Gaussian
  vector with $n$ elements, and let $f$ be its probability density function.
  Then
  \[ \mathbb{E}[\log f(\mathbf{x})] = -\frac{1}{2}\log |\bm\Sigma| + c, \]
  where $c \in \mathbb{R}$ is a constant w.r.t. $\bm\mu$ and $\bm\Sigma$.
\end{lemma}

\begin{proof}
  Similarly to \eqref{eq:normal}, we can write
  \[ \mathbb{E}[\log f(\mathbf{x})] = \mathbb{E}\left[ -\frac{1}{2}(\mathbf{x} -
      \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{x} - \bm\mu) - \frac{1}{2}\log |\bm\Sigma|
      - \frac{n}{2}\log 2\pi \right]. \]
  Note that the term
  \[ \mathbb{E}\left[ -\frac{n}{2}\log 2\pi \right] = -\frac{n}{2}\log 2\pi \]
  can be absorbed into $c$. Now
  \[ \mathbb{E}[(\mathbf{x} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{x} - \bm\mu)] =
    \Tr(\bm\Sigma^{-1}\mathbf{M}) + \mathbf{c}^\intercal\bm\Sigma^{-1}\mathbf{c}, \]
  where $\mathbf{M} = \Var[\mathbf{x} - \bm\mu]$ and $\mathbf{c} =
  \mathbb{E}[\mathbf{x} - \bm\mu]$ \cite{petersen2008matrix}. But
  \[ \mathbf{c} = \mathbb{E}[\mathbf{x} - \bm\mu] = \mathbb{E}[\mathbf{x}] -
    \bm\mu = \bm\mu - \bm\mu = \mathbf{0}, \]
  so
  \[ \mathbb{E}[(\mathbf{x} - \bm\mu)^\intercal\bm\Sigma^{-1}(\mathbf{x} - \bm\mu)] =
    \Tr(\bm\Sigma^{-1}\mathbf{M}) = \Tr(\mathbf{I}), \]
  since
  \[ \mathbf{M} = \Var[\mathbf{x} - \bm\mu] = \Var[\mathbf{x}] = \bm\Sigma. \]
  Thus, since we have that
  \[ \mathbb{E}\left[ -\frac{1}{2}\log |\bm\Sigma| \right] = -\frac{1}{2}\log
    |\bm\Sigma|, \]
  we get that
  \[ \mathbb{E}[\log f(\mathbf{x})] = -\frac{1}{2}\log |\bm\Sigma| + c \]
  for some $c \in \mathbb{R}$ that depends on $n$, but not on $\bm\mu$ or
  $\bm\Sigma$.
\end{proof}

In order to derive the ELBO, let us go back to \eqref{eq:elbo} and
write\footnote{At this point, we will drop the subscript denoting which
  variables the expectation is taken over. Also note that during the derivation
  equality is taken to mean ``equality up to an additive constant''.}
\[ \mathcal{L}(\bm\nu) = \mathbb{E}[\log\pfull] -
  \mathbb{E}[\log\approximation]. \]
By plugging in \eqref{full} and \eqref{eq:approximation}, we get
\[
  \begin{split}
    \mathcal{L}(\bm\nu) &= \mathbb{E}[\log p(\mathbf{X_u}) + \log p(\Theta |
    \mathbf{X_u}) + \log p(\mathbf{u} | \Theta, \mathbf{X_u}) + \log
    p(\mathbf{r} | \Theta, \mathbf{X_u}, \mathbf{u}) + \log p(\mathcal{D} | r)] \\
    &- \mathbb{E}[\log q(\Theta) + \log q(\mathbf{u}) + \log q(\mathbf{r} |
    \Theta, \mathbf{u})].
  \end{split}
\]
Note that $\mathbb{E}[\log p(\mathbf{X_u})]$ is just a constant, so we can
simply drop it from the expression. Furthermore, since $q(\mathbf{r} | \Theta,
\mathbf{u}) = p(\mathbf{r} | \Theta, \mathbf{X_u}, \mathbf{u})$, they cancel
each other out. Then we can substitute various terms with their definitions to
get
\begin{alignat*}{3}
  \mathcal{L}(\bm\nu) &= \mathbb{E}\left[-\frac{1}{2}\Tr[\Kuu^{-2}] -
    \sum_{i=1}^d \log(\lambda_i + 1)\right] &&+ \mathbb{E}[\log
  \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu)] \\
  &+ \mathbb{E}\left[ \sum_{i=1}^N \sum_{t=1}^T Q_r(s_{i,t}, a_{i,t}) -
    V_r(s_{i,t}) \right] &&-
  \mathbb{E}\left[\log\left(\frac{\beta^\alpha}{\Gamma(\alpha)}\zeta^{\alpha -
        1}e^{-\beta\zeta}\right)\right] \\
  &- \mathbb{E}[\log\mathcal{N}(\bm\lambda; \bm\mu, \bm\Sigma)] &&-
  \mathbb{E}[\log\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})].
\end{alignat*}
Next, we can plug in the definitions of $\mathcal{N}(\mathbf{u}; \mathbf{0},
\Kuu)$ and $Q_r$, distribute the log across the gamma distribution, and apply
Lemma \ref{expectation_lemma} to the last two terms\footnote{We cannot
  apply the lemma to $\mathbb{E}[\log \mathcal{N}(\mathbf{u}; \mathbf{0},
  \Kuu)]$ because $\Kuu$ depends on $\Theta$.} to get
\begin{align*}
  \mathcal{L}(\bm\nu) &= \frac{1}{2}\log|\bm\Sigma| + \frac{1}{2}\log|\mathbf{S}| + \mathbb{E}\left[-\frac{1}{2}\Tr[\Kuu^{-2}] -
    \sum_{i=1}^d \log(\lambda_i + 1)\right] \\
  &+ \mathbb{E}\left[-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} -
    \frac{1}{2}\log|\Kuu| - \frac{m}{2}\log 2\pi\right] \\
  &+ \mathbb{E}\left[\sum_{i=1}^N \sum_{t=1}^T r(s_{i,t}) - V_r(s_{i,t}) + \gamma\sum_{s' \in \mathcal{S}}
    \mathcal{T}(s_{i,t}, a_{i,t}, s')V_r(s') \right] \\
  &- \mathbb{E}[\alpha\log\beta - \log\Gamma(\alpha) + (\alpha - 1)\log\zeta - \beta\zeta].
\end{align*}
Now we can remove $\mathbb{E}\left[-\frac{m}{2}\log2\pi\right]$ since it is
constant w.r.t. both the variational parameters and the variables the
expectation is over, and move constants (or variational parameters) independent
of the approximated variables outside of the expectations. Also note that
$\mathbb{E}[\zeta] = \alpha/\beta$ and
$\mathbb{E}[\log \zeta] = \psi(\alpha) - \log \beta$, where $\psi$ is the
digamma function defined as $\psi(x) =
\frac{d}{dx}\log\Gamma(x)$\cite{DBLP:books/lib/Bishop07}. This allows us to
simplify the expression to the following:
\begin{align*}
  \mathcal{L}(\bm\nu) &= \frac{1}{2}\log|\bm\Sigma| + \frac{1}{2}\log|\mathbf{S}| - \frac{1}{2} \mathbb{E}[\Tr[\Kuu^{-2}]] -
    \sum_{i=1}^d \mathbb{E}[\log(\lambda_i + 1)] \\
  &- \frac{1}{2}\mathbb{E}[\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u}] - \frac{1}{2}\mathbb{E}[\log|\Kuu|] + \alpha - \log\beta + \log\Gamma(\alpha) + (1 - \alpha)\psi(\alpha) \\
  &+ \sum_{i=1}^N \sum_{t=1}^T \mathbb{E}[r(s_{i,t})] - \mathbb{E}[V_r(s_{i,t})] + \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}[V_r(s')]. \\
\end{align*}

\subsection{Derivatives}

\subsection{Variational Inference Algorithms} \label{vi_algs}

% TODO: will need to make this equation more precise, once I know what
% approximation I want to use
The typical way to optimise a quantity (the ELBO, in this case) involves
computing its gradient \cite{blei2017variational}. Due to terms involving
$\mathcal{D}$ that are computed by solving an MDP and do not resemble a typical
probability distribution, we turn our attention to the \emph{black box
  variational inference} \cite{DBLP:conf/aistats/RanganathGB14} paper that
suggests a convenient trick:
\[ \nabla_{\bm\nu} \mathcal{L} = \Eq[\nabla_{\bm\nu} \log \approximation
  (\log \pfull - \log \approximation)]. \]
With this trick in mind, we only need to evaluate $\log \pfull$ and take the
gradient of $\log \approximation$. Following the same paper,
$\nabla_{\bm\nu} \mathcal{L}$ then has an unbiased estimate
\[ \nabla_{\bm\nu} \mathcal{L} \approx \frac{1}{S} \sum_{s=1}^S
  \nabla_{\bm\nu} \log \approximationS (\log \pfullS - \log
  \approximationS) \]
computed by drawing $S$ Monte Carlo samples $(\Theta_s, \mathbf{u}_s,
\mathbf{r}_s) \sim \approximation$.

\section{Work Plan}

show how you plan to organize your work, identifying intermediate deliverables
and dates.

\section{Notes on papers (to be removed)}

\subsection{Miscellaneous}

(Directed) similarity between MDPs using restricted Boltzmann machines
\cite{9401f4eeb9a64c77afb3d087261d1080}

Chapter 6 on distance measures \cite{mccune2002analysis}

The PhD thesis behind maximum causal entropy \cite{Ziebart:2010:MPA:2049078}

\subsection{Gaussian Processes}

Simple introduction to GPs for time-series modelling \cite{Roberts2013GaussianPF}

GPs over graphs instead of vectors (haven't actually read)
\cite{DBLP:journals/corr/abs-1803-05776}

Another introduction from physics (skimmed through) \cite{introduction_to_gps}

Learning a GP from very little data \cite{DBLP:conf/nips/PlattBSWZ01}

One GP for multiple correlated output variables \cite{DBLP:journals/jcphy/BilionisZKL13}

Kernels for categorical and count data \cite{savitsky2011variable}

Scalability/Approximations thesis \cite{kth}

\subsection{Interpretability}

Learning latent factors \cite{DBLP:conf/nips/LiSE17}

The behaviour of Reddit users \cite{DBLP:conf/atal/DasL14}

\subsection{Inverse Reinforcement Learning}

One of the first papers on the topic \cite{DBLP:conf/icml/NgR00}

Bayesian setting \cite{DBLP:conf/ijcai/RamachandranA07}

Learning optimal composite features \cite{DBLP:conf/ijcai/ChoiK13}

A different take on IRL with GPs \cite{DBLP:journals/corr/abs-1208-2112}

IRL for large state spaces (haven't read) \cite{DBLP:journals/jmlr/BoulariasKP11}

Multiple reward functions \cite{DBLP:conf/nips/ChoiK12}

A recent survey \cite{DBLP:journals/corr/abs-1806-06877}

Some not-very-successful method \cite{DBLP:conf/uai/NeuS07}

\subsubsection{Multiple Strategies}

EM clustering \cite{DBLP:conf/icml/BabesMLS11}

Structured priors \cite{DBLP:conf/ewrl/DimitrakakisR11}

There are more, but I haven't gotten to them yet.

\subsection{Variational Inference}

Part IV on probabilities and inference \cite{MacKay:2002:ITI:971143}

Stochastic VI \cite{DBLP:journals/jmlr/HoffmanBWP13}

Structured stochastic VI \cite{DBLP:conf/aistats/HoffmanB15}

Another review of recent advances \cite{DBLP:journals/corr/abs-1711-05597}

Tighter ELBOs are not necessarily better \cite{DBLP:conf/icml/RainforthKLMIWT18}

For details on Lebesgue's dominated convergence theorem \cite{cinlar_probability}

Still looking for the relevant version of the theorem \cite{knill2017probability}

Approximation as a multivariate Gaussian (haven't read) \cite{DBLP:journals/sac/TanN18}

How black box VI can be even more black box \cite{li2016wild}

Evaluating VI \cite{DBLP:conf/icml/YaoVSG18}

\subsubsection{for GPs}

Linear VI for GPs \cite{DBLP:conf/nips/ChengB17}

Sparse VI for GP \cite{DBLP:journals/jmlr/HensmanDS17}

Stochastic VI for sparse spectral GPs (no inducing points, so not that relevant) \cite{DBLP:conf/aaai/HoangHL17}

Sparse GPs 2 \cite{DBLP:conf/nips/GalWR14}

The 3 papers I need to focus on:

SVI for sparse GPs \cite{DBLP:conf/uai/HensmanFL13}

distributed 1 \cite{DBLP:conf/icml/HoangHL15}

distributed 2 (haven't read) \cite{DBLP:conf/icml/PengZZQ17}

The paper at the core of the 3 approaches (approximates posterior) \cite{DBLP:journals/jmlr/Titsias09}

Generalized version? \cite{DBLP:conf/icml/ShethWK15}

\bibliographystyle{plain}
\bibliography{mprop}
\end{document}
