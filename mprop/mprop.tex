\documentclass{mprop}
\usepackage{graphicx}
\usepackage{amssymb,amsmath,amsthm,mathtools}
\usepackage{url}
\usepackage{bm}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[multiple]{footmisc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\diag}{diag}

\begin{document}
\title{Variational Inference for Inverse Reinforcement Learning with Gaussian
  Processes}
\author{Paulius Dilkas}
\date{4th December 2018}
\maketitle
\tableofcontents
\newpage

\newcommand{\Kuu}{\mathbf{K}_{\mathbf{u},\mathbf{u}}}
\newcommand{\Krr}{\mathbf{K}_{\mathbf{r},\mathbf{r}}}
\newcommand{\Kru}{\mathbf{K}_{\mathbf{r},\mathbf{u}}}

\newcommand{\DKL}{D_{\mathrm{KL}}}

\newcommand{\pfull}{p(\mathcal{D}, \bm\lambda, \mathbf{X_u}, \mathbf{u}, \mathbf{r})}
\newcommand{\pfullS}{p(\mathcal{D}, \bm\lambda_s, \mathbf{X_u}, \mathbf{u}_s, \mathbf{r}_s)}
\newcommand{\posterior}{p(\bm\lambda, \mathbf{u}, \mathbf{r} | \mathcal{D}, \mathbf{X_u})}
\newcommand{\approximation}{q_{\bm\nu}(\bm\lambda, \mathbf{u}, \mathbf{r})}
\newcommand{\approximationS}{q_{\bm\nu}(\bm\lambda_s, \mathbf{u}_s, \mathbf{r}_s)}

\newcommand{\Eq}{\mathbb{E}_{(\bm\lambda, \mathbf{u}, \mathbf{r}) \sim \approximation}}

\section{Introduction}
%briefly explain the context of the project problem

Inverse reinforcement learning (IRL)---a problem proposed by Russell in
1998 \cite{DBLP:conf/colt/Russell98}---asks us to find a reward function for a
Markov decision process (MDP) that best explains a set of given demonstrations.
IRL is important because reward functions can be hard to define manually
\cite{DBLP:conf/icml/PieterN04,DBLP:journals/corr/abs-1806-06877}, and rewards
are not entirely specific to a given environment, allowing one to reuse the same
reward structure in previously unseen environments
\cite{DBLP:journals/corr/abs-1806-06877,DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}.
Moreover, IRL has seen a wide array of applications in autonomous vehicle
control \cite{DBLP:journals/ijsr/KimP16,DBLP:journals/ijrr/KretzschmarSSB16} and
learning to predict another agent's behaviour
\cite{DBLP:journals/ai/BogertD18,DBLP:conf/aaai/VogelRGR12,ziebart2008maximum,DBLP:conf/huc/ZiebartMDB08,DBLP:conf/iros/ZiebartRGMPBHDS09}.
Most approaches in the literature (see Section \ref{literature}) make a
convenient yet unjustified assumption that the reward function can be expressed
as a linear combination of features. One proven way to abandon this assumption
is by representing the reward function as a Gaussian process (GP)
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11,DBLP:journals/corr/abs-1208-2112}.
The original approach used maximum likelihood estimation
\cite{DBLP:conf/nips/LevinePK11}, while the goal of this project is to use
variational inference (VI) instead, which learns approximate posterior
probability distributions instead of point estimates. This approach can prove
useful in three major ways:
\begin{enumerate}
\item Modelling full posterior distributions for various parameters can result
  in more precise reward predictions, as the model simply holds more
  information.
\item Having variance estimates for rewards can direct our choice in what data
  should be collected next.
\item An approximate Bayesian treatment of many parameters in the model guards
  against overfitting \cite{DBLP:conf/uai/JinDAS17}.
\end{enumerate}

\section{Statement of the Problem}
%clearly state the problem to be addressed in your forthcoming project. Explain
%why it would be worthwhile to solve this problem.

\begin{definition}
  A \emph{Markov decision process} is a set $\mathcal{M} = \{ \mathcal{S},
  \mathcal{A}, \mathcal{T}, \gamma, r \}$, where $\mathcal{S}$ and
  $\mathcal{A}$ are sets of states and actions, respectively; $\mathcal{T} :
  \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is a function
  defined so that $\mathcal{T}(s, a, s')$ is the probability of moving to state $s'$
  after taking action $a$ in state $s$; $\gamma \in [0, 1)$ is the discount
  factor (with higher $\gamma$ values, it makes little difference whether a
  reward is received now or later, while with lower $\gamma$ values the future
  becomes gradually less and less important); and $r : \mathcal{S} \to
  \mathbb{R}$ is the reward function.
\end{definition}

In \emph{inverse reinforcement learning}, one is presented with an MDP without a
reward function $\mathcal{M} \setminus \{ r \}$ and a set of expert
demonstrations $\mathcal{D} = \{ \zeta_i \}_{i=1}^N$, where each demonstration
$\zeta_i = \{ (s_{i,0}, a_{i,0}), \dots, (s_{i,T}, a_{i,T}) \}$ is a multiset of
state-action pairs representing the actions taken by the expert during a
particular recorded session. Each state is also characterised by a number of
features. The goal of IRL is then to find $r$ such that the optimal policy under
$r$
\[ \pi^* = \argmax_\pi \mathbb{E}\left[ \left. \sum_{t=0}^\infty \gamma^t r(s_t)
    \right| \pi \right] \]
matches the actions in $\mathcal{D}$.

Following previous work on GP IRL
\cite{DBLP:conf/nips/LevinePK11,DBLP:conf/uai/JinDAS17}, we use a maximum
entropy IRL model \cite{ziebart2008maximum}, under which we have that
\[ P(a | s) \propto \exp(Q_r(s, a)), \]
where
\begin{equation}
  Q_r(s, a) = r(s) + \gamma\sum_{s' \in \mathcal{S}}
  \mathcal{T}(s, a, s')V_r(s'),
\end{equation} % TODO: fix the wording!!!
and $V_r(s)$ is a `soft' version of the Bellman backup operator, which can be
obtained by repeatedly applying the following equation until convergence:
\cite{DBLP:conf/nips/LevinePK11,supplementary_material}
\[ V_r(s) = \log \sum_{a \in \mathcal{A}} \exp\left( r(s) + \gamma\sum_{s' \in
      \mathcal{S}} \mathcal{T}(s, a, s')V_r(s') \right). \]
The likelihood of the data can then be written down as
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}
\begin{equation} \label{pDr}
  p(\mathcal{D} | r) = \prod_{i=1}^N \prod_{t=1}^T p(a_{i,t} | s_{i,t}) = \exp\left( \sum_{i=1}^N \sum_{t=1}^T Q_r(s_{i,t}, a_{i,t}) - V_r(s_{i,t}) \right).
\end{equation}
However, a reward function learned by maximising this likelihood is not
transferable to new situations
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}. One needs to model the
reward structure in a way that would allow reward predictions for previously
unseen states.

One way to model rewards without assumptions of linearity is with a
\emph{Gaussian process} (GP). A GP is a collection of random variables, any
finite combination of which has a joint Gaussian distribution
\cite{DBLP:books/lib/RasmussenW06}. We write $r \sim \mathcal{GP}(0,
k_{\bm\lambda})$ to say that $r$ is a GP with mean $0$ and covariance function
$k_{\bm\lambda}$, which uses a vector of hyperparameters $\bm\lambda$.
Covariance functions take two state feature vectors as input and quantify how
similar the two states are, in a sense that we would expect them to have similar
rewards.

As training a GP with $n$ data points has a time complexity of
$\mathcal{O}(n^3)$ \cite{DBLP:books/lib/RasmussenW06}, numerous approximation
methods have been suggested, many of which select a subset of data called
\emph{inducing points} and focus most of the training effort on them
\cite{DBLP:journals/corr/abs-1807-01065}. Let $\mathbf{X_u}$ be the matrix of
features at inducing states, $\mathbf{u}$ the rewards at those states, and
$\mathbf{r}$ a vector with $r(\mathcal{S})$ as elements. Then the full joint
probability distribution can be factorised as
\begin{equation} \label{full}
  \pfull = p(\mathbf{X_u}) \times p(\bm\lambda | \mathbf{X_u}) \times p(\mathbf{u}
  | \bm\lambda, \mathbf{X_u}) \times p(\mathbf{r} | \bm\lambda, \mathbf{X_u},
  \mathbf{u}) \times p(\mathcal{D} | r).
\end{equation}
Here $p(\mathbf{X_u})$ and $p(\bm\lambda | \mathbf{X_u})$ are customisable priors,
\begin{align}
  p(\mathbf{u} | \bm\lambda, \mathbf{X_u}) &= \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu) \nonumber \\
                                       &= \frac{1}{(2\pi)^{m/2}|\Kuu|^{1/2}}\exp \left( -\frac{1}{2} \mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} \right) \nonumber \\
                                       &= \exp\left(-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} - \frac{1}{2}\log|\Kuu| - \frac{m}{2}\log 2\pi\right) \label{eq:normal}
\end{align}
is the GP prior \cite{DBLP:books/lib/RasmussenW06}, where $m \in \mathbb{N}$ is
the number of inducing points. The GP posterior is a
multivariate Gaussian \cite{DBLP:conf/nips/LevinePK11}
\begin{equation} \label{eq:r}
  p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) =
  \mathcal{N}(\mathbf{r}; \Kru^\intercal\Kuu^{-1}\mathbf{u}, \Krr - \Kru^\intercal\Kuu^{-1}\Kru),
\end{equation}
and $p(\mathcal{D} | r)$ is as in \eqref{pDr}. The matrices such as
$\Kru$ are called \emph{covariance matrices} and are defined as
$[\Kru]_{i,j} = k_{\bm\lambda}(\mathbf{x}_{\mathbf{r},i},
\mathbf{x}_{\mathbf{u},j})$, where $\mathbf{x}_{\mathbf{r},i}$ and
$\mathbf{x}_{\mathbf{u},j}$ denote feature vectors for the $i$th state in
$\mathcal{S}$ and the $j$th state in $\mathbf{X_u}$, respectively
\cite{DBLP:conf/uai/JinDAS17}.

Given this model, data $\mathcal{D}$, and inducing feature matrix
$\mathbf{X_u}$, our goal is then to find optimal values of hyperparameters
$\bm\lambda$, inducing rewards $\mathbf{u}$, and the rewards for all relevant
states $\mathbf{r}$. While the previous paper to consider this IRL model
computed maximum likelihood (ML) estimates for $\bm\lambda$ and $\mathbf{u}$,
and made an assumption that $\mathbf{r}$ in \eqref{eq:r} has zero variance
\cite{DBLP:conf/nips/LevinePK11}, we aim to avoid this assumption and use
VI to approximate the full posterior distribution $\posterior$.
\emph{Variational inference} is an approximation technique for probability
densities \cite{blei2017variational}. Let $\approximation$ be our approximating
family of probability distributions for $\posterior$ with its own hyperparameter
vector $\bm\nu$. Then the job of VI algorithms is to optimise $\bm\nu$ in order
to minimise the \emph{Kullback-Leibler} (KL) divergence between the original
probability distribution and our approximation.  KL divergence (asymmetrically)
measures how different the two distributions are, and in this case can be
defined as \cite{blei2017variational}
\[ \begin{split}
    \DKL(\approximation || \posterior) &= \Eq [\log\approximation -
    \log\posterior ] \\
    &= \Eq[\log\approximation - \log\pfull] \\
    &+ \Eq[\log p(\mathcal{D}, \mathbf{X_u})].
  \end{split}
\]
The last term is both hard to compute and constant w.r.t. $\approximation$
\cite{blei2017variational}, so we can remove it from our optimisation objective.
The negation of what remains is often called the \emph{evidence lower bound}
(ELBO) and is defined as\footnote{Throughout the proposal, all integrals should
  be interpreted as definite integrals over the entire sample space.}
\cite{DBLP:books/lib/Bishop07,blei2017variational}
\begin{equation} \label{eq:elbo}
  \begin{split}
    \mathcal{L} &= \Eq \left[ \log \frac{\pfull}{\approximation}
    \right] \\
    &= \iiint \approximation \log
    \frac{\pfull}{\approximation}\,d\bm\lambda\,d\mathbf{u}\,d\mathbf{r}.
  \end{split}
\end{equation}

By considering full probability distributions instead of point estimates---as
long as the approximations are able to capture important features of the
posterior---our predictions are likely to be more accurate and rely on fewer
assumptions. Moreover, we hope to make use of various recent advancements in VI
for both time complexity and approximation distribution fit (see Section
\ref{literature}), making the resulting algorithm competitive both in terms of
running time and model fit.

The project is primarily concerned with investigating how a VI formulation of
the GP IRL model compares against the original ML approach. Most importantly, we
aim to compare how the two algorithms converge both over time and as the number
of demonstrations increases. It would also be interesting to see how close the
approximation of the posterior distribution is to the real thing---we discuss
ways to do this in Section \ref{sec:evaluation}. Finally, it is reasonable to
conjecture that VI can outperform point estimates when dealing with more
uncertainty, e.g., when experts make mistakes. We can easily investigate this by
adapting how the evaluation data is generated.

\section{Literature Survey} \label{literature}
%present an overview of relevant previous work including articles, books, and
%existing software products. Critically evaluate the strengths and weaknesses of
%the previous work.

As mentioned in the introduction, most IRL algorithms assume that the reward
function can be represented as a linear combination of features. This assumption
originated in one of the earliest papers on the topic by Ng and Russell
\cite{DBLP:conf/icml/NgR00}, which introduced several linear programming
approaches to the problem. The authors also noticed that often
multiple reward functions can explain the same behaviour, and suggested
heuristics for reward functions that are `far away' from reward functions that
do not fit the data.

A few years later, Abbeel and Ng \cite{DBLP:conf/icml/PieterN04} developed an
algorithm for the same formulation of the problem with a guarantee to converge
quickly. Neu and Szepesv\'ari \cite{DBLP:conf/uai/NeuS07} identified a weakness
in Abbeel and Ng's approach: the algorithm requires features to be
`appropriately' scaled, and optimal scaling may not be known. Instead, they
suggest a way to combine IRL with \emph{apprenticeship learning}, i.e., a
supervised learning task for optimal \emph{policy} recovery (whereas IRL focuses
on recovering the reward function).

Ramachandran and Amir \cite{DBLP:conf/ijcai/RamachandranA07} were the first to
formulate IRL in terms of Bayesian learning. While the model is easily
interpretable and able to handle experts that make mistakes, the algorithm can
only handle small state spaces and requires Monte Carlo Markov Chain (MCMC)
sampling for inference.

Ziebart et al. \cite{ziebart2008maximum} keep the linearity assumption, but
introduce an influential idea: resolving the ambiguity when multiple reward
functions explain the data by appealing to the maximum entropy principle.

Choi and Kim \cite{DBLP:conf/ijcai/ChoiK13} extend the Bayesian model to learn
good features as well as the reward function, trying to overcome the limitation
of linearity. However, the approach is quite limiting: all features are assumed
to have Boolean values, and the algorithm simply learns their conjunctions.

Levine et al. \cite{DBLP:conf/nips/LevinePK11} are the first to suggest a way to
learn nonlinear reward functions without harsh restrictions on the problem
domain by using GPs. We base our work primarily on their paper, and the
weaknesses we hope to address have already been covered in the previous section.
A recent extension to their work by Jin et al. \cite{DBLP:conf/uai/JinDAS17}
aims to harness the power of deep learning by using several layers of GPs,
making the model less dependent on being provided good features. They also use
VI, but with a few simplifying assumptions: deterministic training conditional
for the reward vector, and fully independent training conditional for the latent
state (see Section \ref{sec:vi_literature} for details).

Finally, instead of using GPs to model nonlinear reward functions, one can use a
neural network (NN), as demonstrated by Wulfmeier et al.
\cite{wulfmeier2015maximum}. Their approach benefits from constant time
inference and the ability to learn complex features either from already-given
features or even from raw data. The only disadvantage (as demonstrated in the
paper) is that NNs converge slower than GPs (as measured by the number of
demonstrations).

%A different take on IRL with GPs \cite{DBLP:journals/corr/abs-1208-2112}
%IRL for large state spaces (haven't read) \cite{DBLP:journals/jmlr/BoulariasKP11}
%Multiple reward functions \cite{DBLP:conf/nips/ChoiK12}
%A recent survey \cite{DBLP:journals/corr/abs-1806-06877}
%EM clustering \cite{DBLP:conf/icml/BabesMLS11}
%Structured priors \cite{DBLP:conf/ewrl/DimitrakakisR11}

\subsection{Variational Inference} \label{sec:vi_literature}

Variational inference has seen a recent increase in interest among academics,
with different approaches focusing on different goals: better time complexity,
handling a wider variety of models, making approximations more accurate, and
using more complex function approximation techniques (such as NNs)
to infer local latent variable values without having to calculate them
individually for each data point \cite{DBLP:journals/corr/abs-1711-05597}. As
our IRL model is based on a GP, we will begin by reviewing some of the VI
approaches applied specifically to GP regression. Based on a recent review of
scalable GPs \cite{DBLP:journals/corr/abs-1711-05597}, we will concentrate on
\emph{stochastic variational sparse approximations}, as they have achieved
modelling accuracy close to that of the full GP with no approximations, with
many methods providing a time complexity of $\mathcal{O}(m^3)$. Below we provide
a short overview of various assumptions that have been used in approximating
\emph{sparse} GPs (i.e., GPs that use inducing points), following on a paper by
Qui\~{n}onero-Candela and Rasmussen \cite{DBLP:journals/jmlr/CandelaR05}.

\begin{description}
  \item [Subset of data ($\mathcal{O}(m^3)$)] is a baseline method of simply
    using a subset of data points.
  \item [Subset of regressors ($\mathcal{O}(nm^2)$)
    \cite{silverman1985some,DBLP:conf/nips/SmolaB00,DBLP:conf/nips/WahbaLGXKK98}]
    is a degenerate approximation that uses a weight for each inducing point. A
    GP is called \emph{degenerate} if the covariance function has a finite
    number of non-zero eigenvalues, restricting the prior distribution to only a
    finite number of linearly independent functions
    \cite{DBLP:journals/jmlr/CandelaR05}.
  \item [Deterministic training conditional ($\mathcal{O}(nm^2)$)
    \cite{DBLP:conf/aistats/SeegerWL03}] approximation imposes a zero-variance
    normal distribution for $\mathbf{r} | \mathbf{u}$, resulting in the same mean
    but different variance predictions compared to the subset of regressors.
  \item [Fully independent training conditional ($\mathcal{O}(nm^2)$)
    \cite{DBLP:conf/nips/SnelsonG05}] has the assumption that the GP values are
    independent of each other when conditioned on the inducing values:
    \[ q(\mathbf{r} | \mathbf{u}) = \prod_{i=1}^n p(r_i | \mathbf{u}). \]
  \item [Partially independent training conditional ($\mathcal{O}(nm^2)$)
    \cite{DBLP:journals/neco/Tresp00,DBLP:conf/nips/SchwaighoferT02}]
    approximates the same distribution as the fully independent training
    conditional, but considers a block diagonal rather than a diagonal
    covariance matrix.
  \item [Transduction] tailors the predictive distribution to
    specific test inputs \cite{DBLP:journals/jmlr/CandelaR05}. As we are not too
    concerned about a specific set of test inputs in the IRL setting,
    transduction is of limited interest to us.
  \item [Augmentation \cite{rasmussen2002reduced}] aims to improve predictive
    accuracy by adding each test input to the inducing points.
  \item [Nystr\"{o}m approximation ($\mathcal{O}(nm^2)$)
    \cite{DBLP:conf/nips/WilliamsS00}] approximates the prior covariance of
    $\mathbf{r}$, but can lead to negative predictive variances.
  \item [Relevance vector machine ($\mathcal{O}(m^3)$)
    \cite{DBLP:journals/jmlr/Tipping01}] is a degenerative approximation
    supporting a limited range of covariance functions.
\end{description}

\begin{table}
  \centering
  \begin{tabular}{llllll}
    \toprule
    Authors, year & Inducing points & Hyperparameters & Complexity \\
    \midrule
    Titsias, 2009 \cite{DBLP:journals/jmlr/Titsias09} & variational & variational & $\mathcal{O}(nm^2)$ \\
    Hensman et al., 2013 \cite{DBLP:conf/uai/HensmanFL13} & fixed & variational & $\mathcal{O}(m^3)$ \\
    Gal et al., 2014 \cite{DBLP:conf/nips/GalWR14} & variational & variational & $\mathcal{O}(nm^2)$ \\
    %Hoang et al., 2015 \cite{DBLP:conf/icml/HoangHL15} & fixed & fixed & $\mathcal{O}(m^3)$ \\
    Cheng and Boots, 2017 \cite{DBLP:conf/nips/ChengB17} & variational & variational & $\mathcal{O}(nm_\alpha + nm_\beta^2)$ \\
    Hensman et al., 2017 \cite{DBLP:journals/jmlr/HensmanDS17} & fixed & variational & $\mathcal{O}(nm)$ \\
    Peng et al., 2017 \cite{DBLP:conf/icml/PengZZQ17} & variational & variational & $\mathcal{O}(m^3)$ \\
    \bottomrule
  \end{tabular}
  \caption{Summary of relevant VI approximations to GPs. For both inducing
    points and hyperparameters, `fixed' means `chosen before the algorithm
    starts' and `variational' means `included amongst the variational
    parameters'. Hyperparameters $m_\alpha$ and $m_\beta$ refer to the number of
    bases used to represent the GP's mean and covariance, respectively.}
  \label{table:approximations}
\end{table}

Table \ref{table:approximations} further summarises some of the recent and/or
influential GP approximation approaches that might be relevant to our situation.
For many of the variables in our model (including the inducing points and the
hyperparameters of the covariance function), we have three possible ways of
handling them:
\begin{itemize}
\item Provide a full Bayesian treatment by defining a prior probability
  distribution. While this option can easily prevent overfitting, the choice of
  prior can be difficult to justify.
\item Fix the value of the variable. We can then optimise the value outside of
  the main algorithm, e.g., by running the algorithm with a set of possible
  values and keeping the value that produces the best performance. This option
  can be slow if we are interested in finding the optimal value, but could be
  appropriate if the variable has little impact on the algorithm's performance.
\item Treat the variable as a variational parameter. This method will
  efficiently optimise the value of the variable, but is vulnerable to
  overfitting. Furthermore, this requires us to find the derivative of the ELBO
  w.r.t. the variable, which could be difficult.
\end{itemize}

Titsias \cite{DBLP:journals/jmlr/Titsias09} is the first to suggest a
variational approximation where inducing points are treated as variational
parameters. The paper has influenced many of the later works covered in this
section, and has been reformulated by Gal et al. \cite{DBLP:conf/nips/GalWR14}
into a distributed algorithm. We will fix the inducing points for the baseline
version of our algorithm, but keep the idea in mind for possible future work.

Hensman et al. \cite{DBLP:conf/uai/HensmanFL13} reduce the complexity from
$\mathcal{O}(nm^2)$ to $\mathcal{O}(m^3)$ using a stochastic variational
inference (SVI) approach. While results such as efficiently computable natural
gradients and analytically tractable optimal solutions for variational
parameters are unlikely to transfer to a different problem domain, the overall
SVI framework and an approximating Gaussian distribution for $\mathbf{u}$ will
play an important role in our proposal.

Cheng and Boots \cite{DBLP:conf/nips/ChengB17} suggest using different bases in
the reproducing kernel Hilbert space for the mean and covariance functions.
The paper makes few assumptions and mostly relies on the work of
Titsias \cite{DBLP:journals/jmlr/Titsias09}, making it seem transferable to a
new domain. However, applying the idea to IRL is likely to be beyond the scope
of the project.

The variational Fourier features (VFF) algorithm by Hensman et al.
\cite{DBLP:journals/jmlr/HensmanDS17} is only defined for Mat\'ern kernels,
which is likely to be too restrictive for our situation (the original paper on
using GPs for IRL \cite{DBLP:conf/nips/LevinePK11} used the automatic relevance
detection kernel that has weights controlling how important each feature is).
While extending VFF to support a flexible class of kernels defined by Wilson and
Adams \cite{pmlr-v28-wilson13} is an interesting and promising avenue of work,
it is likely to be beyond the scope of this project as well.

While Peng et al. \cite{DBLP:conf/icml/PengZZQ17} provide a highly efficient
distributed implementation, the derivation of the ELBO relies primarily on the
fact that the evidence for a GP is a Gaussian, while in our case the evidence is
anything but. Hence, it is unlikely that the ideas from this paper could be
applicable to our IRL model.

Finally, since we expect $\posterior$ to be highly irregular, we would like our
approximation to be capable of representing a wide range of possible probability
distributions. The primary way of representing complex posteriors in VI is by
using \emph{normalising flows}, i.e., a collection of invertible
functions---parametrised by additional variational parameters---that are applied
to latent variables \cite{DBLP:conf/icml/RezendeM15}. Normalising flows, along
with some of the ideas mentioned previously, could provide great benefits to the
GP IRL VI model. However, we are forced to keep any non-essential features as
possible future work in order to make the project feasible in a given time
frame. Furthermore, note that due to various differences between regression and
IRL, we cannot simply apply a GP VI model as a whole. Instead, smaller tricks
and ideas from various papers can (and will) be used throughout the project.

\section{Proposed Approach} \label{sec:proposed_approach}
%state how you propose to solve the software development problem. Show that your
%proposed approach is feasible, but identify any risks.

In this section we show a feasible way to apply VI to the GP IRL model. Section
\ref{sec:preliminaries} defines several previously-missing parts of the model
and provides intuition for how the approximating distribution could be
structured. Section \ref{sec:structure} then defines the approximating
distribution, where some choices are justified by a body of literature, and some
are novel. Next, Section \ref{sec:elbo} derives a simplified expression for the
ELBO. Finally, in Section \ref{vi_algs} we describe our strategy for
optimising the ELBO.

\subsection{Preliminaries} \label{sec:preliminaries}

In order to properly investigate the difference between variational inference
and ML estimation, we keep other parts of the model the same.
Namely, we set the covariance function to a version of the automatic relevance
detection kernel \cite{DBLP:conf/nips/LevinePK11,neal2012bayesian}
\[ k_{\bm\lambda}(\mathbf{x}_i, \mathbf{x}_j) = \lambda_0\exp\left(
    -\frac{1}{2}(\mathbf{x}_i - \mathbf{x}_j)^\intercal\bm\Lambda(\mathbf{x}_i -
    \mathbf{x}_j) - \mathbbm{1}[i \ne j]\sigma^2\Tr(\bm\Lambda) \right), \]
where $\lambda_0$ is the overall `scale' factor for how similar or distant the
states are, $\bm\Lambda = \diag(\lambda_1, \dots, \lambda_d)$ is a diagonal
matrix that determines the relevance of each feature (where $d$ denotes the
number of features), $\mathbbm{1}$ is defined as
\[ \mathbbm{1}[b] = \begin{cases}
    1 & \mbox{if $b$ is true} \\
    0 & \mbox{otherwise},
  \end{cases} \]
and $\sigma^2$ is set to $10^{-2}/2$ (as the original paper noted that
the value makes little difference to the performance of the algorithm
\cite{DBLP:conf/nips/LevinePK11}). Our vector of hyperparameters for the
covariance function is then $\bm\lambda = (\lambda_0, \dots,
\lambda_d)^\intercal$. Similarly, we keep the expression for the prior of
$\bm\lambda$:
\begin{equation} \label{eq:theta_prior}
  p(\bm\lambda | \mathbf{X_u}) = \exp\left( -\frac{1}{2}\Tr(\Kuu^{-2}) -
    \sum_{i=1}^d \log(\lambda_i + 1) \right).
\end{equation}
Defining the prior for $\bm\lambda$ means that we commit ourselves to
a Bayesian treatment of the variable as opposed to, e.g., treating it as a
variational parameter. While the remainder of the proposal will stick with this
assumption, the alternative is certainly worth exploring in its own right, and
will be investigated after the main formulation is implemented.

Next, we can rewrite the posterior by using the chain rule and Bayes' theorem in
order to get a better sense of what we are trying to approximate:
\[
  \begin{split}
    \posterior &= p(\bm\lambda | \mathbf{X_u}, \mathcal{D}) p(\mathbf{u}
    | \bm\lambda, \mathbf{X_u}, \mathcal{D}) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}, \mathcal{D}) \\
    &\propto p(\bm\lambda | \mathbf{X_u}, \mathcal{D}) p(\mathbf{u} |
    \bm\lambda, \mathbf{X_u}, \mathcal{D}) p(\mathcal{D} | r)
    p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) \\
    &\propto p(\bm\lambda | \mathbf{X_u}, \mathcal{D}) p(\mathcal{D} |
    \bm\lambda, \mathbf{X_u}, \mathbf{u}) p(\mathbf{u} | \bm\lambda,
    \mathbf{X_u}) p(\mathcal{D} | r) p(\mathbf{r} | \bm\lambda,
    \mathbf{X_u}, \mathbf{u}) \\
    &\propto p(\mathcal{D} | \bm\lambda, \mathbf{X_u}) p(\bm\lambda |
    \mathbf{X_u}) p(\mathcal{D} | \bm\lambda, \mathbf{X_u}, \mathbf{u})
    p(\mathbf{u} | \bm\lambda, \mathbf{X_u}) p(\mathcal{D} | r)
    p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u})
  \end{split}
\]
Note that now there are only two unknown probability distributions:
$p(\mathcal{D} | \bm\lambda, \mathbf{X_u})$ and $p(\mathcal{D} | \bm\lambda,
\mathbf{X_u}, \mathbf{u})$, which can be expressed as follows:
\begin{align*}
  p(\mathcal{D} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) = \int &p(\mathcal{D} | r) p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) \, d\mathbf{r}, \\
  p(\mathcal{D} | \bm\lambda, \mathbf{X_u}) = \iint &p(\mathcal{D} | r) p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) p(\mathbf{u} | \bm\lambda, \mathbf{X_u}) \, d\mathbf{u} \, d\mathbf{r}.
\end{align*}
This suggests the following form for the approximation:
\begin{equation} \label{eq:approximation}
  \approximation = q(\bm\lambda) \times q(\mathbf{u} | \bm\lambda) \times q(\mathbf{r}
  | \bm\lambda, \mathbf{u}).
\end{equation}

\subsection{Structure of the Approximating Distribution} \label{sec:structure}

At this point, we are forced to make assumptions about the approximate posterior
in order to arrive at an implementable solution. Time permitting, ways to relax
the assumptions may be investigated towards the end of the project.

First, as is common in the literature for applying VI to GPs
\cite{DBLP:conf/nips/ChengB17,DBLP:conf/uai/HensmanFL13,DBLP:conf/icml/HoangHL15,DBLP:journals/jmlr/Titsias09},
we simply set
\begin{equation}
  q(\mathbf{r} | \bm\lambda, \mathbf{u}) = p(\mathbf{r} | \bm\lambda, \mathbf{X_u},
  \mathbf{u}).
\end{equation}
We can make a similarly justified choice for $q(\mathbf{u} | \bm\lambda)$:
\begin{equation}
  q(\mathbf{u} | \bm\lambda) = q(\mathbf{u}) = \mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S}),
\end{equation}
where $\mathbf{m} \in \mathbb{R}^{m}$ is the mean vector and the $m \times m$
positive semi-definite matrix $\mathbf{S}$ is the covariance matrix
\cite{DBLP:conf/nips/ChengB17,DBLP:journals/jmlr/HensmanDS17,DBLP:conf/aaai/HoangHL17}.

Next, we need to choose an approximating distribution for $\bm\lambda$, but,
unfortunately, all papers in Table \ref{table:approximations} either fix it or
treat it as a variational parameter. Hence, we make our first assumption without
justification from previous literature:
\begin{equation} \label{eq:theta}
  q(\bm\lambda) = \prod_{i=0}^dq(\lambda_i).
\end{equation}
We want to restrict $\lambda_0$ to be positive so that $k_{\bm\lambda}$ would
produce non-negative values and not become trivial. Similarly, we want that
$\lambda_i \ge 0$ for $i = 1, \dots, d$ so that $\bm\Lambda$ is a
positive-definite matrix. Considering the possible distributions for all $d + 1$
variables, we would like the mean to be flexible (i.e., not tied to zero, like
in the exponential distribution), and the tails to converge to zero as the
value of the random variable moves away from the mean. We might want to support
some right skew, but the distribution should be close to symmetric with at least
some parameter values. This limits our choice of distributions quite
significantly, and we decide to go with the gamma distribution as it is fairly
flexible and commonly used \cite{hogg2018introduction}. We then define the
probability density functions as
\begin{equation} \label{eq:zeta}
  q(\lambda_i) = \Gamma(\lambda_i; \alpha_i, \beta_i) =
  \frac{\beta_i^{\alpha_i}}{\Gamma(\alpha_i)}\lambda_i^{\alpha_i - 1}e^{-\beta_i\lambda_i}, \quad i = 0, \dots, d,
\end{equation}
where $\alpha_i > 0$ and $\beta_i > 0$ are parameters of the distribution, and
$\Gamma(\cdot)$ is the gamma function. This gives us our
vector of variational parameters $\bm\nu = (\mathbf{m}, \mathbf{S}, \alpha_0,
\beta_0, \dots, \alpha_d, \beta_d)^\intercal$.

\subsection{Evidence Lower Bound} \label{sec:elbo}

In this section we derive and simplify the ELBO for this (now fully specified)
model. In order to derive the ELBO, let us go back to \eqref{eq:elbo} and
write\footnote{At this point, we will drop the subscript denoting which
  variables the expectation is taken over. Also note that throughout the
  derivation equality is taken to mean `equality up to an additive constant'.}
\[ \mathcal{L} = \mathbb{E}[\log\pfull] -
  \mathbb{E}[\log\approximation]. \]
By plugging in \eqref{full} and \eqref{eq:approximation}, we get
\[
  \begin{split}
    \mathcal{L} &= \mathbb{E}[\log p(\mathbf{X_u}) + \log p(\bm\lambda |
    \mathbf{X_u}) + \log p(\mathbf{u} | \bm\lambda, \mathbf{X_u}) + \log
    p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u}) + \log p(\mathcal{D} | r)] \\
    &- \mathbb{E}[\log q(\bm\lambda) + \log q(\mathbf{u}) + \log q(\mathbf{r} |
    \bm\lambda, \mathbf{u})].
  \end{split}
\]
Note that $\mathbb{E}[\log p(\mathbf{X_u})]$ is just a constant, so we can
simply drop it from the expression. Furthermore, since $q(\mathbf{r} | \bm\lambda,
\mathbf{u}) = p(\mathbf{r} | \bm\lambda, \mathbf{X_u}, \mathbf{u})$, they cancel
each other out. Then we can substitute various terms with their definitions to
get
\begin{alignat*}{3}
  \mathcal{L} &= \mathbb{E}\left[-\frac{1}{2}\Tr(\Kuu^{-2}) -
    \sum_{i=1}^d \log(\lambda_i + 1)\right] &&+ \mathbb{E}[\log
  \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu)] \\
  &+ \mathbb{E}\left[ \sum_{i=1}^N \sum_{t=1}^T Q_r(s_{i,t}, a_{i,t}) -
    V_r(s_{i,t}) \right] &&-
  \sum_{i=0}^d\mathbb{E}\left[\log\left(\frac{\beta_i^{\alpha_i}}{\Gamma(\alpha_i)}\lambda_i^{\alpha_i
        - 1}e^{-\beta_i\lambda_i}\right)\right] \\
  &-
  \mathbb{E}[\log\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})].
\end{alignat*}
We can simplify the last term by noting that $-\mathbb{E}[\log
\mathcal{N}(\mathbf{u}; \mathbf{m}, \mathbf{S})] = \frac{1}{2}\log|\mathbf{S}|$
up to an additive constant that depends on the number of dimensions of the
distribution \cite{DBLP:journals/tit/AhmedG89}. Also note that we cannot use
this fact for $\mathbb{E}[\log \mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu)]$
because $\Kuu$ depends on $\bm\lambda$. We also plug in the definitions of
$\mathcal{N}(\mathbf{u}; \mathbf{0}, \Kuu)$ and $Q_r$, and get:
\begin{align*}
  \mathcal{L} &= \frac{1}{2}\log|\mathbf{S}| + \mathbb{E}\left[-\frac{1}{2}\Tr(\Kuu^{-2}) - \sum_{i=1}^d \log(\lambda_i + 1)\right] \\
  &+ \mathbb{E}\left[-\frac{1}{2}\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u} -
    \frac{1}{2}\log|\Kuu| - \frac{m}{2}\log 2\pi\right] \\
  &+ \mathbb{E}\left[\sum_{i=1}^N \sum_{t=1}^T r(s_{i,t}) - V_r(s_{i,t}) + \gamma\sum_{s' \in \mathcal{S}}
    \mathcal{T}(s_{i,t}, a_{i,t}, s')V_r(s') \right] \\
  &- \sum_{i=0}^d \mathbb{E}[\alpha_i\log\beta_i - \log\Gamma(\alpha_i) + (\alpha_i - 1)\log\lambda_i - \beta_i\lambda_i]
\end{align*}
Now we can remove $\mathbb{E}\left[-\frac{m}{2}\log2\pi\right]$ since it is
constant w.r.t. both the variational parameters and the variables the
expectation is over, and move constants (or variational parameters) independent
of the approximated variables outside of the expectations. Also note that
$\mathbb{E}[\lambda_i] = \alpha_i/\beta_i$ and
$\mathbb{E}[\log \lambda_i] = \psi(\alpha_i) - \log\beta_i$, where $\psi$ is the
digamma function defined as $\psi(x) = \frac{d}{dx}\log\Gamma(x)$
\cite{DBLP:books/lib/Bishop07}. Moreover, we can simplify
$\sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t})$ by choosing to represent all visited states
in $\mathbf{r} = (r_1, \dots, r_k)^\intercal$, and defining a new vector
$\mathbf{t} = (t_1, \dots, t_k)^\intercal$, where $t_i$ is the number of times
the state associated with the reward $r_i$ has been visited across all
demonstrations. Then
\begin{align*}
\mathbb{E} \left[ \sum_{i=1}^N\sum_{t=1}^Tr(s_{i,t}) \right] &=
    \mathbb{E}[\mathbf{t}^\intercal\mathbf{r}] =
    \mathbf{t}^\intercal\mathbb{E}[\mathbf{r}] =
    \mathbf{t}^\intercal\mathbb{E}_{(\bm\lambda, \mathbf{u}) \sim
  q(\bm\lambda)q(\mathbf{u})}[\Kru^\intercal\Kuu^{-1}\mathbf{u}] \\
  &= \mathbf{t}^\intercal\mathbb{E}_{\bm\lambda \sim q(\bm\lambda)}[\Kru^\intercal\Kuu^{-1}\mathbf{m}] = \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}.
\end{align*}
Finally, as $\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u}$ is a function of
$\mathbf{u}$ and $\bm\lambda$, we can take the expectation of $\mathbf{u}$, leaving
the expectation of $\bm\lambda$:
\[ \mathbb{E}[\mathbf{u}^\intercal\Kuu^{-1}\mathbf{u}] =
  \mathbb{E}[\Tr(\Kuu^{-1}\mathbf{S}) +
  \mathbf{m}^\intercal\Kuu^{-1}\mathbf{m}] =
  \Tr(\mathbb{E}[\Kuu^{-1}]\mathbf{S}) +
  \mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}. \]
This allows us to simplify $\mathcal{L}(\bm\nu)$ to the following:
\begin{align*}
  \mathcal{L}(\bm\nu) &= \frac{1}{2}\log|\mathbf{S}| - \frac{1}{2}\Tr(\mathbb{E}[\Kuu^{-2}]) - \sum_{i=1}^d \mathbb{E}[\log(\lambda_i + 1)] - \frac{1}{2}\Tr(\mathbb{E}[\Kuu^{-1}]\mathbf{S}) \\
  &- \frac{1}{2}\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m} - \frac{1}{2}\mathbb{E}[\log|\Kuu|] + \sum_{i=0}^d \alpha_i - \log\beta_i + \log\Gamma(\alpha_i) + (1 - \alpha_i)\psi(\alpha_i) \\
  &+ \mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m} - \sum_{i=1}^N \sum_{t=1}^T \mathbb{E}[V_r(s_{i,t})] - \gamma\sum_{s' \in \mathcal{S}} \mathcal{T}(s_{i,t}, a_{i,t}, s')\mathbb{E}[V_r(s')]. \\
\end{align*}

\subsection{Variational Inference} \label{vi_algs}

The typical way to optimise a quantity (the ELBO, in this case) involves
computing its gradient \cite{blei2017variational}. Unfortunately, some of the
terms in $\mathcal{L}$ are still left as expected values. \emph{Black box
  variational inference} (BBVI) \cite{DBLP:conf/aistats/RanganathGB14} suggests
a way to express the gradient as an expectation without having to take the
gradient of the posterior:
\[ \nabla_{\bm\nu} \mathcal{L} = \Eq[\nabla_{\bm\nu} \log \approximation
  (\log \pfull - \log \approximation)]. \]
The gradient of the ELBO then has a unbiased estimate
\[ \nabla_{\bm\nu} \mathcal{L} \approx \frac{1}{S} \sum_{s=1}^S
  \nabla_{\bm\nu} \log \approximationS (\log \pfullS - \log
  \approximationS), \]
computed by drawing $S$ Monte Carlo samples $(\bm\lambda_s, \mathbf{u}_s,
\mathbf{r}_s) \sim \approximation$.

The main theorem behind the BBVI trick is the Lebesgue's dominated convergence
theorem \cite{royden1988real}, which---under some conditions---allows one to
claim that:
\[ \lim_{n \to \infty} \int_E f_n\,d\mu = \int_E \lim_{n \to \infty}
  f_n\,d\mu. \]
We can use the same idea on parts of $\mathcal{L}$, derivatives of which cannot
be taken otherwise. In our case, we want a gradient instead of a limit, and the
integral represents an expected value.

Let us briefly examine the options available in case the theorem is not
applicable to some of our expected values. First,
\[ \frac{1}{2}\Tr(\mathbb{E}[\Kuu^{-2}]) \quad \text{and} \quad \sum_{i=1}^d
  \mathbb{E}[\log(\lambda_i + 1)] \]
come from a chosen prior distribution for $\bm\lambda | \mathbf{X_u}$.
There are two ways to deal with them: replacing the prior with an
easier-to-handle alternative, and treating $\bm\lambda$ as a variational
parameter.

Then,
\[ \frac{1}{2}\Tr(\mathbb{E}[\Kuu^{-1}]\mathbf{S}), \quad 
  \frac{1}{2}\mathbf{m}^\intercal\mathbb{E}[\Kuu^{-1}]\mathbf{m}, \quad
  \text{and} \quad \frac{1}{2}\mathbb{E}[\log|\Kuu|] \]
all derive from $p(\mathbf{u} | \bm\lambda, \mathbf{X_u})$, which cannot be
eliminated or replaced without changing the idea of the project. However, notice
that the expressions inside $\mathbb{E}[\cdot]$ are all functions of $\Kuu$,
which depends only on $\bm\lambda$ and $\mathbf{X_u}$. Thus, if we make
$\bm\lambda$ a variational parameter, the expected values are no longer taken
w.r.t. $\bm\lambda$, and $\mathbb{E}[f(\Kuu)] = f(\Kuu)$ for any function $f$
that does not involve $\mathbf{u}$ or $\mathbf{r}$. As a result, the three
terms simplify to expressions without expected values.

Lastly,
$\mathbf{t}^\intercal\mathbb{E}[\Kru^\intercal\Kuu^{-1}]\mathbf{m}$ and both
instances of $\mathbb{E}[V_r(s)]$ come from $p(\mathcal{D} | r)$. The first
expected value can be resolved by making $\bm\lambda$ a variational
parameter. The MDP value function requires more effort. We can start by making
the deterministic training conditional assumption (as is commonly done in
previous work \cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}), which
sets the posterior covariance matrix of $\mathbf{r}$ to the zero matrix. Then
the posterior $\mathbf{r}$ becomes $\Kru^\intercal\Kuu^{-1}\mathbf{u}$. But we
still have a (complicated) function of $\bm\lambda$ and $\mathbf{u}$ in the
integral w.r.t. both variables. The integral (or expected value) w.r.t.
$\bm\lambda$ can be eliminated the usual way, i.e., by making $\bm\lambda$
variational. As for $\mathbf{u}$, we can borrow a trick from the deep GP IRL
paper \cite{DBLP:conf/uai/JinDAS17} and set $q(\mathbf{u}) = \delta(\mathbf{u} -
\mathbf{m})$, where $\delta$ is the Dirac delta function. This is essentially a
different way to impose a zero-variance restriction, modelling $\mathbf{u}$ as a
random vector with variance tending to zero. The expected value then becomes
$\mathbb{E}[V_r(s)] = V_{\Kru^\intercal\Kuu^{-1}\mathbf{m}}(s)$.

While the ELBO can now be computed without approximations, we ended up imposing
two zero-variance restrictions, resulting in a less informative model. Moreover,
we had to abandon the Bayesian treatment of $\bm\lambda$, making the model more
vulnerable to overfitting.

Assuming that such workarounds will not be needed (or desired), we can then take
the BBVI algorithm as the basis for our algorithm, while keeping in mind the
numerous improvement possibilities provided both by the BBVI paper
\cite{DBLP:conf/aistats/RanganathGB14} and many others (see Section
\ref{literature}).

\section{Work Plan}
%show how you plan to organize your work, identifying intermediate deliverables
%and dates.

The work plan can be summarised as follows:
\begin{enumerate}
\item Derive expressions for the derivatives of the ELBO w.r.t. the variational
  parameters. \emph{Deliverables}: full derivations that can be included as
  supplementary material of the final paper, including formal proofs for
  applications of the dominated convergence theorem. \emph{Date}: 15~December.
\item Implement the algorithm as a more specialised version of Algorithm~2 in
  the BBVI paper \cite{DBLP:conf/aistats/RanganathGB14}. As most of the relevant
  IRL algorithms have been implemented in
  MATLAB\footnote{\url{http://graphics.stanford.edu/projects/gpirl/irl_toolkit.zip}}\footnote{\url{https://github.com/jinming99/DGP-IRL}},
  it makes sense to do the same in order to have accurate time-sensitive
  comparisons between the algorithms. \emph{Deliverables}: a working
  implementation that can be integrated into the IRL toolkit by Levine et al.
  \cite{DBLP:conf/nips/LevinePK11}, and the same algorithm written in pseudo
  code for the paper. \emph{Date}: 1~February.
\item Evaluate the algorithm by comparing it to multiple alternatives, as
  detailed in Section \ref{sec:evaluation}. \emph{Deliverables}: multiple plots
  in an SVG or PDF format, along with code that produces them. \emph{Date}:
  1~April.
\item Write/finalise the paper. \emph{Deliverable}: a 14-page paper.
  \emph{Date}: 12~April.
\end{enumerate}

\subsection{Evaluation} \label{sec:evaluation}

We would like to compare our approach to alternatives that support nonlinear
reward functions. According to a recent survey
\cite{DBLP:journals/corr/abs-1806-06877}, this leaves us with five algorithms:
\textsc{LEARCH} \cite{DBLP:journals/arobots/RatliffSB09}, \textsc{MmpBoost}
\cite{DBLP:conf/nips/RatliffBBC06}, the original GP IRL paper
\cite{DBLP:conf/nips/LevinePK11}, its deep GP extension
\cite{DBLP:conf/uai/JinDAS17}, and an NN-based reward function
approximation \cite{wulfmeier2015maximum}. As the first two have already been
shown to underperform \cite{DBLP:conf/nips/LevinePK11}, we will focus on the
remaining three.

The algorithms will be compared on variations of two commonly used fictional
scenarios: object world \cite{DBLP:conf/nips/LevinePK11} and highway driving
behaviour \cite{DBLP:conf/icml/PieterN04,DBLP:conf/nips/LevinePK10}. While more
recent papers (especially those using deep learning) often use the binary world
benchmark as well \cite{DBLP:conf/uai/JinDAS17,wulfmeier2015maximum}, our
approach is not able to construct new features, and thus cannot perform well on
such a task.

Our main evaluation metric is \emph{expected value difference} (EVD)
\cite{DBLP:conf/uai/JinDAS17}, calculated as
\[ \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^tr(s_t) \middle| \pi^* \right] -
  \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t r(s_t) \middle| \hat\pi
  \right], \]
where $\pi^*$ is the optimal policy, and $\hat\pi$ is the policy generated using
our reward function. We can measure how EVD changes both over time and as the
number of demonstrations increase during training as well as use EVD to quantify
how well the learned GP reward functions transfer to previously unseen yet
similar environments. The learned reward functions can also be visualised as
shades of grey on maps, just like in previous papers
\cite{DBLP:conf/uai/JinDAS17,DBLP:conf/nips/LevinePK11}. Lastly, to evaluate how
well the variational approximation models the real posterior, we can use two
metrics recently proposed by Yao et al. \cite{DBLP:conf/icml/YaoVSG18}:
Pareto-smoothed importance sampling that measures overall goodness of fit, and
variational simulation-based calibration that can detect bias in point
estimates.

\bibliographystyle{plain}
\bibliography{mprop}
\end{document}
